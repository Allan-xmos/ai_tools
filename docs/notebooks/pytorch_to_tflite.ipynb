{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c42ac65",
   "metadata": {},
   "source": [
    "# Converting PyTorch to TensorFlow Lite for xCORE Using ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models. We can convert from PyTorch to ONNX, then from ONNX to TensorFlow, then from TensorFlow to TensorFlow Lite, and finally, run it through xformer to optimise it for xCORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!pip install onnx\n",
    "!pip install nvidia-pyindex\n",
    "!pip install onnx-graphsurgeon\n",
    "!pip install polygraphy\n",
    "!pip install onnxruntime\n",
    "!pip install onnxsim\n",
    "!pip install simple_onnx_processing_tools\n",
    "!pip install protobuf==3.20.3\n",
    "!pip install h5py==3.7\n",
    "!pip install onnx2tf\n",
    "!pip install onnx-tf\n",
    "!pip install tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0876174",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90a93c",
   "metadata": {},
   "source": [
    "## Import PyTorch Model\n",
    "\n",
    "For this example, we use mobilenet_v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69af6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161313fe",
   "metadata": {},
   "source": [
    "### Run inference on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image to test against\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b181f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Image Labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f56306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Open testing image\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# Show top categories per image\n",
    "vals, idxs = torch.topk(probabilities, 5)\n",
    "pytorch_results = [(categories[idx], prob) for (idx, prob) in zip(idxs.tolist(), vals.tolist())]\n",
    "print(pytorch_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fde0a",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "sample_input = torch.rand((batch_size, channels, height, width))\n",
    "\n",
    "onnx_model_path = \"mobilenet_v2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    onnx_model_path,\n",
    "    input_names=['image'],\n",
    "    output_names=['probabilities']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d0a67",
   "metadata": {},
   "source": [
    "### Check ONNX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec78973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime \n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def softmax(xs):\n",
    "    return np.exp(xs)/sum(np.exp(xs))\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_batch)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "data = zip(range(len(ort_outs[0][0])), softmax(ort_outs[0][0]))\n",
    "\n",
    "onnx_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "print(onnx_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1db68",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb) for more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def representative_dataset():\n",
    "    batch_size = 8\n",
    "    for _ in range(100):\n",
    "      data = np.random.uniform(-0.1, 0.001, (batch_size, height, width, channels))\n",
    "      yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277af37",
   "metadata": {},
   "source": [
    "## Using onnx-tensorflow (no longer maintained)\n",
    "\n",
    "Official ONNX package, however no longer officially maintained: https://github.com/onnx/onnx-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "saved_model_path = \"saved_model\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "prepare(onnx_model).export_graph(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8 \n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_model_path = 'mobilenet_v2.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0f492",
   "metadata": {},
   "source": [
    "## Using onnx2tf\n",
    "\n",
    "Using unofficial package: https://github.com/PINTO0309/onnx2tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5ad56",
   "metadata": {},
   "source": [
    "### Convert ONNX to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ef1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_model_path,\n",
    "    output_signaturedefs=True,\n",
    "    non_verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f3aa5",
   "metadata": {},
   "source": [
    "### Convert Keras to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_model_path = 'mobilenet_v2.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a981e",
   "metadata": {},
   "source": [
    "### Check it Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bfdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "tf_interpreter.allocate_tensors()\n",
    "\n",
    "tf_input_details = tf_interpreter.get_input_details()\n",
    "tf_output_details = tf_interpreter.get_output_details()\n",
    "\n",
    "# Convert PyTorch Input Tensor into Numpy Matrix and Reshape for TensorFlow\n",
    "# (Pytorch model expects C x H x W but TF expects H x W x C)\n",
    "tf_input_shape = tf_input_details[0]['shape']\n",
    "tf_input_data = input_batch.detach().numpy().reshape(tf_input_shape)\n",
    "\n",
    "tf_interpreter.set_tensor(tf_input_details[0]['index'], tf_input_data)\n",
    "tf_interpreter.invoke()\n",
    "\n",
    "tf_output_data = tf_interpreter.get_tensor(tf_output_details[0]['index'])\n",
    "probs = tf_output_data[0]\n",
    "data = zip(range(len(probs)), probs)\n",
    "tf_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)]\n",
    "print(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "arr_ = np.squeeze(tf_input_data, 0)\n",
    "plt.imshow(arr_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9ae5b",
   "metadata": {},
   "source": [
    "# Analysing Models\n",
    "\n",
    "Defined below is a function to print out the operator counts of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc21b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def get_operator_counts(model_content):\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        tf.lite.experimental.Analyzer.analyze(model_content=model_content)\n",
    "        model_structure = buf.getvalue()\n",
    "\n",
    "    operators = [op.strip().split(\" \")[1].split(\"(\")[0] for op in model_structure.split(\"\\n\") if \"Op#\" in op]\n",
    "    op_counts = {}\n",
    "    for operator in operators:\n",
    "        if operator in op_counts:\n",
    "            op_counts[operator] = op_counts[operator]+1\n",
    "        else:\n",
    "            op_counts[operator] = 1\n",
    "        \n",
    "    return (len(operators), op_counts)\n",
    "\n",
    "def print_operator_counts(model_content):\n",
    "    total_op_count, op_counts = get_operator_counts(model_content)\n",
    "    print(f\"{'Operator'.upper():<20} {'Count'.upper():>6}\")\n",
    "    print(\"-\"*20 + \" \" + \"-\"*6)\n",
    "    \n",
    "    for operator, count in op_counts.items():\n",
    "        print(f\"{operator.lower():<20} {count:>6}\")\n",
    "        \n",
    "    print(\"-\"*20 + \" \" + \"-\"*6)\n",
    "    print(f\"{'Total'.upper():<20} {total_op_count:>6}\")\n",
    "    print(\"-\"*20 + \" \" + \"-\"*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_operator_counts(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15cbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
