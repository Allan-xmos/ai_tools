{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c42ac65",
   "metadata": {},
   "source": [
    "# Converting PyTorch to TensorFlow Lite for xCORE Using ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models. We can convert from PyTorch to ONNX, then from ONNX to TensorFlow, then from TensorFlow to TensorFlow Lite, and finally, run it through xformer to optimise it for xCORE.\n",
    "\n",
    "Ensure that you have installed Python 3.8 and have the installed requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdecdb6-82c4-4dc3-be6c-524b017c2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90a93c",
   "metadata": {},
   "source": [
    "## Import PyTorch Model\n",
    "\n",
    "For this example, we use YOLOv8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69af6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.23M/6.23M [00:01<00:00, 5.75MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "pytorch_yolo = YOLO(\"yolov8n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82593e-5112-4daa-8297-2179f153a758",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "YOLO is trained on the COCO dataset. First lets download the images and their annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a82b12-820b-41e8-b341-00a110cc758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/val2017.zip -nc\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -nc\n",
    "\n",
    "def unzip_dataset(path):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./coco\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"./coco\"):\n",
    "    unzip_dataset(\"val2017.zip\")\n",
    "    unzip_dataset(\"annotations_trainval2017.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffaf43-97eb-48d9-b49b-970075afd3be",
   "metadata": {},
   "source": [
    "Once we have downloaded and extracted the dataset, we can create generators which returns a tuple of [path, original image, original shape, Tensor of type BCHW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ec430-0343-4ba1-932d-00a4bb055f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy\n",
    "import json\n",
    "\n",
    "def open_and_preprocess(path: str):\n",
    "    size = pytorch_yolo.model.args[\"imgsz\"]\n",
    "    pil_img = Image.open(path).convert(\"RGB\")\n",
    "    resized_img = pil_img.resize((size, size))\n",
    "    np_arr = numpy.array(resized_img).transpose([2, 0, 1])\n",
    "    return (\n",
    "        path,\n",
    "        numpy.array(pil_img),\n",
    "        numpy.array(pil_img).shape,\n",
    "        torch.from_numpy(numpy.ascontiguousarray(numpy.expand_dims(np_arr, 0).astype(numpy.single) / 255))\n",
    "    )\n",
    "\n",
    "# generator that returns validation images as tuple [path, original image, original image shape, torch tensors of shape BCHW]\n",
    "def validation_images():\n",
    "    with open(\"./coco/annotations/instances_val2017.json\") as fh:\n",
    "        data = json.load(fh)\n",
    "        images = data[\"images\"]\n",
    "    \n",
    "        for image in images:\n",
    "            yield open_and_preprocess(\"coco/val2017/\" + image[\"file_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2630c0",
   "metadata": {},
   "source": [
    "## Perform an infrence on the pytorch model\n",
    "\n",
    "Perform inference on the model to see how it works. The class you import when you run the imported \"model\" is not itself a pytorch model, it is a wrapper around it which applies some post-processing to the model output to put it into a nice `Results` object. Lets run both with and without the wrapper to see how to get the outputs into the `Results` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013896a2-c279-4901-b656-3d0d4d37ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same demo image we will use for all examples\n",
    "demo_image = next(validation_images())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6db91e-c005-4df4-8995-87c9a02dc41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using yolo wrapper\n",
    "wrapper_results = pytorch_yolo(demo_image[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49960e00-b5cb-4803-9e3c-03a2fd642ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using model directly\n",
    "torch_tensor = demo_image[3]\n",
    "pytorch_results = pytorch_yolo.model(torch_tensor)\n",
    "a, _ = pytorch_results\n",
    "\n",
    "from ultralytics.utils.ops import non_max_suppression\n",
    "from ultralytics.engine.results import Results\n",
    "\n",
    "preds = non_max_suppression(\n",
    "    a,\n",
    "    pytorch_yolo.predictor.args.conf,\n",
    "    pytorch_yolo.predictor.args.iou,\n",
    "    agnostic=pytorch_yolo.predictor.args.agnostic_nms,\n",
    "    max_det=pytorch_yolo.predictor.args.max_det,\n",
    "    classes=pytorch_yolo.predictor.args.classes\n",
    ")[0]\n",
    "pytorch_processed_results = Results(\n",
    "    orig_img = demo_image[1],\n",
    "    path = demo_image[0],\n",
    "    names = demo_image[2],\n",
    "    boxes=preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9d3bf-ef83-410e-99ce-1b0d9fdbee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output wrapper results\n",
    "wrapper_results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aed4cc-2e61-4b8d-98d8-dd42e44da91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output pytorch processed results\n",
    "pytorch_processed_results.boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fde0a",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e96873-0da5-4856-a1cc-b7242819d4a1",
   "metadata": {},
   "source": [
    "Pytorch has an onnx exporter included. It runs the model with the sample input and generates a trace of it. **For YOLO, which can normally accept a variety of input types (such as URLs, paths, PIL objects or numpy arrays), the converted model will only be able to accept the same type as the input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for shape info for tracing the model during conversion\n",
    "sample_input = next(validation_images())[3]\n",
    "\n",
    "onnx_model_path = \"yolov8_v2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    pytorch_yolo.model,\n",
    "    sample_input,\n",
    "    onnx_model_path,\n",
    "    input_names=['images'],\n",
    "    output_names = ['output0']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6475e32",
   "metadata": {},
   "source": [
    "### Check ONNX output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d41df4-5dfe-4e71-b299-2cec1aee45ab",
   "metadata": {},
   "source": [
    "Check the onnx model with the check_model helper, then run the model with the same input as before to check that the results are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5853f-f7d0-457a-98a3-0f20d54936b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "onnx_outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"images\": sample_input.numpy()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c09ca1-0acf-4394-98f8-45c01edc07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first output is what we're interested in\n",
    "onnx_result = onnx_outputs[0]\n",
    "\n",
    "onnx_preds = non_max_suppression(\n",
    "    torch.from_numpy(onnx_result),\n",
    "    pytorch_yolo.predictor.args.conf,\n",
    "    pytorch_yolo.predictor.args.iou,\n",
    "    agnostic=pytorch_yolo.predictor.args.agnostic_nms,\n",
    "    max_det=pytorch_yolo.predictor.args.max_det,\n",
    "    classes=pytorch_yolo.predictor.args.classes\n",
    ")[0]\n",
    "onnx_processed_results = Results(\n",
    "    orig_img = demo_image[1],\n",
    "    path = demo_image[0],\n",
    "    names = demo_image[2],\n",
    "    boxes=preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562a899-6b30-407e-8e07-b9534fdc6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_processed_results.boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5ad56",
   "metadata": {},
   "source": [
    "### Convert ONNX to Keras\n",
    "\n",
    "We do this using the `onnx2tf` package: https://github.com/PINTO0309/onnx2tf\n",
    "\n",
    "**NB: onnx2tf will transpose your inputs to change BCHW into BHWC as is the TensorFlow convention.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93812f5-bd99-4aa5-9720-2509dfbad9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_model_path,\n",
    "    output_folder_path=\"yolo_saved_model\",\n",
    "    copy_onnx_input_output_names_to_tflite=True,\n",
    "    non_verbose=True,\n",
    "    enable_rnn_unroll=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549092a",
   "metadata": {},
   "source": [
    "### Check the conversion to keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1941714-3e8b-4529-91dd-af385d5e8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to BHWC\n",
    "tf_input_data = numpy.transpose(sample_input.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "keras_unprocessed_results = keras_model(tf_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b58cba-c557-4729-8809-6f6b4690c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use helpers from YOLO to run post-processsing\n",
    "keras_preds = non_max_suppression(\n",
    "    torch.from_numpy(keras_unprocessed_results.numpy()),\n",
    "    pytorch_yolo.predictor.args.conf,\n",
    "    pytorch_yolo.predictor.args.iou,\n",
    "    agnostic=pytorch_yolo.predictor.args.agnostic_nms,\n",
    "    max_det=pytorch_yolo.predictor.args.max_det,\n",
    "    classes=pytorch_yolo.predictor.args.classes\n",
    ")[0]\n",
    "keras_processed_results = Results(\n",
    "    orig_img = demo_image[1],\n",
    "    path = demo_image[0],\n",
    "    names = demo_image[2],\n",
    "    boxes=preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692d53d-35db-48ed-bf36-0d37c26999fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_processed_results.boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497e02a",
   "metadata": {},
   "source": [
    "## Convert Keras to TFLite (int8)\n",
    "We will still feed the data into the model in float32 format for convinence but the internals of the model will be int8. This will require representitive data but as we interface in float32 we can use the pytorch preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1db68",
   "metadata": {},
   "source": [
    "### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb) for more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bbdbe4-42b4-493c-8075-f1f1b82fb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def representative_dataset():\n",
    "    # in the interest of running the notebook fast, we only provide ten images;\n",
    "    # ideally this would be done with a few hundred images.\n",
    "    for image in itertools.islice(validation_images(), 10):\n",
    "        # change to BHWC\n",
    "        tf_batch = numpy.transpose(image[3].numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855d75e-73d8-4dbe-a96d-499e945f3da9",
   "metadata": {},
   "source": [
    "### Run converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9ad9c-ef70-4b07-a970-1dc75e49c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8 \n",
    "# leave output as float32 so that we can use the YOLO post-processing functions\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = 'yolo.tflite'\n",
    "with open(tflite_int8_model_path, 'wb') as f:\n",
    "  f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdb4f8-bcd4-4393-8d9f-f885935f0269",
   "metadata": {},
   "source": [
    "### Check output\n",
    "Lets use the same image as before to check that it gives us the same outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a974790-17c1-4f4a-a7ba-e747379fef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert intput to int8\n",
    "tf_int_data = (tf_input_data*255 - 128).astype(numpy.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5972519-bd21-43d2-bce6-3d6844d77730",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "tfl_input_data = tf_int_data\n",
    "\n",
    "tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tfl_input_data)\n",
    "tfl_interpreter.invoke()\n",
    "\n",
    "tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f56018-eaaa-438f-828b-16b54ad3c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_preds = non_max_suppression(\n",
    "    torch.from_numpy(tfl_output_data),\n",
    "    pytorch_yolo.predictor.args.conf,\n",
    "    pytorch_yolo.predictor.args.iou,\n",
    "    agnostic=pytorch_yolo.predictor.args.agnostic_nms,\n",
    "    max_det=pytorch_yolo.predictor.args.max_det,\n",
    "    classes=pytorch_yolo.predictor.args.classes\n",
    ")[0]\n",
    "tfl_processed_results = Results(\n",
    "    orig_img = demo_image[1],\n",
    "    path = demo_image[0],\n",
    "    names = demo_image[2],\n",
    "    boxes=preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf81f2-4709-474d-b09d-9bc80518eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_processed_results.boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9ae5b",
   "metadata": {},
   "source": [
    "# Analyse Models\n",
    "Now the model is converted and we have confirmed that it works, let us take a look inside the converted models to see how good the conversion is.\n",
    "\n",
    "## Check Operator Counts\n",
    "\n",
    "Let us take a look at the operator counts inside the converted model. This uses a helper function defined in `../utils`, but this step is not necessary to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00458d-007d-403b-a590-1a6c9376a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_operator_counts(tflite_int8_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
