{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c42ac65",
   "metadata": {},
   "source": [
    "# Converting PyTorch to TensorFlow Lite for xCORE Using ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models. We can convert from PyTorch to ONNX, then from ONNX to TensorFlow, then from TensorFlow to TensorFlow Lite, and finally, run it through xformer to optimise it for xCORE.\n",
    "\n",
    "Ensure that you have installed Python 3.8 and have the installed requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdecdb6-82c4-4dc3-be6c-524b017c2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90a93c",
   "metadata": {},
   "source": [
    "## Import PyTorch Model\n",
    "\n",
    "For this example, we use YOLOv8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69af6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Ultralytics YOLOv8.0.146 üöÄ Python-3.10.8 torch-1.12.0+cu102 CPU (Intel Core(TM) i5-1038NG7 2.00GHz)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
      "\n",
      "Dataset 'coco.yaml' images not found ‚ö†Ô∏è, missing path '/home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/val2017.txt'\n",
      "Unzipping /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco2017labels-segments.zip to /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets...\n",
      "Skipping /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco2017labels-segments.zip unzip (already unzipped)\n",
      "Downloading http://images.cocodataset.org/zips/train2017.zip to '/home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/images/train2017.zip'...\n",
      "Downloading http://images.cocodataset.org/zips/test2017.zip to '/home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/images/test2017.zip'...\n",
      "Unzipping /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/images/val2017.zip to /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/images...\n",
      "Skipping /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/images/val2017.zip unzip (already unzipped)\n",
      "Unzipping /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/images/test2017.zip to /home/jovyan/work/pytorch_to_tflite/YOLOv8/datasets/coco/images...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "pytorch_yolo = YOLO(\"yolov8n\")\n",
    "pytorch_yolo.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82593e-5112-4daa-8297-2179f153a758",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "YOLO is trained on the COCO dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffaf43-97eb-48d9-b49b-970075afd3be",
   "metadata": {},
   "source": [
    "Once we have downloaded and extracted the dataset, we can create generators which returns a tuple of [path, original image, original shape, Tensor of type BCHW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "485ec430-0343-4ba1-932d-00a4bb055f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy\n",
    "\n",
    "def open_and_preprocess(path: str):\n",
    "    size = pytorch_yolo.model.args[\"imgsz\"]\n",
    "    pil_img = Image.open(\"./coco/\" + path.strip()).convert(\"RGB\")\n",
    "    resized_img = pil_img.resize((size, size))\n",
    "    np_arr = numpy.array(resized_img).transpose([2, 0, 1])\n",
    "    return (\n",
    "        path,\n",
    "        numpy.array(pil_img),\n",
    "        numpy.array(pil_img).shape,\n",
    "        torch.from_numpy(numpy.ascontiguousarray(numpy.expand_dims(np_arr, 0).astype(numpy.single) / 255))\n",
    "    )\n",
    "\n",
    "# generator that returns validation images as tuple [path, original image, original image shape, torch tensors of shape BCHW]\n",
    "def validation_images():\n",
    "    with open(\"./coco/val2017.txt\") as fh:\n",
    "        for path in fh.readlines():\n",
    "            yield open_and_preprocess(path)\n",
    "                \n",
    "# generator that returns training images as tuple [path, original image, original image shape, torch tensors of shape BCHW]\n",
    "def train_images():\n",
    "    with open(\"./coco/train2017.txt\") as fh:\n",
    "        for path in fh.readlines():\n",
    "            yield open_and_preprocess(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2630c0",
   "metadata": {},
   "source": [
    "## Perform an infrence on the pytorch model\n",
    "Perform inference on the model to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6db91e-c005-4df4-8995-87c9a02dc41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image = next(validation_images())\n",
    "# using yolo wrapper\n",
    "results = pytorch_yolo(demo_image[3]))  # return a list of Results objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49960e00-b5cb-4803-9e3c-03a2fd642ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using model directly\n",
    "tensor = next(demo_image[3])\n",
    "pytorch_results = pytorch_yolo.model(tensor)  # return a list of Results objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d362be9a-2f8a-46d5-86b1-70d0241565d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.nn.tasks.DetectionModel"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what the type of the model is\n",
    "type(pytorch_yolo.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60795ea2-ce48-4a4f-ae7a-9137dde33b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    https://github.com/ultralytics/ultralytics/blob/c3c27b019a9516a9b2c78c291b61ef7cf97ff7f3/ultralytics/engine/results.py#L66\n",
    "\n",
    "    The class for holding results is Results, which takes instances of Boxes, Masks, Keypoints and Probs, which take tensors and process their values:\n",
    "    \n",
    "        boxes (torch.Tensor | numpy.ndarray): A tensor or numpy array containing the detection boxes,\n",
    "        masks (torch.Tensor | np.ndarray): A tensor containing the detection masks, with shape (num_masks, height, width).\n",
    "        keypoints (torch.Tensor | np.ndarray): A tensor containing the detection keypoints, with shape (num_dets, num_kpts, 2/3). \n",
    "        probs (torch.Tensor | np.ndarray): A tensor containing the detection keypoints, with shape (num_class, ).\n",
    "\"\"\"\n",
    "a, b = pytorch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4501c37-dfaf-4b18-9c50-4b67333a0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 8400])\n",
      "torch.Size([1, 144, 80, 80])\n",
      "torch.Size([1, 144, 40, 40])\n",
      "torch.Size([1, 144, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# The first tensor has the same shape as output0 here: https://github.com/ultralytics/ultralytics/blob/c3c27b019a9516a9b2c78c291b61ef7cf97ff7f3/ultralytics/engine/exporter.py#L320\n",
    "# but that should only be the case for DetectionModels, which should only have a single output. This model however has two outputs.\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93458ff4-3e32-442e-bba6-37e5423a3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.ops import non_max_suppression\n",
    "from ultralytics.engine.results import Results\n",
    "\n",
    "preds = non_max_suppression(\n",
    "    a,\n",
    "    pytorch_yolo.predictor.args.conf,\n",
    "    pytorch_yolo.predictor.args.iou,\n",
    "    agnostic=pytorch_yolo.predictor.args.agnostic_nms,\n",
    "    max_det=pytorch_yolo.predictor.args.max_det,\n",
    "    classes=pytorch_yolo.predictor.args.classes\n",
    ")\n",
    "results = Results(\n",
    "    orig_img = demo_image[1],\n",
    "    path = demo_image[0],\n",
    "    names = demo_image[2],\n",
    "    boxes=preds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fde0a",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a13b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/ultralytics/nn/modules/head.py:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif self.dynamic or self.shape != shape:\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "# This is only for shape info for tracing the model during conversion\n",
    "sample_input = next(validation_images())\n",
    "\n",
    "onnx_model_path = \"yolov8_v2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    pytorch_yolo.model,\n",
    "    sample_input,\n",
    "    onnx_model_path,\n",
    "    input_names=['images'],\n",
    "    output_names = ['output0']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6475e32",
   "metadata": {},
   "source": [
    "### Check the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bdc8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d2393",
   "metadata": {},
   "source": [
    "### Check ONNX Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5ad56",
   "metadata": {},
   "source": [
    "### Convert ONNX to Keras\n",
    "We do this using the `onnx2tf` package: https://github.com/PINTO0309/onnx2tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549092a",
   "metadata": {},
   "source": [
    "### Check the conversion to keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497e02a",
   "metadata": {},
   "source": [
    "## Convert Keras to TFLite (int8)\n",
    "We will still feed the data into the model in float32 format for convinence but the internals of the model will be int8. This will require representitive data but as we interface in float32 we can use the pytorch preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1db68",
   "metadata": {},
   "source": [
    "### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb) for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9ae5b",
   "metadata": {},
   "source": [
    "# Analyse Models\n",
    "Now the model is converted and we have confirmed that it works, let us take a look inside the converted models to see how good the conversion is.\n",
    "\n",
    "## Check Operator Counts\n",
    "\n",
    "Let us take a look at the operator counts inside the converted model. This uses a helper function defined in `../utils`, but this step is not necessary to convert the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b76da4",
   "metadata": {},
   "source": [
    "## Compare Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d503b-4dd3-43b1-bfc7-6ac74882a505",
   "metadata": {},
   "source": [
    "Let's compare the accuracy of the converted model to the original PyTorch model.\n",
    "\n",
    "To do this, we take a large sampel from imagenet_v2 and compare the classifications returned by the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
