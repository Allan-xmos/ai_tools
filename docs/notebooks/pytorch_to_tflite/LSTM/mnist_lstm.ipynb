{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53aeff71-1811-4956-95db-13c0ad7f276d",
   "metadata": {},
   "source": [
    "# Training and Converting an LSTM Model from PyTorch to Tensorflow Lite for Micro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a794d61-a6ff-42b3-bc1d-849c2dd2ab75",
   "metadata": {},
   "source": [
    "## Training in PyTorch\n",
    "\n",
    "Let's train a hand-written character recognition model with the MNIST dataset which uses LSTM models.\n",
    "\n",
    "Start off with importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517f9774-2b1d-4903-a089-1fd032b6532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\"\"\"\n",
    "Each data point is of form [PIL.Image, class] where the class is a number 0-9 of the\n",
    "respective digit. The image is a single channel image with values from 0 to 1.\n",
    "\"\"\"\n",
    "def mnist_data(train: bool) -> torchvision.datasets.MNIST:\n",
    "    return torchvision.datasets.MNIST(\n",
    "        './mnist/',\n",
    "        download=True,\n",
    "        train=train, \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "# iterator returning Tensors of type [batch (10), channels (1), height (28), width (28)]\n",
    "data_loaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        mnist_data(True),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        mnist_data(False),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f54c38-9bd9-4b43-8176-603f1c151256",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2670a68-ae13-4d9e-b9de-297cf74f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):\n",
    "        super(MNIST_LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Remove batch parameter\n",
    "        x = x.reshape(-1, self.input_size, self.input_size)\n",
    "\n",
    "        # Set initial hidden and cell states \n",
    "        hidden_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        cell_states_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, (hidden_initial, cell_states_initial))\n",
    "        \n",
    "        dropout_out = self.dropout(lstm_out)\n",
    "        \n",
    "        fc_out = self.fc(dropout_out[:, -1, :])\n",
    "        return fc_out\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5304fc-83cd-4bae-8334-5de38e890f56",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4f4496e-63dd-4753-8a90-004351b15674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=28\n",
    "\n",
    "# instantiate model\n",
    "model = MNIST_LSTM(input_size, hidden_size=128)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a154b68-b258-405e-a2eb-a03af2b7361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, model: MNIST_LSTM, loaders):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    print(\"Ended epoch: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4adfd31-a7c9-42c8-a604-f39afaa5d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Ended epoch: \n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=3, model=model, loaders=data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "756e71c9-c34d-4d83-9cd2-ff8b75f040df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.07 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in data_loaders['test']:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "        \n",
    "        correct = correct + (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6161c20-3be0-4a6b-af5f-e375cc2d7ae8",
   "metadata": {},
   "source": [
    "### Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8748974-a7dd-45f2-a57a-b3414b383733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5ElEQVR4nNXRIUuDURTG8Yep6BDDkjOZtj5kgsGyaHLB7JdY8AOITLEp7BssWERRjGIQLfqGfYUXxSYaLP+H13CH6OtZMHrD5XB+nHsO90j/9Ewt1DsHhe3j+TI1BycjwAA3tRJmKZ+u05SrjG2tMQ5unyUt/yycHWHgaHFuE/xYbtru9e82pJlru8gmzLwDeDe2Vg48LMX4AtALaevJwP2vT5Ck1oUL+zC0lRzMayOyWg58XHXDhh2A8++ZylfUPgtLJEnVy8K2w2E0TAvZDjGtrB8/m2HYm45x/e19f3WC/f18AlM3eNWTAF7OAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD58E8E9C10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "results = model(images).tolist()\n",
    "processed_results = list(map(lambda res: res.index(max(res)), results))\n",
    "\n",
    "display(torchvision.transforms.ToPILImage()(images[0]))\n",
    "print(processed_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a7040-957d-4f2a-9b0c-f42864cef56a",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5391abd8-c0d1-4910-87bc-372d760da611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "sample_input, sample_labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "onnx_filename = \"handwriting.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    onnx_filename,\n",
    "    input_names= [\"image\"],\n",
    "    output_names=[\"number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ecc8f-3cab-46e7-b6e2-084b1aa351a0",
   "metadata": {},
   "source": [
    "## Test ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8cab25e-2e3b-4edd-a6f2-6ada65cb67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc577c33-5f28-4581-84d5-84c0f006299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAk0lEQVR4nMWSwQ0CMRADR9RBCtlCSCHXRxrZQiLqMIWYx3EgwYYn+BFFscYbR4H/qg1ra7UX8pRVc3KHdK/MVAOY+To6PXc3zsC+rC7lktx14bompxddWu/TW+1ttm2NEk3b2YcVFZkZQEiLsQBRP9JDOqq+9wTal1RmERtxePnhkc4GIWcVO2zLHvWwmOtv8jPdAWBCQ3dhk/YBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD58D861FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_filename)\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"image\": sample_input.numpy()},\n",
    ")\n",
    "onnx_processed_results = [numpy.argmax(res) for res in outputs[0]]\n",
    "\n",
    "print(onnx_processed_results[0])\n",
    "display(torchvision.transforms.ToPILImage()(sample_input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e2af0-ee1e-498e-8293-0bc41834aa4f",
   "metadata": {},
   "source": [
    "\n",
    "## ONNX To TFlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99dcf-4503-4e8b-acc9-5e06be6a67fb",
   "metadata": {},
   "source": [
    "### First to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7d5d4beb-6978-4dfb-ad1f-a39415f80c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n",
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n"
     ]
    }
   ],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=\"handwriting.onnx\",\n",
    "    output_folder_path=\"mnist.tf\",\n",
    "    copy_onnx_input_output_names_to_tflite=True,\n",
    "    non_verbose=True,\n",
    "    param_replacement_file=\"param_replacement.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36939950-ceb7-47a9-99e2-0609a2d8c37e",
   "metadata": {},
   "source": [
    "#### Check Keras Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f797340-539f-45bc-97ba-862fcea5e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_correct = 0\n",
    "keras_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tf_input_data = np.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    keras_output_data = keras_model(tf_input_data)\n",
    "    processed_keras_output = [numpy.argmax(res) for res in keras_output_data]\n",
    "    \n",
    "    keras_results = zip(processed_keras_output, test_labels)\n",
    "    keras_batch_correct = sum([1 if output == label.item() else 0 for output, label in keras_results])\n",
    "        \n",
    "    keras_correct = keras_correct + keras_batch_correct\n",
    "    keras_total = keras_total + test_labels.size(0)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy of the keras model on the {keras_total} test images: {100 * keras_correct / keras_total} % ({keras_correct}/{keras_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb30033-ad26-4568-90f3-f85c064f29d3",
   "metadata": {},
   "source": [
    "### Now exoirt as TensorFlow Lite for Micro\n",
    "Now let's quantise our model to int8 and export it to a TensorFlow Lite flatbuffer.\n",
    "\n",
    "\n",
    "#### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to Converting a keras model into an xcore optimised tflite model for more details on this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "61ff7a3f-2826-4a3f-8a77-e9b93a68dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for test_images, _ in data_loaders['test']:\n",
    "        tf_batch = np.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1043d1fd-b8c2-4184-9305-4a6f22e36549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = 'handwriting.tflite'\n",
    "with open(tflite_int8_model_path, 'wb') as f:\n",
    "  f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ae7bc907-7baa-484d-9314-b49a64b32917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running accuracy: 488/500 (97.6%)\n",
      "running accuracy: 982/1000 (98.2%)\n",
      "running accuracy: 1473/1500 (98.2%)\n",
      "running accuracy: 1963/2000 (98.15%)\n",
      "running accuracy: 2453/2500 (98.12%)\n",
      "running accuracy: 2946/3000 (98.2%)\n",
      "running accuracy: 3430/3500 (98.0%)\n",
      "running accuracy: 3922/4000 (98.05%)\n",
      "running accuracy: 4415/4500 (98.11111111111111%)\n",
      "running accuracy: 4903/5000 (98.06%)\n",
      "running accuracy: 5396/5500 (98.10909090909091%)\n",
      "running accuracy: 5890/6000 (98.16666666666667%)\n",
      "running accuracy: 6383/6500 (98.2%)\n",
      "running accuracy: 6872/7000 (98.17142857142858%)\n",
      "running accuracy: 7358/7500 (98.10666666666667%)\n",
      "running accuracy: 7842/8000 (98.025%)\n",
      "running accuracy: 8333/8500 (98.03529411764706%)\n",
      "running accuracy: 8822/9000 (98.02222222222223%)\n",
      "running accuracy: 9310/9500 (98.0%)\n",
      "running accuracy: 9798/10000 (97.98%)\n",
      "Test Accuracy of the TensorFlow Lite for Micro model on the 10000 test images: 97.98 % (9798/10000)\n"
     ]
    }
   ],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "tfl_correct = 0\n",
    "tfl_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tfl_input_data = np.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tfl_input_data)\n",
    "    tfl_interpreter.invoke()\n",
    "\n",
    "    tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "    tfl_processed_results = [numpy.argmax(res) for res in tfl_output_data]\n",
    "    \n",
    "    tfl_results = zip(tfl_processed_results, test_labels)\n",
    "    tfl_batch_correct = sum([1 if output == label.item() else 0 for output, label in tfl_results])\n",
    "        \n",
    "    tfl_correct = tfl_correct + tfl_batch_correct\n",
    "    tfl_total = tfl_total + test_labels.size(0)\n",
    "    \n",
    "    if tfl_total % 500 == 0:\n",
    "        print(f\"running accuracy: {tfl_correct}/{tfl_total} ({100 * tfl_correct / tfl_total}%)\")\n",
    "\n",
    "print(f\"Test Accuracy of the TensorFlow Lite for Micro model on the {tfl_total} test images: {100 * tfl_correct / tfl_total} % ({tfl_correct}/{tfl_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4afd62-fc44-4798-ba66-2510f0fcda0d",
   "metadata": {},
   "source": [
    "## Training in TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1370e5-8444-43df-a80f-2f15ae29bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#Importing the data\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data() \n",
    "\n",
    "#Normalizing the data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d19ab-552f-44fb-97af-a564d18b954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129585c3-cd5a-4cdc-9906-0f306f6695eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initializing model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#Adding the model layers\n",
    "model.add(keras.layers.LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.LSTM(128))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Compiling the model\n",
    "model.compile( loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam(lr=0.001, decay=1e-6), metrics=['accuracy'] )\n",
    "\n",
    "#Fitting data to the model\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761ac44-a26f-4d0b-be1d-269f51843676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f95adb-e040-45b6-916e-9ef4c5981241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test loss and accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dbc3b-ba82-4781-8a86-00e88cebfd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
