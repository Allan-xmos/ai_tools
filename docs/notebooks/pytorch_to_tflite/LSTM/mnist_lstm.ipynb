{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53aeff71-1811-4956-95db-13c0ad7f276d",
   "metadata": {},
   "source": [
    "# Training and Converting an LSTM Model from PyTorch to Tensorflow Lite for Micro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a794d61-a6ff-42b3-bc1d-849c2dd2ab75",
   "metadata": {},
   "source": [
    "## Training in PyTorch\n",
    "\n",
    "Let's train a hand-written character recognition model with the MNIST dataset which uses LSTM models.\n",
    "\n",
    "Start off with importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517f9774-2b1d-4903-a089-1fd032b6532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\"\"\"\n",
    "Each data point is of form [PIL.Image, class] where the class is a number 0-9 of the\n",
    "respective digit. The image is a single channel image with values from 0 to 1.\n",
    "\"\"\"\n",
    "def mnist_data(train: bool) -> torchvision.datasets.MNIST:\n",
    "    return torchvision.datasets.MNIST(\n",
    "        './mnist/',\n",
    "        download=True,\n",
    "        train=train, \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "# iterator returning Tensors of type [batch (10), channels (1), height (28), width (28)]\n",
    "data_loaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        mnist_data(True),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        mnist_data(False),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f54c38-9bd9-4b43-8176-603f1c151256",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2670a68-ae13-4d9e-b9de-297cf74f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):\n",
    "        super(MNIST_LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Remove batch parameter\n",
    "        x = x.reshape(-1, self.input_size, self.input_size)\n",
    "\n",
    "        # Set initial hidden and cell states \n",
    "        hidden_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        cell_states_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, (hidden_initial, cell_states_initial))\n",
    "        \n",
    "        dropout_out = self.dropout(lstm_out)\n",
    "        \n",
    "        fc_out = self.fc(dropout_out[:, -1, :])\n",
    "        return fc_out\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5304fc-83cd-4bae-8334-5de38e890f56",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4f4496e-63dd-4753-8a90-004351b15674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=28\n",
    "\n",
    "# instantiate model\n",
    "model = MNIST_LSTM(input_size, hidden_size=128)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a154b68-b258-405e-a2eb-a03af2b7361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, model: MNIST_LSTM, loaders):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    print(\"Ended epoch: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4adfd31-a7c9-42c8-a604-f39afaa5d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Ended epoch: \n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=3, model=model, loaders=data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6161c20-3be0-4a6b-af5f-e375cc2d7ae8",
   "metadata": {},
   "source": [
    "### Check that the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c8748974-a7dd-45f2-a57a-b3414b383733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4ElEQVR4nGNgGGggPPXpv3///5ULYZNM/gsBsQghfjjr+N+/r578/fv3gRRcyA3OilsVoyBa+vfvXz0c9mrs//t3Hwt2uaATqHbCAYuA4uw/f//+TOPBlPPY8ffv379/rzliMzEP4g8duAATppqpWN0iZmzsuvfv35fYXcrAwOD2Hbck/1XcOnkP//37xRdVub8yhKF26e/fLwGo6vX/PozmZuASdXn19+9JdwZ0yb9/L6++9Pfv37+fMEKc/Sg0Gv+e9cLikNl///79+/dpizA2V7JXf/37fao2Dj/QFQAAbTNxi6b3H+wAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD563A9A100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAwklEQVR4nGNgGI5Apv7bvzn62OVYT/999fLd61ImbJL1f1ewMQg1/M1eL48hJ/DtsQoDA4PI379/kzAkk/7ZMkAlbWBicBsYGdUYGBj4YhgZO45g6DT997hMOn7q379/rbA4KPXj379//+GQZBAITxMWVv17gxWbJAMDAwOD1d+rCA66l20Yj+KW1Pm/Fqepmh//CuPUqchz9RtOnYf+puCUY/j/UQ6nXPC/l8hcVDv5GfbglmRguILbSpuDErgl0QAAGNs+6BGvhOAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD564AE5790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0klEQVR4nN3QMQtBYRQG4FdZDIpkMSCDUFJ3scl2DQq7uspPUDY/wiTbzeYHWGS13IGyiIVElMWiO+g9DJQ7OH/A2b7vec/pfB/wb5XMew6xocwBAH4AgF2ObmaTG7B1C0a2mnmGPNnmhUJSuF4KSdnlvGPb9pT8BEhpvG9930DASAE4mx2MWspqwQOdgLZ3n4+a+qgTR6oNnlLXrOiK7VcsfqRrao0LsquZJTLWLHznLqFYZC8saY09yiCoWEVkpf1b+koa2tA+6WgGi05ExV/1Au5YWBdPFaeuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD564AE5520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6UlEQVR4nGNgGAyAUah8ZSx2KfnI+f/+/bskiCnDaTbx479//26vssCQcu06+e/fv/vbi8QxpBS6//67+mqpkxQ2yw7/+/fFRgy7Q1z//tvHxciKKsgCpf/8ZeFqEXZexcDQ/RxTa/mzf/9eP7z649+vcCwGK/r6aoozOPre/zYHiy+hQOPfP3cokwlD8gkDgw5OSQYGBi+ckmwIJqZkHSOaALunI4zp/fffv70oktIvT/JAWCnX//37FAYVhpphu+b1kT0MDDJROhwMDMX9aAZLLf8HBb+6eTAd6dz179+/f5faGzGk6AoACbBXmM8ABWgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD563B4CBE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgGEqAEZmj7Pt4LXZlmtNff/z743WvAha5Ce/+QsCbC8poUto3fv/9uz7eSmHa379/X/aGokiW/f37t4WNgYGBU61h69+/vzqRJef9/buGDcpmb7j/93cCQk7l518nJIfoPfh7D2Hxw78P+ZAN0n34dw6c8/9vH6oD+//+RZK8h6KT/z6S5PO/f7WQ5Fgb//7pgfM0n/69XcEKc63lpr+/e5HUpr/++3ddhaKioqJizca/fz+GobhA+8lfBHgQBRGEx4q2baEgAz/Ln48MK64duIEmycDAwMAQIfZ8NcNQBQACbG6Jgn4C6wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD563B4E6A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAcElEQVR4nGNgGGgg+L+RGbfk37/1+CQ/qiK4TOjyT27jkVzOgEeSAaekHj7J7/gk3fFJ4rVTHZ/Sl3//uuKTFMBlrLsgHjsFmBm+/8MlKcPAsPYTLskANCegSH5iYDiKU3IRA8MFnJIMDLtfMgwwAACYohqqFj5DtQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD563B4EFD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCklEQVR4nL2OrUtDYRSHH/ehKGxgNTlBsQlDhE0Ulkz6B2i2TBiIQYvBJJgMtygMFAYiKBgG9ikGYSIYhG1yvUURP5IDP+Cn5YrufV/rTjmc33Oew4H2V0fLNJNOLP27WlSzejbmZrmb60C67Hax+KmufElHLrgh7QxNfajR6YCeXsZhItCszfqftQawqK0wifzC+d76OkCMdxuOUH4DGGXPupr+DFIA7MszzchytOQD9AzSMMU51ZIAHEgZEx6GxzL3KvUZLFpTHuhaeFUxbopZKQ/JFel20vq18FVJTJ8/SeUBi7GtzUpTkob/hLGw31EAgt26b4tkJZ2s5lrDH/Px4RjvwmG1u74BJKVhcpx+Xx8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD563B4CBE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7klEQVR4nNWRsUtCURTGf+nb0geBa2BDU0NDLlJSY6vh8mho9J9obhOiP8C9wNloSd7mIOUSCk1tgVORoPHd55A38zznwG+65/zOPd8598I6acPE+dxWHYDGmyHF3Yu+vAyrfkhujvo9IFiws+YmAIMRzffHyZLnficEjmEwSk1WHUqSnh5WjX374+Wm3Z3fXMYWZUv19J7RHgAnZZ4PVnUGuHIuOfVBYGCSkBjP2k244JPxcv2LulE4P+nSNHOSWpUcHDqpZuCrJKkXx99On0cGRkP/F+7rOrVB8c7D9p+sf4SgsH0OEN+PUzf/VTNti3Kc2oK8qQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD564AE5DF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAyklEQVR4nM2SoRKCQBRFH45JK1SpUKWq2YofQfUDqGazM36CaJTsEKnq2FATO4N/sPethcAArxm87d37zu7dmSX6M1kdx5n4s0dWvfq24wIaGmXcpXyHwWAYmIRo2Mz8s50ZU532RKHdBpdgFXXcGlRaTYXiwYWVEBHtZI5iVt36tZxC5ohyToJACj1o6K3wEFqkd3DiSLC7ho4a86AZVh/LCiVyo3HtuXTsErk3Rt5X6FhGqdI4zHsPhMEzX7Xc+puMPKJ3JXX5sb7BhlO9qtSR/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD590BB0100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABFklEQVR4nM3PvUtCcRTG8ScxCyq7mmtDoC0NQdDcIqLUEoQ0VUOT9LZEW7NrRLQ5FC1C/0Ok3LAxIiLkDsGdIkrlWoN9a7hD/W66NfRMD3w4h3Ok/57Z48eq1cOsC+B+q6tlrgG4inaxXAM/K79oYPcNoN2hHZMkhb+tf7soeZed98npvuEXc26oBDSzChcOqAR2TlThoxyXJJcz00IPn+D53aVp4hJQsSRJsSf2zP/qUItLkkYOIWlgCVoZv26Cbf20bAdvwa/pV3cxYQwewbnfkvWb+Q3zHBtmJElzjrMc+FH28+mYpMR+fXw0FMRyay2SSBcb5JWKBHH1FseltpMcDIqk3AncrXcBSVKKQnRK+V78t/kCxtZ4Dri56HkAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD564AE5D30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load one batch to test\n",
    "images, labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "results = model(images).tolist()\n",
    "processed_results = list(map(lambda res: res.index(max(res)), results))\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(images[idx]))\n",
    "    print(f\"{processed_results[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687d372-cd62-4c01-9cd8-30cf67065bd5",
   "metadata": {},
   "source": [
    "### Check Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "756e71c9-c34d-4d83-9cd2-ff8b75f040df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.07 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in data_loaders['test']:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "        \n",
    "        correct = correct + (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a7040-957d-4f2a-9b0c-f42864cef56a",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb269b-2cf5-4104-baf3-3e1bd24fa114",
   "metadata": {},
   "source": [
    "Now we can convert this to ONNX using PyTorch's built-in exporter. \n",
    "\n",
    "To do this, we need to provide a sample input. This sample input is passed as input to the model and the trace generated will be used to construct the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5391abd8-c0d1-4910-87bc-372d760da611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "sample_input, sample_labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "onnx_filename = \"handwriting.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    onnx_filename,\n",
    "    input_names= [\"images\"],\n",
    "    output_names=[\"number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ecc8f-3cab-46e7-b6e2-084b1aa351a0",
   "metadata": {},
   "source": [
    "## Check ONNX\n",
    "Let's just check that the ONNX conversion worked before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e8cab25e-2e3b-4edd-a6f2-6ada65cb67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fc577c33-5f28-4581-84d5-84c0f006299a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAp0lEQVR4nGNgGL6Azbfj701uCJsFRUZVy9NdnoGBlQVDj6pR85d///79+/fRBdPAPf/+/bs/IejZv34stnn19VmzM0z+t08Ul3usP/zywenY6f+acMrV/7slhUsu/NP3IJwaD/9rwCmX/ueeBC459uP/gnFqnPZvMydOjU/++ePUOP1fEyMuOd6H/7xxalT99xEzpmBgJh4/8t//hzvEU/5dwymHFQAAo2w5cVm7kEIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5908B1220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABA0lEQVR4nM3RIUtDURjG8b/ebXiFBU0LJosMZSIDucExMGhSWbBatGgQxOSH8AMITg0Dm4zNZtaiuOKKa4JBGbuKwvByH7Xcu3DOjQbf9Jzz433PCwf+cZXC75+j7URabHxKUncjwWbfJD3dSOHu4G44DntZoL1eaw0VHKtzS1Knk8c900HaRE+hnwFw6po0x8LHWACgw8Aa68mP4/u53ZmeidPqnIXuWhQeR6YtJBcdqmwaGPTZSRl7DPD+EiJMEZrrjt/qOgtQ6mvefLN3h7cEOPsZayw8QAXc0xWefcxyjvW6zISksmUw9aWX5pV0MpqAdCVJdetTACjWJF0sJNof1i9TQ1i8SMOqdAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD58F3D4F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8klEQVR4nGNgGIRAZvqfu9P9dTElGBkYGOSWWjMwMPw9zcCw7vZGDBUsM4qW37rz7/+/f3+/tnFjM5zV2bn53b9//zxx2e536d8TFexSCp3v/z3VxirFGHr33797eljldBb8+/dvLxNWObdP//79+/f+aB4nFsnX/56dOvXq379/p4LYMCQNTGQYGBRiVv/7928qducyMDBqnv/8754aLmmGim//buOUZCj58i8Vt2zSv6MQBorH/HwhLofxWVAk/29mYBBNzmHYhUWSQddcWS1NguF9HxbLWv/9+/fv3789vNhcwhW+/N+/LSn8uN1KFQAAn05ctXqskWwAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD563A9AE20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nN3PMUtCURjG8eeKlxAKEdOhLyC05CBBhIurfoMgEBsK16aW8Au4tVo0B0FgBA7i4OTYFBJ3MVq0JbyG8CcHC73xni/Quxw4v/c8PEf6N+P9nLmCJJ3c1AuS9+1dn85XO5vPRCcpSbEl+rvRwN58DT+bkj6CIAjGkvTVmq2v1uk1DiWpBnAezUkVz5bF3mA62LHb3wEXjp+VQyBvW7UDPKZNy3aA+7hp/gPQzpi2/QS0HWVKQNd+p4N3oGrb/gi4jNnYB4Z7th2H8Jr7c/mbs7UhXb04qlYmMM06UEdwm3ChNQsDA2cZ76svMAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5908B14F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCklEQVR4nMXOsS8DYRzG8eeoScRSEU2IwSoxiUXTBJMYJSYDBgMGDD0MFrvFn6ESCxqhS8Vm6EgqkZoM0rucNJLvvQa9at87K7/pefP5vb880v9O2r0LS25fMq4AId7hYJIFQAiU52O25gMUTgMIZi3LeVAp7koTVdixcBOehyRJqx7+XIf1P0Gpmbfg0v74ONzM0x+8dGAN9qKcusXLSlLXj9+3umXVOy5Jqe/3RkZyWmuO83rShjKSiWzKmFDW2Wj2D6SHX3BkvUfVZRvHJEnuTUY6rrevLzbgfUndk9efwNVA57EaUC+cY4CiZTp6AwCDv52O9VjwAWhczMT7S6P5Cmf5XBL95XwBKd194Mp4d60AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD58F3D4F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/ElEQVR4nM3PL0tDcRTH4S9DhyhqECYIwyDDMqZt0aIu7R0YpsMs2AYWg8FgEg0iaDMoqMEgBtlwcYwFwbBhkM3kRK7Cde5+ZlTu70SDJx3Ow/kn/fvIXQMXJq14AUAw70h//gXu1tOvePGwjdagvRTRBtyEbaxCdy8t6ZLPuZDFTvlYlqQjn7NwY5KvgqSBTZ+6s3GN5pCUqkJxwjl1Ae6ji+DvDrofjmzBEzSnXZI0XKTHc/RXJfKTBg/qaadjNs6WAM7NmftvNLK3Nmahnph859iwqRaV8b5DSBmYoZWYqRKUramrPBY8uifmqRmAzrZpil3RPkja9ufxDXAveBwR/Hh+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD563A9AE20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+klEQVR4nL2RP2sCQRDF3wUxVSrLQ7TxiliY0s4yTUoXrPIF0qRMG4LYKEiwEHs9UlheHQIhwU9wiMEmVUKKQCzfYy1O8cytrVMMu/PbNzt/gOObt3e7qnij1aGnT1RkjHGhwuSbIslOPsOKEZlA+RmYxKcuePZspd8ugqXUyySl+HUJGJLX/+Fp+3VQBe5i8sPdSq4ocl5xwxuKbDpR0JdsXHLrHkjqPP3L7vh2AeDlBED5LwDeU4MvtB4tAO9zBtR+6nuycryZ3NZNU7UsJatxo2slyWpc2zGzIKn7POAPwzD0/fRaRJL17KK2cO7oPan2FogWbt2xbQ0Gt4b95WYnVgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5908B15B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAp0lEQVR4nGNgGAzA/d8xAVxy3Lv+/lXCJdn/9+8yNiibCV1SjIHhwy8cGhUf/v1riUOOpe3v382sOCRd//79ZY/LOaf//t2FS67xx9/P7jjk5F79/VuES2PH37+3JHDI8f/4+y0Xl8amv393oAgghRC7DbpqJEl7ewaGEzgM5dnx9+8DBRySyn///rXAZWwCA8O7e7gkNRkYZr7CYSpD0t9tzLjk6AgAwF03YCD57iAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD58DA54E80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0ElEQVR4nGNgGDDgyYxbLu3vTQ6ckvv+BTPilPz/D6tGJgYGBgaG8/jc8xW7Tgj4d5UFzs45eWdnKh+ynQvhTKGn22oP/HtghqQzCs5U/mfBwJz3+Q0v3EGMPCi2/J3UKxSI0HkFbqf8jwQGBgamA8vhklP/2TPB2KfvSDMwMGw8CpcUefNvrQxU2vVnFrt0z+taBgYGaKiJzPBmP7ruO8Ono54MpYpXdH7OLEB2hNbUV3/+/fv379+/Px8/vnGBuBNJ3omdgYGBgeHnPoYRBgCxEkSyopoWLQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD58F3D4F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAkUlEQVR4nGNgGKLAZfbf/8+nWrAjRJhgDMlL63gidGMZj4pj0ccdycrAwMAg8FcOt9kokky41WGCsn3MuCWvFeCWs3zLisRDs9P/zm+cksz+L3Gbqv8nEbdk5iEULoqxzAa/cGs0+pvPjVNnxN+nkjgluXfq3EHmsyBzGKXx2PnglwRuyV8zcMt5vzTGLUkaAADHlyGCNEfJRAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FD5908B16A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_filename)\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"images\": sample_input.numpy()},\n",
    ")\n",
    "onnx_processed_results = [numpy.argmax(res) for res in outputs[0]]\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(sample_input[idx]))\n",
    "    print(f\"{onnx_processed_results[idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e2af0-ee1e-498e-8293-0bc41834aa4f",
   "metadata": {},
   "source": [
    "## ONNX To TFlite\n",
    "\n",
    "Our ONNX model seems to be working correctly, so let's continue to convert our model into a Keras Model. From there, we can convert it into a TFLite file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99dcf-4503-4e8b-acc9-5e06be6a67fb",
   "metadata": {},
   "source": [
    "### ONNX to Keras\n",
    "\n",
    "Here we use a library by [@PINTO0309](https://github.com/PINTO0309/) to convert our model into a Keras model. For full documentation, see: https://github.com/PINTO0309/onnx2tf\n",
    "\n",
    "#### Important note about Parameter Replacement\n",
    "\n",
    "`onnx2tf` converts PyTorch Tensors of shape BCHW into the form BHWC, as is convention for TensorFlow. It usually manages this fine, however, if it gets the shapes wrong, you can use the `param_replacement_file` to reshape an operator's input's our outputs to correct it. \n",
    "\n",
    "For example in this example, the `squeeze` after the LSTM operation was changed to operate on the 3rd dimension, but the output of the LSTM output was not updated to match; therefore, we add a `post_process_transform` to the LSTM operator to make it work. For more details, see: https://github.com/PINTO0309/onnx2tf#parameter-replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7d5d4beb-6978-4dfb-ad1f-a39415f80c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n",
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n"
     ]
    }
   ],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_filename,\n",
    "    output_folder_path=\"mnist.tf\",\n",
    "    copy_onnx_input_output_names_to_tflite=True,\n",
    "    non_verbose=True,\n",
    "    param_replacement_file=\"param_replacement.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36939950-ceb7-47a9-99e2-0609a2d8c37e",
   "metadata": {},
   "source": [
    "#### Check Keras Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b8e67-e2f2-48d5-83ad-08cfd485db28",
   "metadata": {},
   "source": [
    "Now that we have a Keras model, let's check its accuracy. If all went well, it should not have changed significantly from the original model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f797340-539f-45bc-97ba-862fcea5e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_correct = 0\n",
    "keras_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tf_input_data = np.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    keras_output_data = keras_model(tf_input_data)\n",
    "    processed_keras_output = [numpy.argmax(res) for res in keras_output_data]\n",
    "    \n",
    "    keras_results = zip(processed_keras_output, test_labels)\n",
    "    keras_batch_correct = sum([1 if output == label.item() else 0 for output, label in keras_results])\n",
    "        \n",
    "    keras_correct = keras_correct + keras_batch_correct\n",
    "    keras_total = keras_total + test_labels.size(0)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy of the keras model on the {keras_total} test images: {100 * keras_correct / keras_total} % ({keras_correct}/{keras_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb30033-ad26-4568-90f3-f85c064f29d3",
   "metadata": {},
   "source": [
    "### Keras to TensorFlow Lite\n",
    "Now let's quantise our model to int8 and export it to a TensorFlow Lite flatbuffer.\n",
    "\n",
    "\n",
    "#### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model for more details on this.](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "61ff7a3f-2826-4a3f-8a77-e9b93a68dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for test_images, _ in data_loaders['test']:\n",
    "        tf_batch = np.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1043d1fd-b8c2-4184-9305-4a6f22e36549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = 'handwriting.tflite'\n",
    "with open(tflite_int8_model_path, 'wb') as f:\n",
    "  f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b447d46-da8b-44e6-88a5-e88fcb538d20",
   "metadata": {},
   "source": [
    "#### Check Accuracy of Converted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ae7bc907-7baa-484d-9314-b49a64b32917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the TensorFlow Lite for Micro model on the 10000 test images: 97.98 % (9798/10000)\n"
     ]
    }
   ],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "tfl_correct = 0\n",
    "tfl_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tfl_input_data = np.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tfl_input_data)\n",
    "    tfl_interpreter.invoke()\n",
    "\n",
    "    tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "    tfl_processed_results = [numpy.argmax(res) for res in tfl_output_data]\n",
    "    \n",
    "    tfl_results = zip(tfl_processed_results, test_labels)\n",
    "    tfl_batch_correct = sum([1 if output == label.item() else 0 for output, label in tfl_results])\n",
    "        \n",
    "    tfl_correct = tfl_correct + tfl_batch_correct\n",
    "    tfl_total = tfl_total + test_labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy of the TensorFlow Lite for Micro model on the {tfl_total} test images: {100 * tfl_correct / tfl_total} % ({tfl_correct}/{tfl_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec22f2d-c1dc-41b8-a537-22aa8c052bfb",
   "metadata": {},
   "source": [
    "### Check operator counts of converted model\n",
    "Let us take a look at the operator counts inside the converted model. Ideally, we want the same accuracy with minimal operators. This uses a helper function defined in ../utils, but this step is not necessary to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "36764ad8-6643-4227-98e6-fc37e291a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPERATOR              COUNT\n",
      "-------------------- ------\n",
      "quantize                  6\n",
      "reshape                   4\n",
      "transpose                 2\n",
      "dequantize                5\n",
      "while                     1\n",
      "gather                    2\n",
      "fully_connected           3\n",
      "less                      1\n",
      "add                       8\n",
      "split                     1\n",
      "logistic                  3\n",
      "mul                       3\n",
      "tanh                      2\n",
      "concatenation             3\n",
      "slice                     2\n",
      "-------------------- ------\n",
      "TOTAL                    46\n",
      "-------------------- ------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import utils\n",
    "utils.print_operator_counts(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259ccf2-5d14-4b03-8288-a28a8e77c954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
