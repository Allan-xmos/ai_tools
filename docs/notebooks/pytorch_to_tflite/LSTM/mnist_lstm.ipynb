{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53aeff71-1811-4956-95db-13c0ad7f276d",
   "metadata": {},
   "source": [
    "# Training and Converting an LSTM Model from PyTorch to Tensorflow Lite for Micro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a794d61-a6ff-42b3-bc1d-849c2dd2ab75",
   "metadata": {},
   "source": [
    "## Training in PyTorch\n",
    "\n",
    "Let's train a hand-written character recognition model with the MNIST dataset which uses LSTM models.\n",
    "\n",
    "Start off with importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c8b314-8eba-4b61-a11e-80014415d78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0+cu102'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517f9774-2b1d-4903-a089-1fd032b6532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\"\"\"\n",
    "Each data point is of form [PIL.Image, class] where the class is a number 0-9 of the\n",
    "respective digit. The image is a single channel image with values from 0 to 1.\n",
    "\"\"\"\n",
    "def mnist_data(train: bool) -> torchvision.datasets.MNIST:\n",
    "    return torchvision.datasets.MNIST(\n",
    "        './mnist/',\n",
    "        download=True,\n",
    "        train=train, \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "# iterator returning Tensors of type float32 and shape [batch (10), channels (1), height (28), width (28)]\n",
    "data_loaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        mnist_data(True),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        mnist_data(False),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f54c38-9bd9-4b43-8176-603f1c151256",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2670a68-ae13-4d9e-b9de-297cf74f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):\n",
    "        super(MNIST_LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Remove batch parameter\n",
    "        x = x.reshape(-1, self.input_size, self.input_size)\n",
    "\n",
    "        # Set initial hidden and cell states \n",
    "        hidden_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        cell_states_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, (hidden_initial, cell_states_initial))\n",
    "        \n",
    "        dropout_out = self.dropout(lstm_out)\n",
    "        \n",
    "        fc_out = self.fc(dropout_out[:, -1, :])\n",
    "        return fc_out\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5304fc-83cd-4bae-8334-5de38e890f56",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f4496e-63dd-4753-8a90-004351b15674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=28\n",
    "\n",
    "# instantiate model\n",
    "model = MNIST_LSTM(input_size, hidden_size=128)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a154b68-b258-405e-a2eb-a03af2b7361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, model: MNIST_LSTM, loaders):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    print(\"Ended epoch: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4adfd31-a7c9-42c8-a604-f39afaa5d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Ended epoch: \n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=3, model=model, loaders=data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6161c20-3be0-4a6b-af5f-e375cc2d7ae8",
   "metadata": {},
   "source": [
    "### Check that the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8748974-a7dd-45f2-a57a-b3414b383733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+invDLEEMkbpvXcu5SNw9R6jimkFSQwII6g0ley/Cn4daDc21l4l8VajYm2nk2WenyTKPNfcVG8E88jhR17+lO8WfFfxT4d8R3+jXmneHri8spDHFdrZktGpGVCZbjAPQj868p13W77xHrVzq+pOj3dyQZGRAgOFCjgcdAKteEbnRLTxTYzeI7VrrSNzLcxLnOGUqDwQeCQ3HPFes+d8H/BbRa9ot5dapqlvmW0t/MfBfnbu+UAAe/Psa8W1TUrnWNVu9SvH33N1K00h7bmOTj2qpRRRRX//Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0ElEQVR4AWNgGL6A26KYA+g7NlVuBgYWNG+yr3FjkHjEaKSvt9MLTYpB7/y/v0AAJtB1Gu0QeTPdyJuB8dqUzw/QNIY8//tXl4FZSEgIZC0K4Kt8BzSxQQxFEMphWg6xbjI2SWOQM649/ncMIYnwyu0djKfX3jgi8xwhicZK//svEiHEBGHGK0NoYYbbGxCSUGPnf645a8z4XzecYd13hCSUBXEo0Cd7MKQYGFb9A4L///7dkMIiyTrzKyhEF8ihyDHCePI+IsfuP/wN4w5OGgCoK1pWsy9X3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+u58IfCfxL4z0w6nYiztrDcyi4u5SisV64ABOO2cY6+lXte+CPjDQLa5upY7K5tLa1e6lnt5/lVU5ZcMFO7HPTBHfPFecUV9CeGPDi/Fj4R+H9O+0tYro2oGC5I58xAmSVGPvYdMZ4HzdelZ/xW+I9npugj4e+GnlaKzjWxvLmTrtjATyhxyflwx4HGBnPHhdFfXXwp0ptA8A+HItOtkuIdSBvL+5EwxEWjyOOpOQiYHTByeOfmTxxbWNn451yDTZEezS9l8oocqBuJwD3A6fhWBRWpD4l163046dBrepRWJBU2yXTrFg9RtBxWXRX//Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAoUlEQVR4Ac2RURKDIAxE154snqx4MvRkNJuMsqD97Ez3I4Z9IQEE/lhWaqMMxeZjWhCG2toM3QqfnxkWJ6wPFhleuv+4FltkA6RjDNvOKOJ5gMqu2VQY4EPtGwO3UmXYci6S2bkElp7CR7rEuZ12lWqBFnZGqci05sybT8Nvwhf3f3JJ2gI7Bz7CN3AI8DJRThzadsqZrtIdyeJdq4nzq/QDsGlxbJTmYPMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+u90P4PeLte0WLVYLa3ggnXdbpczeW84IyCox0I5GcZHPSuGngltp5IJ42jljYq6OMFSOoIqOtjwpZ2+o+MNFsrtQ1vcX0EUqn+JWcAj8QcV13xh1jV2+JNzbztJbRaaypYRJlVjQAFXX3PXI9h2qx8VFt7/QvCniSfTxZ63q9vJJfbCdsuzaqPg9CRz+POa8xp0cjwypLE7JIhDKynBUjoQexr0hPjBcXlnAviDw3o+uX9uMQX11CBIPTdgYbHHp/WtH4lajc+L/AIY+E/FlzhZ0mnsp1CbVLEkgqPTEZ/lXklFFTve3cllHZPdTNaxMXjgMhKIx6kL0BPrUFf/Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGLyAtX71jTmu2N2nffrv37//Pqlglf339++Hdff/rsQm2//v72FxBp4lf1dianV5/++EEFBY+f6XYAzZo/8+uoAFZ/77d5ETTfrV32kQEZ4df/+pQyWZILQkG8MZCOtLAVQGSEEluZkRQgwPXkM5UMk73xm8oSLeDE/fIakEMV//+58MFtI8+e8ImhxDz49/jyOArrS7/PfvYXRJhqXAELq8fsNHYBhiSnJo3QSF7atJMxCSLDAjflxzsnVmYJh8qYNBV+4RTBSd1oA5DV0CxGc78m8rVBzqTyRVv/YwmOCUZADajFty68tGmCxtaADzol9RaxpOUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APC9L0u91rUoNO063a4u5yVjiUgFiAT346A16bp3wI1XyoZfEWuaboYlJ2xyuHkwPbIB6jjPeua8a/DTWfBNtb31xLbXum3LbYru1YspOMgHI4yMkdQcVxdWtNlv4dRgl0xrhL1G3QtbZ8wN/s45zVnUNJ1xJJ7vUdP1BXLF5pp4XHzE8lmI65PevUNXhn0T9mvTbe8m86TU79ZoI5cgwxkFhtz2+XPHH7z8/HK9L+CXiHS9A8Yz/wBpSQWzXdsYba8mXKwyEjGenB6Hkf1HsWmX/ijwhLLqfjvxppDaSzM0MSQgvPnpswFIxxwA3X8a4D4gfErwR4/0O8trqy1CDULLzDpdwACHJx154DYGQRwB1zXiNFFFFf/Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABHElEQVR4AWNgoD9gRLLSXv9/umbhg01IQlCmgueGt3///Pnz99PNNjTZpq2XQRJQXMEKkYYau9UdxC0AETUiDIxqd0EsGCj/c1YEyta6++dvEkwcTLMqw+QYGCz//N0MkWSBUL+RzHkD18YEZyEz1iFzkNkKV/7+V4YIYOrU1Pj/D6oYLikA9RuDKAPDh98QWaiDMtX1H3xkYPw/8zpDNQPD4kfI9rT/BYP/QBKEkaUY4r4Dw23vPnjweXIhSef++fPcnYvb3ePwR4iCqdwIWaBkDpgHCjpw4M9CSDLc+vs3l5/BfjXIzvLM8///3kSSjAUqv3cTFJ+XylkZRJTngyMJqkA05g1Q+s/fOwXySFrgTLuVQMkcfjh/wBgANeWRqQRsXNMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+nRxvNKkUalndgqqO5PQVv+K/B2o+Dbi0t9TltTcXMXneVC5Zox2D5AwT+Nc9RXf/AAf0CTV/HVvfs6xWWkf6dcytjCqvI6+/8jWJ488Rp4s8aajrMSMkM7gRhjzsUBR+grm6K9a1iF/h18JLbTY5Cur+JgJrrIw0VvgfIPrkA/Uj0ryWilBKkEEgjkEVb1HVtS1eZZtT1C6vZUXYr3MzSMq+gLE8e1U6K//Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxElEQVR4AWNgGOwgd9HfDpgbWWAMKK1nFfr4KZoYnJvy7s9qZTgPlaH+5c8fU7gQE5wFZhixMlx9AhdCk3z+Fy4DZKBJRjIxrPuBLI/Edj/z54A9Eh+ZyTHp758gZAFkdsifv4d5kARQ7LRnZJrwBUkSmemy/88pdmQBZHbUnz+hyHxkNu/eH2uQ+chs/sQ/33B5gyHoz58eZMVANsK1GQwM99Ak4VztJ3+WqMN5EAZcZ6oEw/ubaJIwrvbjP891YBwa0wCudzMSBdPn5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+t3wr4Yn8VXt7a28wjltrGa8AKFvM8sZ2DHc5xmsKiivTvgkqR69r99LkRWmiXEjsOw+XtjnjPpXmNFFepfCe4jg8LePwk0a3baQ3lxyEAMuHDHr2yPzry2iiiiiv/9k=\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjElEQVR4AWNgGGhQs50ZtxM2/dNElmRC5gDZksh8dMlYfJLIcgzoOnFLMqIpReGqOqNoRDX24Qk8ksoODE9QpZF4Wv/wBIIuI5JKIBPFQZ7/8UiiSqHpZGD8/htZAbKxrEL/99zBJSnsgywDZCPrfLuNQZIPWR5Z8vcbBhNpZEkUtvztbXiSCYpSmnEADvUVeZpOj9wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+nRxvLIscaM7sQFVRkk+gFPubaezuZLa6gkgniYrJFKhVkPoQeQaior3bT/Fmi+Cfgtb3Xhh0m1hrmKK5uBB0nZfMYNvByoUFeMe2Oa4T4tRtL4xg1ZxCjaxptrfmKLpGWjCkH8UJ+hFcJRXReHPEtvpVleaZqmmnU9Kunjle2+0NCVlQnawYA44JB9Qfatr4qSm71fQr7ykgS70K0mSCM5SIFSNq55AGO9cHRRWnrOuXOuNYtcpChsrOKyj8oEZSMYBOSefXoPasyv/Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA00lEQVR4AWNgGJ6AhY275IMvdr9lnb92/cv7VVglA7/++/f/3797HFhkrb/9+7dXofXfPwEskgn//v374s2Uj02S9RlQsoeBweBfKqZO3a+HX37jY2DQ+vUKU5JBkefAPqCw+o9/cEkmOOv+lwMqnAwMnGy74EIISaCQjCVI/BtWyf5nNgzs4Qyn4ZIscBYDw8eFOd9TVRg+IAkhMXlWf7jl+2QdkggKU4yD4T5CEtlYoDKQHyVRlKNy7mPzJ1zJXTgLxZ8Q0cv4JOFyDFh0IiTxsgC8BkGJmTudBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+nrFI6O6IzKgyxAyFGcc+nNKYJViWUxOI2JCuVOCR1ANR11Xw90Sx1zxSF1Rd+nWdtLe3Ue7bvSNS23I55OOnbNV/D3jjXvCl7eXOg3aWP2s/vI0iV0wCSAA4OMZOO9ekeBPih4y17X2Gs6ray6FawvPqQntYgnkYwR8qgkkkAAdSQDxmvHtQmiuNSup7eMRwyTO8aAYCqSSB+Are8BeJLbwz4jNxfwtLYXVvJZ3YQkMIpBglcEcjg9fy61eTT/h9ptsJrnXNT1mfKsLaztfsykZ5VnkyRxnoP8ay/EfiyfXYobK3srXTNJtzmCxtVO1Tz8zMeWY55J64HFc9RRRRX//Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2UlEQVR4AWNgGJ5AK4cNCAyKNdjYmNB9KHr3HxxkSSLJqroAOa0/4JL/UhCSXh9mgziS0lDw7d9uhOTOnwg2iOX9LxNIQix2Nt2GIqmixMACF0j91wxngxg2f68KwwUO/nOHs4EMrRMfdUB8iLGXGG4iS0qYlT9B8A/+80dwsl+3CzCCuRB7XzNchEk6pbmtuPMBxgPRy/9dgHLL/vxbzw5lQyn116cYGHhCO/Ye+7NWHyNYff+uW3fhwb9fFYGsSNogNjPw+lv7vjp5Z+sNJClkJgszMm9wsgFZoU/ju6U7SAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iuru/At7p/w+tvFl7cx26XdysVraOpDyoQxMg9BwMccg5z0B5Siu3+FfguTxn4xt4ZYN+mWpE96zA7SgPCZHdjx16ZPanfFXxe3ivxfMsKmPTdPJtbOHsFU4LY7biM/QAdq4aivaNS1GL4c/BWx0axu1Ot+IVW7naKTmKFwDkY/2Qq++WIPFeL0UUUUV/9k=\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApklEQVR4AWNgGBwg+S9ud2R++lOLS1bg3p8/mjgkGff/+bMChxzDhL9/ruOS83z797k7DknTF3/+zsMhx+D5589fZVySt/7+DcEl1/79zyVccqJ///zJQZJkQmKL7kTioDO3/f1/D1fAiZ7887cLXQOMH/fnz1s5GAedvvnnzwR0MRg/7+9fnPHI+OLPnwaYQnQ65M+fs/LogjA+459LOOVgauhBAwAd6T1Qz9ZuxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+lVSzBVBJJwAO9dB4q8Hal4OksINWkthd3duLj7NGzM8Kk4G/gDJIPQnoa56iuz+FXh7/AIST4i6XasEMEEn2qYP0KR8498nA/GqnxF8RDxT471TU45JHt2l8u33jGI1+VeO3TP41y9Fet+C/J8H/AAf1/wAW7ymqam50yyYdUX+Ij3+8f+ACvJKKKne9upLOKze5ma1iZnjhaQlEY9SF6AnAz9Kgor//2Q==\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAs0lEQVR4AWNgGOyAfdP/GzjduOzvNy1ckqYf/n6EyzHBWRCGFS/DCjQhBPfw37/SCB4qy/T735mMcCE0Y7nYGL78h0uiMlh2/v3rjSqE4Nn+/buOE8FFZS34+1cTSQTNTiQZdKbIu7972NEFYfzmv39dYWx0muf338Mo1iBz6pgYdv9D1wHjf/77zQ7GRqf9fv3diiqGZKwsM8NKVEkk3sy/L3mRuEAmks5b/6I/o0rSiAcAykow0klsrLMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load one batch to test\n",
    "images, labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "results = model(images).tolist()\n",
    "processed_results = list(map(lambda res: res.index(max(res)), results))\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(images[idx]))\n",
    "    print(f\"{processed_results[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687d372-cd62-4c01-9cd8-30cf67065bd5",
   "metadata": {},
   "source": [
    "### Check Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756e71c9-c34d-4d83-9cd2-ff8b75f040df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.69 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in data_loaders['test']:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "        \n",
    "        correct = correct + (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a7040-957d-4f2a-9b0c-f42864cef56a",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb269b-2cf5-4104-baf3-3e1bd24fa114",
   "metadata": {},
   "source": [
    "Now we can convert this to ONNX using PyTorch's built-in exporter. \n",
    "\n",
    "To do this, we need to provide a sample input. This sample input is passed as input to the model and the trace generated will be used to construct the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5391abd8-c0d1-4910-87bc-372d760da611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:3220: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "sample_input, sample_labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "onnx_filename = \"handwriting.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    onnx_filename,\n",
    "    input_names= [\"images\"],\n",
    "    output_names=[\"number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ecc8f-3cab-46e7-b6e2-084b1aa351a0",
   "metadata": {},
   "source": [
    "## Check ONNX\n",
    "Let's just check that the ONNX conversion worked before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cab25e-2e3b-4edd-a6f2-6ada65cb67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc577c33-5f28-4581-84d5-84c0f006299a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiu58D+PdP8HabdJJ4YsdTv5ZgyXNyASkeMMnQkZ9vXmte5+Nd9JcSNbeFfDcUDEYjey3naOgJyM4PPQV0tpc6d45+GXijXtY8I6Ppws7Yi2vrOIRu9wqnaBgZxkxg5OOcfTwqiuo8B+Cb7xz4hj0+2zFap891dFcrCn/xR6Adz7ZNb/wASPGltcRReDvDP7jwzpn7tdp5upAeXY9xnJHqcnuMecUV1un+LNS0n4dXui2PkwQ315/pE6KRK6hR8u7P3ePTufWuSor//2Q==\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA80lEQVR4AWNgGBmAEexN1+Dzf1Wf3teX3yT/ULsDxef1n/7+Q4Dfnz69mICQP/ofLHXn0f2z9++/+/fv7L+/S8GyIGNnpu5Z4Sa8dA3r3098DLy6LozZbAxMML0CoZIwJoTu//cPVQCJp/Tp3x0kLgpT+PS/f+0oIkicAKDzfJD4yEyBF//+rWRDFkFiL/r3b6McEh+JyVjx999VKSQBJCZT2L9/D0WRBJCYjGb//v3VRxJAZioAHZqCLIDE5tn5799WFiQBJCbvjn//XqoiCSAzM//922mALIDE5rj374MCEh+Fqf3jTRqKAArH3R6Fy8AAALHtY/mgcTMaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tvwr4cm8Ua2thHIYIEjee5ufKaQQRIpZmIHJ6YA7kgd6xXCh2CtuUHg4xkUlFeuQWkngP4LzytDcR634r2xxbCD5dsCDyO29SRjr846YrkvEXg2Lw14N0bUL67ZdY1NjMtljHl2+OGbIyGz79/Y1yFWLGeK11C2uJ4BPFFKrvCxwJFBBKn6jivTR8WfFPijx1p0NpqC6Pp0t1FDFbIEMcKF1GWLAbjj6egxmsP4va1PrXxL1ZpnDR2cn2SEKchUQn+pY/jXDUUUUV//Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6UlEQVR4AWNgGHBg/N4CtxveP1TFKanwzwpZjgmZo7yDwRCZj8Je+fefNIoAEkfqwv8+JC4qM/PvXwFUEQTP+ePfv2De8WaoIAtCUonn7wwgjyXD/CFCEMZ69HcSiJnz9/8KmBCc3vjvXyiIs+r/WQG4IJRh8O/v3Tg1Bp/SL3/j0eXYl/3/+/fvvePfgSRcDhZCpeH/vx8JP3LmMwPDQrgkjLHq7/8oEDv071sjmBgD3CuMgRtAgoxM287BJWEMYWigXv4bAhMCehnKfAul+RFSDAwwB0HFHFAkkRWC2JPv4k4K6GppwgcAGdZFN3hYswUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+vT/h54E0q98La34s8WQzLo9rAy222QxmWTByV9cHAHYscc4NeYUVt+D9Jttd8Y6Rpd47JbXV0kUhTrgnoPr0z716D43s/Efjnx7P4S0CxZdJ0aQWsFvEuyC3A4Luegzg++BwOudXQ/hB4PuLm60S78Tz3euRxF5Gs4v3NpjrvYgg/iR0rxB1Cuyg7gCQD610HgXWdO8PeNNM1bVbaS4tLaXeyR/eBwdrD1wcHHtXuE/jb4WzaHqNnD4k1K0XUrtr68WCKYTTM33o92wgKcAYBHHGa878SfErS7TRrrw14C0ldL0i5BW6uJBunuAQQRkkkDBPUk/Tv5jRRRRX/9k=\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABH0lEQVR4AWNgoApgm/rPD6dBVX//3kWWZELisLozMFxF4qMw/f/+/WuKIoLgiLz9+3crgovCEjn09+8jXRQhOIdxwd+/s/nhXFTGXKAcqgiCl/P77yw2GNfOTx7GBNFyT/+eZYcKWLd//3vNF0m28d+/Dgg3dfc/MFAEclkgQo//M1wAszaZiP9nOCOkBOZAQ+g+RI33ER/xm5P1HedDuFDS9tffCAaBE7///jnlxcCa9/7vSqRw1Xn305UBGApPgQ5hTf7794E4kt6sv88Z5D/8PQD0gvJ0YBCvQpKT//73U9WKv38DuDpffPz792sEJ5KkElAxCNz/+P/v3187xZCkGBg4+z5AZP/++3u/EEUKxHHvvw6W7i8SwZAbIgIArBOB2AQnpe8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBbS0ub+6jtbO3luLiQ7UiiQszH2A616JofwL8a61bGeS3tdMXjat/IyM3/AAFVYj8cVzXjPwLrXgXUIbTV0hbz0LxTQMWjcA4IBIByOOMdxXNVueD/ABLN4Q8U2WuQQidrZmzEW27wylSM4OOD6V9N+GPiX4U8U6RpranrNrb6nuWV7aSVoNkuThRyAwHbJORgmvL/ANoWXX5tfsftlsseiRofsUkT71kY43FjgYbGOPQcE5NeMVf0SXToNbspdXt5LjTkmU3EUb7WZM8gH/P1HWvW/Evwh/4Sq8fXvAN3pVxpM6rstYnMZiIUDHOeTjJzg5J4rV+IMMnhb4D6Z4a1u7gm1gyoEiRlJRQzNx3IC/Lu9TXgFFWrHU7/AEuXzdPvrm0kyDvt5WjPHTkEVFc3VxeTGa6nlnlIwXlcs35moq//2Q==\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABFUlEQVR4AWNgGLyAM/Tyv3///x2qxHCitErP6b8Q8MkNVZaj9SVUBkTd4EeRVYFKnd72CcSCaWUCK/ryBEy9nXOcBci4dRxFJ8Pqv39n/YBq/+gIk4PoZDh2M5qHFSL2d9N+mCSCvgbV2IYQQrCi/kBkf/bCxRjhLAZPZi8dIU0Ghu/Ly94jRJFYQumXgPrrBJCEkJlC04CyiRARqGsZFApXQFz77g1QwhNZOYPinb8/uCEiHMf//l2ForNWkeEmF0TkP5RGaBbe+/fvpRQhkEAU0M4pEBmYV0z38DAwXHl28qpiqhLDB6ubCH0glvYBoA4ouC2JKsfAwB97ESL3qUkKKgczFsTlCtJWCZrybecBqBytKACqDZNpdxphMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uo0j4c+Ldc0wajYaNK9ofuSSSJF5n+4HYFvwzXO3dnc2F3LaXkEtvcRNtkilUqyn0IPSoakgEbXEQmYrEXAcjsuea9D+LsOrXPj5oY4biTTlihXSViDNGYdi7fLxwckHp3+lc34y0fXdG1S0i8RTeZfzWcUu1nLPGmCqq+R94BenPbmucorodN8deKdJ08afYa/f21oAQsaTHCA/3fT8K3vjFubx0rkySI+n2rJO5J88eUvzgn1OR9Qa4CiitLUdf1PVrDT7K+ufOg0+MxW26NQyJ/d3AbiB2BJx2xWbX//Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4AWNgGKJAfOO/f//nqGNzPXvVq793rm99/bEGi2zY378tnAwMsi0vTTFlV34vYAaLZhthSPK9ewUVY2OCS8JYdvzXoGK//mFIMjCcgYvBGTCdDAwWEDHmyRe54bJQBvfjzxaMQDZr9t8r7OiSDB5//6bYSNrM+Pt3NYYcA0PIq3/AEPry/l8EXBJkFBSIOKgxMKwI6HbbCxNhgTEYGN6sAbF/MejCJRGuhaqyRahmQJdk/4vNw1ANok/+CsH1out8PZ/BGS6JwQj+ewpDDC7AeuVbApyDwfD49lEGQ5BUAQBYnUDDnXKL0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+t7wh4S1Lxpr8Ok6anzN80szA7IUHVm/zySBW945+FHiDwS9xcvE15o0RULqCBVB3YHzJuJX5jjnjOOea4OivoeLxhpukWdj4H+EsaXGp3bgyag8WUXIy0hLDlgOuQQoGMHoMj45eJvsmm6Z4Hi1GW/ntkWfULp2BMkmDgMOSD1bGcAFevbw+ruj6ZNrWtWOl27Is15OkCFzgBmYAZ/OvorQNS+HHwcmj0qXUftWsz/u7+9ijMpiI5KttzsXPG0ZbOM9MjzjxT4d+HdhpF9fw+Nb7WdXkH+jxxqGLyE9ZCR09TkH0z0ry+iiiiv/Z\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlklEQVR4AcWSUQ7EIAhE6cmWo+HJ8GbuDBLTBt3PLkkrnecAmor8P2z4cQgbY+iBKhjCdzwZcDU7XRFWoE9911aDwYK1GIOpCPsWCC1ELPWkeQAHlBIatThNNc7NbPiTaSkaAn2bhsHY7wmvVSRvu0H4aLOlI5kXFF6+nlM5JePdMZJlWcVnbx0lDE9ncovzD3Db9Eb6BSjCh4kCFSB+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+vTdE+BHjLWtOivSLGwSVFdEvJWVyCMj5VVsfQ4Ncf4s8Iav4M1htO1aHDdY54w3lTDAyUYgZAzg8cGsKivefAvxI0vxZ4cTwV4wuprSXyxHb6ks/llip+UlyflccYJBBxz6Hofj1pcafDSxmmgW8vbW4hiN86YkVdrBmJA6MQMjpkjuBXzLXZ/Dj4f3Hj/WZrZbtLW0tVWS5lIy20nGFHqcHrwMfhXqEepfCH4Zu76eh1zVYgQj5E7Bv9/ARfqozjPXkHzPx38UNd8cymKeT7JpgOUsoWO09OXP8RyM89O1cRVi1v7yxaRrO7ntzIhjcwyFNykYKnHUEEjFV6KK//9k=\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0UlEQVR4Ac1R0RGCMAxte44Be9A5sAzCjzvodQ/QNRB1DL1zCq6pTRuMR/HPD5s7SN/j5SVBiL89pbHOhziudXieXIx7lbO9d0DRz6yaEw/gYWxsePoZ4/cYdGMhetQzSplBx0FU6Nwm7F1WnHY3JTXUUkkGuUTUxo4Z+8jQF0Mvy+K9uYRe1/tF2kQlTbpBhE9bgwqxMmlpkuMjX6DuaLv7jLPUJwy0ALbSXfojbtoWjKZMk1eXqQIfdUOdecVRpFTP5rqsR/fqMGWqL5/+An4BW52UPT6esv0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+pbW3ku7uG2ix5kzrGu44GScDJrQ8SaOmgeIr7So7tbtLWTYJ1TaH464yf5msqiuq+HelPq3jK0jzGsEIaad5RlEjCncTyP51gapfy6pq15fzyPJLcTPKzv1JJzzVSivQLGTUPC/wmnvovs6p4hne2yd3meWgwcdsHLjn2Irz+iiiiiv/2Q==\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAArUlEQVR4AWNgGATgbT7CEUwIJpjl8ssCIYIuGcy+Hrfk3Y3sOCVd70c8R0iyIJgglpDGloMIETQ7ZYwW/kZIouk8xvIXIceAqpPFtPgXTkkz618nkCRRmRH/tyMLoBgr7X1uLbIkMpv78KtTzMgCyK41FNrOh+xYBmRJPUm9dGSNKJLfMxhOoUgiOYjH8dNmFDlkTsr3FXrIfAbkEHJnu3IJlySvwa93qHIDwgMAOuUk8OHyKAcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iupsfhv4y1LTW1C18O372wXcGMe0uMZyqnBYY9Ac1y1FekfCPwdcat4qtNR1HTFbRot37+8XEDSlT5Y5I3/Pt+UZ961PEfjSy8P3+tQJd6nrniG5Wa0nu7otb29puyGEEWSwI+7kkdOODg+R113gO58NWV1eXWuyRLdRov2Fbm1a4t9xJDtIi8sQMFR0J6mtrUPEnht9Vi1bVdX1fxRewOWhtzCLS1TByowSxCf7KgdK4XWtVn13W77VblY0nvJ3ndYwQoLHJAzniqNFFFFf/9k=\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA30lEQVR4AWNgGAqAG7cjpfuu7lkvjlXe/fmf///+/f8DkmRBVcFksu7/qge3OCfVo4qDeYH/PtgBdTy9i0VO/++7JgYG2WX/Pqpiyhr+62UR3/z636UoLA42/Pd8/79/lxMxtQFFQv/9+3Zanx2bnEjfn3//urDJMFi0fvy3NfPZRxVMWenVP5/WCDMCDTZFl+Td+OpjiRhQlH06hqRQ47/tNiANfBX/Pqmh6Vz6PoyRgUEtuP35n93SSHLgsI18L3CYicGc8e6m9TuQ5BiAOhgYArOAxO6nP9cgyww5NgBqkFDrwlBsFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+r9toWr3sEU9rpV9PDNIYopIrd2V3AztUgcnHYVB9gvPtrWX2Sf7WrFTB5Z3g+m3Gc1AQVYqwII4IPatXwzp+n6r4ksbLVdQWwsJZMT3LYwi4J78DOMZ7Zr1bWvirNZva+DvhlbJDZJttobhY90s0hPVN3HJ7kZOSeK6vx58SbnwLotpaGLTpfGl1ApvJYIhtgGOC3OSeeAeOpxjAr5snmkubiSeZy8srF3Y9SxOSaLe3muriO3t4nlmlYJHGi5ZmPAAA6mvZba1074K6FHqF7HFd+OL2Jvs8Gdy2KsMFj2zg/jyBxk14/f393ql/PfX1w9xdTsXklkOSxqtW54S8U3ng7Xo9YsLa0nuY0ZEF1GXVcjGRggg++e5rP1bVb3W9UuNS1G4ae6uHLyOx7nsPQeg7VTor/9k=\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8klEQVR4AWNgGI4g6/MZo+mvj1gA/aYQP10fxYtc5/6Bweu++D1v/v2bhiK54z9Q7sX69xAl38KRJHkW/Pz37846KYZJYMmbUUhyXLuBYq08PAwMPE5AVcukkOQYlgHNbOYCifBs/vfvEoocw7F/P1vAcgw9//7dQpVjyD4AdUDr33/fo5HNRLAZDb/9+4tDjkEa6KwJCLUoLMmL//518aIIwTmu14FyLHAuCsMOqG8TDn3St/7928qGohzOkbr8799GYAhhA9KX/v3by4lNhoFB6hpQnw4OOaBb9mtjl2OY/u/fGwFMOSao0Cf/D5iSeEUAw850Me9N/oIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_filename)\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"images\": sample_input.numpy()},\n",
    ")\n",
    "onnx_processed_results = [numpy.argmax(res) for res in outputs[0]]\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(sample_input[idx]))\n",
    "    print(f\"{onnx_processed_results[idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e2af0-ee1e-498e-8293-0bc41834aa4f",
   "metadata": {},
   "source": [
    "## ONNX To TFlite\n",
    "\n",
    "Our ONNX model seems to be working correctly, so let's continue to convert our model into a Keras Model. From there, we can convert it into a TFLite file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7d54e-8598-48fb-ac75-9c55c0903897",
   "metadata": {},
   "source": [
    "### ONNX to Keras\n",
    "\n",
    "Here we use a library by [@PINTO0309](https://github.com/PINTO0309/) to convert our model into a Keras model. For full documentation, see: https://github.com/PINTO0309/onnx2tf\n",
    "\n",
    "#### Important note about Parameter Replacement\n",
    "\n",
    "`onnx2tf` converts PyTorch Tensors of shape BCHW into the form BHWC, as is convention for TensorFlow. It usually manages this fine, however, if it gets the shapes wrong, you can use the `param_replacement_file` to reshape an operator's input's our outputs to correct it. \n",
    "\n",
    "For example in this example, the `squeeze` after the LSTM operation was changed to operate on the 3rd axis, but the output of the LSTM output was not updated to match; therefore, we add a `post_process_transform` to the LSTM operator to make it work. For more details, see: https://github.com/PINTO0309/onnx2tf#parameter-replacement\n",
    "\n",
    "To see the source of the problem by checking input and output shapes, it may be useful to view the ONNX model in the [Netron](https://netron.app/) visualiser. This is also an easy way to check the `op_name` and `param_name` of the operations you wish to target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "141257a9-b9de-4ca3-ac18-3aa6cd985652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create param_replacement.json file\n",
    "# (This should be a separate file, but for reader convenience, we create the file in this script)\n",
    "\n",
    "param_replacement_path = \"param_replacement.json\"\n",
    "param_replacement_content = \"\"\"\n",
    "{\n",
    "    \"format_version\": 1,\n",
    "    \"operations\": [\n",
    "        {\n",
    "            \"op_name\": \"LSTM_22\",\n",
    "            \"param_target\": \"outputs\",\n",
    "            \"param_name\": \"onnx::Squeeze_94\",\n",
    "            \"post_process_transpose_perm\": [0, 2, 3, 1]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "with open(param_replacement_path, \"w\") as fh:\n",
    "    fh.write(param_replacement_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d5d4beb-6978-4dfb-ad1f-a39415f80c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n",
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n"
     ]
    }
   ],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_filename,\n",
    "    output_folder_path=\"handwriting.tf\",\n",
    "    copy_onnx_input_output_names_to_tflite=True,\n",
    "    non_verbose=True,\n",
    "    param_replacement_file=param_replacement_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36939950-ceb7-47a9-99e2-0609a2d8c37e",
   "metadata": {},
   "source": [
    "#### Check Keras Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b8e67-e2f2-48d5-83ad-08cfd485db28",
   "metadata": {},
   "source": [
    "Now that we have a Keras model, let's check its accuracy. If all went well, it should not have changed significantly from the original model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f797340-539f-45bc-97ba-862fcea5e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the keras model on the 10000 test images: 97.69 % (9769/10000)\n"
     ]
    }
   ],
   "source": [
    "keras_correct = 0\n",
    "keras_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tf_input_data = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    keras_output_data = keras_model(tf_input_data)\n",
    "    processed_keras_output = [numpy.argmax(res) for res in keras_output_data]\n",
    "    \n",
    "    keras_results = zip(processed_keras_output, test_labels)\n",
    "    keras_batch_correct = sum([1 if output == label.item() else 0 for output, label in keras_results])\n",
    "        \n",
    "    keras_correct = keras_correct + keras_batch_correct\n",
    "    keras_total = keras_total + test_labels.size(0)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy of the keras model on the {keras_total} test images: {100 * keras_correct / keras_total} % ({keras_correct}/{keras_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb30033-ad26-4568-90f3-f85c064f29d3",
   "metadata": {},
   "source": [
    "### Keras to TensorFlow Lite\n",
    "Now let's quantise our model to int8 and export it to a TensorFlow Lite flatbuffer.\n",
    "\n",
    "\n",
    "#### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model for more details on this.](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61ff7a3f-2826-4a3f-8a77-e9b93a68dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for test_images, _ in data_loaders['test']:\n",
    "        tf_batch = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1043d1fd-b8c2-4184-9305-4a6f22e36549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = 'handwriting.tflite'\n",
    "with open(tflite_int8_model_path, 'wb') as f:\n",
    "  f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b447d46-da8b-44e6-88a5-e88fcb538d20",
   "metadata": {},
   "source": [
    "#### Check Accuracy of Converted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae7bc907-7baa-484d-9314-b49a64b32917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the TensorFlow Lite for Micro model on the 10000 test images: 97.83 % (9783/10000)\n"
     ]
    }
   ],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "tfl_correct = 0\n",
    "tfl_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tfl_input_data = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tfl_input_data)\n",
    "    tfl_interpreter.invoke()\n",
    "\n",
    "    tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "    tfl_processed_results = [numpy.argmax(res) for res in tfl_output_data]\n",
    "    \n",
    "    tfl_results = zip(tfl_processed_results, test_labels)\n",
    "    tfl_batch_correct = sum([1 if output == label.item() else 0 for output, label in tfl_results])\n",
    "        \n",
    "    tfl_correct = tfl_correct + tfl_batch_correct\n",
    "    tfl_total = tfl_total + test_labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy of the TensorFlow Lite for Micro model on the {tfl_total} test images: {100 * tfl_correct / tfl_total} % ({tfl_correct}/{tfl_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec22f2d-c1dc-41b8-a537-22aa8c052bfb",
   "metadata": {},
   "source": [
    "### Check operator counts of converted model\n",
    "Let us take a look at the operator counts inside the converted model. Ideally, we want the same accuracy with minimal operators. This uses a helper function defined in ../utils, but this step is not necessary to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36764ad8-6643-4227-98e6-fc37e291a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPERATOR              COUNT\n",
      "-------------------- ------\n",
      "quantize                  6\n",
      "reshape                   4\n",
      "transpose                 2\n",
      "dequantize                5\n",
      "while                     1\n",
      "gather                    2\n",
      "fully_connected           3\n",
      "less                      1\n",
      "add                       8\n",
      "split                     1\n",
      "logistic                  3\n",
      "mul                       3\n",
      "tanh                      2\n",
      "concatenation             3\n",
      "slice                     2\n",
      "-------------------- ------\n",
      "TOTAL                    46\n",
      "-------------------- ------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import utils\n",
    "utils.print_operator_counts(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259ccf2-5d14-4b03-8288-a28a8e77c954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
