{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53aeff71-1811-4956-95db-13c0ad7f276d",
   "metadata": {},
   "source": [
    "# Training and Converting an LSTM Model from PyTorch to Tensorflow Lite for Micro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a794d61-a6ff-42b3-bc1d-849c2dd2ab75",
   "metadata": {},
   "source": [
    "## Training in PyTorch\n",
    "\n",
    "Let's train a hand-written character recognition model with the MNIST dataset which uses LSTM models.\n",
    "\n",
    "Start off with importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517f9774-2b1d-4903-a089-1fd032b6532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\"\"\"\n",
    "Each data point is of form [PIL.Image, class] where the class is a number 0-9 of the\n",
    "respective digit. The image is a single channel image with values from 0 to 1.\n",
    "\"\"\"\n",
    "def mnist_data(train: bool) -> torchvision.datasets.MNIST:\n",
    "    return torchvision.datasets.MNIST(\n",
    "        './mnist/',\n",
    "        download=True,\n",
    "        train=train, \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "# iterator returning Tensors of type [batch (10), channels (1), height (28), width (28)]\n",
    "data_loaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        mnist_data(True),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        mnist_data(False),\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    ),}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f54c38-9bd9-4b43-8176-603f1c151256",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2670a68-ae13-4d9e-b9de-297cf74f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):\n",
    "        super(MNIST_LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Remove batch parameter\n",
    "        x = x.reshape(-1, self.input_size, self.input_size)\n",
    "\n",
    "        # Set initial hidden and cell states \n",
    "        hidden_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        cell_states_initial = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, (hidden_initial, cell_states_initial))\n",
    "        \n",
    "        dropout_out = self.dropout(lstm_out)\n",
    "        \n",
    "        fc_out = self.fc(dropout_out[:, -1, :])\n",
    "        return fc_out\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5304fc-83cd-4bae-8334-5de38e890f56",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f4496e-63dd-4753-8a90-004351b15674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=28\n",
    "\n",
    "# instantiate model\n",
    "model = MNIST_LSTM(input_size, hidden_size=128)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a154b68-b258-405e-a2eb-a03af2b7361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, model: MNIST_LSTM, loaders):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    print(\"Ended epoch: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4adfd31-a7c9-42c8-a604-f39afaa5d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Ended epoch: \n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=3, model=model, loaders=data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6161c20-3be0-4a6b-af5f-e375cc2d7ae8",
   "metadata": {},
   "source": [
    "### Check that the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8748974-a7dd-45f2-a57a-b3414b383733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+UlEQVR4nMXPsUsCcRjG8ddLKodyjIYGG+IOgiJoqhY5EteKwP6CNicXW/sPgtYW2x2kpoguqSlBcFEph64hDoUQAvO+1uJQ93tprGd8P8PzPiL/G2upPfocee6Uhg7UXoAzxRaa4JevQnqrUZovPgJVRwawHbGNDsD1ptgfJpbhzs4mxH6GS+enrQ3geELE8eEpHq0s9KCSWPTpByvmr6kAvAsoLmsr3RYAc5qJ5ADqMzpOAlCyfkHOVTyCzuE7lDS8AU/2hrylFezD65bUoeua2ACq6YMQHmYN3AegFgLrBsbyjHM6Pb58Uyu5m9mR2/vgZKjO+ZN8AQKUlIb2GD4CAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC948E12A60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA20lEQVR4nN3OPwuBURQG8Oe+pCiDwcBisEiS9R35BjIalJLNyITFoBhMympgUMpMBoNZUrJYxVuSQaIeBuTPdX0Az3I699c59wB/GREqLYxu1vzNbC2SJNsW2WJzcpbQ9QqTkkXOXMWtsBdinEjY5zoAoExy+XjT7jUYFrUZABuA9SeatEsHAASA3OdWz54phyNaPZBj+dgG79n7ZbTnd6RhkEXZALh9Pmed/DJ4i35iRTxb7Q3DZgwvikHRY1MoDF4yrTJkeHS99u9/Yrr6gVsocYORGgc75Tlyrqy9UevOELp5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC948E1A9D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAyElEQVR4nL2QoRKCUBBFH47JLJUMVa1kjUi1U/UbDEarH0BlHBo/YGNsMkQzVOreXYMwI/PeM7Jpd+7enXNXqenLGU2pVz/qp3kz6ECg69Kspi0YXLlmdX0DgQorRcko+3amXfZFhn4+JtpFoTgq1+8lSdmBQDgbNAaDwVlgAAlBILwOCyNmC4YloVLbvwnTFpxZ1ROBTDi9l5H+jMMT4iDK73Ek4uuWDeSbEoXuFGFRokTJRXe6bwGDmyI0oewrao4rz4o6WX0ApEpp89vDXPIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC948E12F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAqElEQVR4nGNgGNyAd1+LEU7Jlj9/LrDjkrzw588fDziPCVVSF4WHKhmOzz1X//75c10Qu85IVQYGhq/vsUuKMDEwfKrEZScDA8O23TisXPbn71NWHHKWn/7+f4rLrTf//Pk7H1kAyU5eVgaG//twaAz98+fPRxQRhE6hRAYGhj4cGj3//PnzB1UI1Z9luNzq+efPPLQgQXD3ZTB8/IdLJ4PuXiGcclQFAI5fMirdkKrWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC94909BA60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+ElEQVR4nGNgoA4QPvlPHJcc15G/J/hxSRr9/aCBS07hyd9cnDYm/r3Hg0vO9v9HOVQRJgQz8P/pR7g0yrx+bYnTxsa/h9GFEMayM9zCqZHh6D81CEPY2hpdjv31eQYGBgYG5UWP/v49KIUq6f93NgMDA4P8rr9//977d1EC1U4GBgYGhqg9LhujxPQKdcwYGBgYWBASagwM8tN51sZ9Z2BAjxr5T/8qWQ/9e87AwMDAgBFW0//eM/37N4CBgV2k5V8WmlbL1//+/v17v6vr+N+/Z6XRPWP67u/fv3///f170p8DIsKIJKsiF8kffOTW3rW/0fUNCQAATuBU96DomVMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC94909BA60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nGNgoAlghDFUXP0E2zh/bsCihmP2p79//vz583mfMaZk+Z8/f/+8fP7z759PMSzoklqz9jaEsDO4Tdz7/U8Xbkfo3/zTgls29+9tWZySqq//HmHHJenw5+8fUQiTCUPSDcHEkOR0QxdBAhP//v3/XhC7nOz7P3/+4vCL/NU/f/68UMAmJeBw68+fP3+Ksclx7vrz98+fR9ZYpCSrv/79+//vfWVMqYZ9j//8+fPn758gLPpW/vkLkfx0rp4TJggLhKcwAS69ZCkYG5ZMuMtqLjM8u8vAwLDr9Atsrh0UAABG12KgqPdDRwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC94909BD30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABA0lEQVR4nN2QvUoDURCFx0SEXYQg/kWDItpKOsFOsEtlpyspfAFDWgsbn0DBRq20sxGEBEQbe/OD6QJhIQhWUbcQQ4RviEVEuXe9L+BphrkfZ+bcEfmPSqwc3vdrerw3FWf5EqAATykLefvvAHpzWoNzCy4AnKX9pGTgzTNhFcJgWkSyl/CxaLClLuGmiIxut4GKaVztUfHF2+0AsPH9Ojwos9Fk6khyM0N9EXm4swKVAWhd7ER01u1fjl3DbWFc5sqE6fgRBjpQDVxsucFr5qdLmDA7L/Vnh9H/hKJr6pbSGHHBKyVI/rbGzvyatCP92zfRRI3jWGm7Jy74UpLeoytOTF/0i291Sev2ggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9485856D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjElEQVR4nGNgGNwg7ttkGQSPCVXyP0e2CE5JVIAmKYxP7cl/l3hx6ZSSZNj7GZekrAzDCZx2BuOzcvG/V+y45Ljv/VuMU6PFv3/qyHwUO90YGF7ilJRgeP0Hl6lKn/4twmnlxH8/TXBKHvl/AlUAyU4Rif9XcGo0+vcnAKdkwr93OOUYZJ9OxC1JTQAAxf8khvvxksgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC94909BD30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxklEQVR4nGNgGJqAfc3/SpySWX//PsMpef7v36l4JH/KMjAwMDDsWi+JRfI7AwMDA8PeP393QIWY4JLnGNiSGRgYoowZGcQwdIb//fuQTWnBn79/3xthWvrwLxRchYkgjGVY8h9CrwvC5t6mx3///v37NwG7b4z//v37dxkTdkmJv3//3lLELsfQ/PfvbVxyzr///l2NQ46h9u/f2yghh2y7KAPDrOe4dH7++9kSlxzD579rUQWQjb3IsBGnRga3Dey4JUkDAKRcTcJGRKRhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC948585640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCklEQVR4nM3QIUtDURjG8WeCVg1bsOhgGLT4DVzUVTFqGMKKIoJlcQNZ1i9gsRlEGMPmDeaBQbBsFosgE67B9H+4hnuvu7tbNPiW83J+5+Gc80r/uU6CtiP7pjoLDRj4XJmihWaKftjJWaHJL/KSw04C3VI/xbk0VzuIm/v+x+PUfQAenG5LujDDpQyuARAsSjFyPLbys20HyUFHHm2NsQYwiHOlXu61d0DYiHM9cNJLkqojoJ6ZUj0TjGy7IkmHjmxfFjPYAq7mpfLZF4bWxC9X32C4LG2+g/nen5xBB6hoLwDMeW5AG6+27cj2U1H5ag8BTNidNmk9nm1jYrOQrkfS7u11OCP35/UDe6+4RWL5O7kAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC948E12F40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load one batch to test\n",
    "images, labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "results = model(images).tolist()\n",
    "processed_results = list(map(lambda res: res.index(max(res)), results))\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(images[idx]))\n",
    "    print(f\"{processed_results[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687d372-cd62-4c01-9cd8-30cf67065bd5",
   "metadata": {},
   "source": [
    "### Check Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756e71c9-c34d-4d83-9cd2-ff8b75f040df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.47 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in data_loaders['test']:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "        \n",
    "        correct = correct + (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a7040-957d-4f2a-9b0c-f42864cef56a",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb269b-2cf5-4104-baf3-3e1bd24fa114",
   "metadata": {},
   "source": [
    "Now we can convert this to ONNX using PyTorch's built-in exporter. \n",
    "\n",
    "To do this, we need to provide a sample input. This sample input is passed as input to the model and the trace generated will be used to construct the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5391abd8-c0d1-4910-87bc-372d760da611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:3220: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "sample_input, sample_labels = next(iter(data_loaders[\"test\"]))\n",
    "\n",
    "onnx_filename = \"handwriting.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    onnx_filename,\n",
    "    input_names= [\"images\"],\n",
    "    output_names=[\"number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ecc8f-3cab-46e7-b6e2-084b1aa351a0",
   "metadata": {},
   "source": [
    "## Check ONNX\n",
    "Let's just check that the ONNX conversion worked before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8cab25e-2e3b-4edd-a6f2-6ada65cb67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc577c33-5f28-4581-84d5-84c0f006299a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAp0lEQVR4nGNgGAzA/d8xAVxy3Lv+/lXCJdn/9+8yNiibCV1SjIHhwy8cGhUf/v1riUOOpe3v382sOCRd//79ZY/LOaf//t2FS67xx9/P7jjk5F79/VuES2PH37+3JHDI8f/4+y0Xl8amv393oAgghRC7DbpqJEl7ewaGEzgM5dnx9+8DBRySyn///rXAZWwCA8O7e7gkNRkYZr7CYSpD0t9tzLjk6AgAwF03YCD57iAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9480E5700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABGElEQVR4nGNgGKpAa/r5379//97iDRNghFB6Mv7svnzMUMGkBSiaWv79+/fv3+c1yXK8vNb/bhmgSE779+/fGhk+BgYGtfnv//0xRjFWW+7j7a/fGBgYGOY4cN35m3Qfm5uks9/9/DIfq3OlnQ79+/fv7d99Algk9/77929ziLjLjhVYJMN+9IRxMjAwmP5zw5TkUIHQwf86GRgYGBiYkCV/3GFgYGBg4BNm9MHqJgYGBob8f/+wWcrAwMAgM+XDv2uOWKWY5e/8+/etEiGwXwfG4rfe+u/f39MwfSwMDAyP9808dVCE3VrAzZKb4ffHrh5ko7K+//v379+////+/fv/wQbdIonmb79+/fr1a8PiOC2cfqATAAC1Y2mmuWLyqgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBD00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4nGNgoDVglGma9+/fP0NscoKz/v79++VjvzgWOabcv38/TJLDbmjy37+zFHFYKH31Z78SLtf0/72HxSoIxSzMsA6XPgbev2/kcepkYPjxEI/kIyzmwSUXw4WsXDcFsqLaeQrGTPny9+/fySyoDlKAsBZ8ebRmzbm/PEiSnNf/noSY+eVvDQNDFookQ9LfV0oMDAziR/92sWNIMi/8e0OJgcH+719DBgaGWaiSDNJX/t4IY1j/968hA0Pf7/PsqJ6Suvr314Ubf/92F8z9/Tcc3ctSV//CQDwjAwMDAwMjkixLuFq04vynDAyT3/5H1zk4AAAsu2KdVJbSmgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBE50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABJ0lEQVR4nGNgoD9gRGKLz+E9u+L8H2zK2INe/fn758+mlRN4MSVn/fnz5++fP3/+/rk4WQ1NbvrfP39O9Sz9Mz39z58/j1BlbT7+/94JMU+q8N2/J/JIcmqv/vxdDefV/v3ThCRp+efPIWEE9++fI1wMDAxMDAwMDAy8nQwM698iJOcwCLHCJf2tGQ/1Ixn0hfHHPwYGBhYGBgYGBsP/DM0o7vsvxAbXycDAcBvVZzKcyJIowAhCQSUZkcOYwceecc0ThOT//5IIOd3Z//8/Q3Az/vzphrG58j78+bsKKfAFlv75YAxhpj798+dPAyfEMoiQ1FE5xmcrGc4bMqTw/mcI3ITqPqdXf2FRdi2EE8P1DRDJjVnaWPzGYrrvw8fubn6sHqcfAACVY3StT3iG6wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBBB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4nGNgGMKg+ML/fZ3M2OUW/Xmw/9q/W5EYEnJnnsR8W8rHwGX+8wRUiAkuWWvEKPu/+RMDixsWY2P+TeNfzM3A4P7vXzSGJNvJNYwSDExmr/9lsGFqTf4Xz8Bg9+9PHjbXcjw/xVr/52codo9E/zv351sEdjkG7jX/vkXhkGPgPvPvIi45htB/379oIgswIbGV/pX/nYNL57ptDN6fnXHoZGBgOH6jDbfkuxpdDeySWxgYGG5yZGOXfG3lyMDAwIsQYEGS3DxvZp0ww29c7o3e8m+TKC5JUgAA3P1G25FnMi8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBBB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/klEQVR4nGNgGASA31dCQql71b8uTCmx/Dt///799/fvdRsMuZQb//7+/fv338cZvAwMDAwsKHITORiOrjZhOHToNqahO//+beDA7hTeq//+bcHhTLHyv38vK6EIMcIYVlMM/jMwPLzz/8Hagz/RNV74C3Ho379/HWFiyK7ddo2BgfF/qPwmp9M4rGZw+/evFItOKPjP8B/KYsKQe/AGzoRKtl5Qg4koiKCp9v769yA3hCl27u9fbhRJsW///ilCmIsxg6n3798r2gwMDAxWd/6+EUaTVHr99++Pnalu6///feXIgA4c38JCqBfTcwzCDaf+/v37t8uBDYskHQEAJHJv1DWRxMIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBF40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAUlEQVR4nGNgGFAgcPzfv3//n2RikzN+8/fv379//77RwpRz2PsXCh5iSl79+/dOg63t7r9/fyejy/Hd/nY7koGBQfz837+NMEEWKB0sXt3PwMDA8PI1kg6Y5DmlNwwMDAwMKkpYJC9CKPWdsth8wsDAu/rx4zd///79zocpp3sV4pNv2EIh6O/fv3//fsi0xbSTgSGPgeFT7t8X+7FaeeDv32o0ISbsjkMDnr/+fmBnYGBg4MnIYEO3k5mZgcXH2o+BgUWWoezfvZ5dyDp3/EUBX7yRdcJC5tFXBgYGFVYmVhSvMDD8f3N0M8OuZwwMDMG8HzYgG3v17/w8olxOYwAAESZxQd15J7sAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBDC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAt0lEQVR4nGNgIBLIfG5AFWBCYvNw6+OWxAAkSDLikzyCW1KLgQ23pArDC9yS7AyncLqN5ew/YZySIv/OMOM2luH7X5yScoy/GBgYGHiT7dAMYGBgYEj+V8rAwBD0/N+/cGzGMjAwaPSJb2KoZMXQafSvlIGh918r3xssrhb5f5hB5+0LFYYjMEkWhOTfb7op3IIdd5Skn/zAdJH/13+f/728/vlfG6YcA4Ny1bN/b9ZPCsPilaEOAMbAMsBLlRm7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBE20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2ElEQVR4nGNgGBmAkYGBgcHc75syg+f+iHf9qfIMT9ZOu4WkIO3nPxTw8a4KQnLeP3RwXhwuKT3r8pHdqYEsEKAw/9+/f3YIraxcyM5gO/vvXz8DAwMDAwsDAwPD79/Ikr9+MDCcwuUBzov/ljIzMDAwMDBhSgbqMtz4i0OjxLt/LwxwyDFt/vcvAZeNVv/+nTHBISf39t8nDRxyfHv+/fPCZWjZv393+HDIqT76910WuxSv+op/35Nx6Jvz79+/Vbgs3PTv3yZ23JL7OVEEkMP29u0L31EkAY3+Zz4WdQF3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC948E1A850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAsElEQVR4nGNgGAzg/98X+lAmE7pc7b//nHK4JIUZGJ5txiEpasfAsBaXjVv//P3jjkPO/tOfv3u5YDw0Ywu5GBi+fcOh8e2fP38TcZia9/fv3xBcznn5588lHFJsFX//fIvBIan65++f67gMVf37/68HsgCyV2r+//v/H4dG0xd//p4QwaEzTYSB4fUbHDpv/f37XxmHXNz3P3/ms6IIoYbtx984dDLc/PPHDpcc/QAAjFNBYnZHWZQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FC9481CBDC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_filename)\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"images\": sample_input.numpy()},\n",
    ")\n",
    "onnx_processed_results = [numpy.argmax(res) for res in outputs[0]]\n",
    "\n",
    "# show first ten images and their calculated labels\n",
    "for idx in range(10):\n",
    "    display(torchvision.transforms.ToPILImage()(sample_input[idx]))\n",
    "    print(f\"{onnx_processed_results[idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e2af0-ee1e-498e-8293-0bc41834aa4f",
   "metadata": {},
   "source": [
    "## ONNX To TFlite\n",
    "\n",
    "Our ONNX model seems to be working correctly, so let's continue to convert our model into a Keras Model. From there, we can convert it into a TFLite file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99dcf-4503-4e8b-acc9-5e06be6a67fb",
   "metadata": {},
   "source": [
    "### ONNX to Keras\n",
    "\n",
    "Here we use a library by [@PINTO0309](https://github.com/PINTO0309/) to convert our model into a Keras model. For full documentation, see: https://github.com/PINTO0309/onnx2tf\n",
    "\n",
    "#### Important note about Parameter Replacement\n",
    "\n",
    "`onnx2tf` converts PyTorch Tensors of shape BCHW into the form BHWC, as is convention for TensorFlow. It usually manages this fine, however, if it gets the shapes wrong, you can use the `param_replacement_file` to reshape an operator's input's our outputs to correct it. \n",
    "\n",
    "For example in this example, the `squeeze` after the LSTM operation was changed to operate on the 3rd dimension, but the output of the LSTM output was not updated to match; therefore, we add a `post_process_transform` to the LSTM operator to make it work. For more details, see: https://github.com/PINTO0309/onnx2tf#parameter-replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5d4beb-6978-4dfb-ad1f-a39415f80c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n",
      "\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. debian/ubuntu: apt install -y flatbuffers-compiler Other than debian/ubuntu: https://github.com/google/flatbuffers/releases\n"
     ]
    }
   ],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_filename,\n",
    "    output_folder_path=\"handwriting.tf\",\n",
    "    copy_onnx_input_output_names_to_tflite=True,\n",
    "    non_verbose=True,\n",
    "    param_replacement_file=\"param_replacement.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36939950-ceb7-47a9-99e2-0609a2d8c37e",
   "metadata": {},
   "source": [
    "#### Check Keras Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b8e67-e2f2-48d5-83ad-08cfd485db28",
   "metadata": {},
   "source": [
    "Now that we have a Keras model, let's check its accuracy. If all went well, it should not have changed significantly from the original model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f797340-539f-45bc-97ba-862fcea5e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the keras model on the 10000 test images: 97.47 % (9747/10000)\n"
     ]
    }
   ],
   "source": [
    "keras_correct = 0\n",
    "keras_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tf_input_data = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    keras_output_data = keras_model(tf_input_data)\n",
    "    processed_keras_output = [numpy.argmax(res) for res in keras_output_data]\n",
    "    \n",
    "    keras_results = zip(processed_keras_output, test_labels)\n",
    "    keras_batch_correct = sum([1 if output == label.item() else 0 for output, label in keras_results])\n",
    "        \n",
    "    keras_correct = keras_correct + keras_batch_correct\n",
    "    keras_total = keras_total + test_labels.size(0)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy of the keras model on the {keras_total} test images: {100 * keras_correct / keras_total} % ({keras_correct}/{keras_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb30033-ad26-4568-90f3-f85c064f29d3",
   "metadata": {},
   "source": [
    "### Keras to TensorFlow Lite\n",
    "Now let's quantise our model to int8 and export it to a TensorFlow Lite flatbuffer.\n",
    "\n",
    "\n",
    "#### Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model for more details on this.](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61ff7a3f-2826-4a3f-8a77-e9b93a68dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for test_images, _ in data_loaders['test']:\n",
    "        tf_batch = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "        yield [tf_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1043d1fd-b8c2-4184-9305-4a6f22e36549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = 'handwriting.tflite'\n",
    "with open(tflite_int8_model_path, 'wb') as f:\n",
    "  f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b447d46-da8b-44e6-88a5-e88fcb538d20",
   "metadata": {},
   "source": [
    "#### Check Accuracy of Converted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bc907-7baa-484d-9314-b49a64b32917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "tfl_correct = 0\n",
    "tfl_total = 0\n",
    "\n",
    "for test_images, test_labels in data_loaders['test']:\n",
    "    #transpose the input_batch into BHWC order for tensorflow\n",
    "    tfl_input_data = numpy.transpose(test_images.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "    tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tfl_input_data)\n",
    "    tfl_interpreter.invoke()\n",
    "\n",
    "    tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "    tfl_processed_results = [numpy.argmax(res) for res in tfl_output_data]\n",
    "    \n",
    "    tfl_results = zip(tfl_processed_results, test_labels)\n",
    "    tfl_batch_correct = sum([1 if output == label.item() else 0 for output, label in tfl_results])\n",
    "        \n",
    "    tfl_correct = tfl_correct + tfl_batch_correct\n",
    "    tfl_total = tfl_total + test_labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy of the TensorFlow Lite for Micro model on the {tfl_total} test images: {100 * tfl_correct / tfl_total} % ({tfl_correct}/{tfl_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec22f2d-c1dc-41b8-a537-22aa8c052bfb",
   "metadata": {},
   "source": [
    "### Check operator counts of converted model\n",
    "Let us take a look at the operator counts inside the converted model. Ideally, we want the same accuracy with minimal operators. This uses a helper function defined in ../utils, but this step is not necessary to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36764ad8-6643-4227-98e6-fc37e291a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "# allow importing helper functions from local module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import utils\n",
    "utils.print_operator_counts(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259ccf2-5d14-4b03-8288-a28a8e77c954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
