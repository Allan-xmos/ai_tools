{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcf0b88-e2ba-48c2-8a57-ddc7a7dc520a",
   "metadata": {},
   "source": [
    "# Process Overview\n",
    "\n",
    "Start with a Keras model, which is then converted into a tflite model. The tflite model is then run through the xformer compiler to make an xmos optimised tflite file.\n",
    "\n",
    "We can use the relavent interpreters for each model to verify that given the same input, they both produce the same output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c94f93-a1aa-4ac7-8b69-a4bccd22cae6",
   "metadata": {},
   "source": [
    "<img src=\"./conversion_process.jpg\" alt=\"Diagram of conversion Process\" style=\"width: 500px; height: auto; margin: 1rem auto 2rem;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12967287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tflite2xcore import utils, analyze, version\n",
    "import tflite2xcore.converter as xcore_conv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592ad9d-79e0-4304-a005-57eb03bf26ef",
   "metadata": {},
   "source": [
    "# Make a Model to convert\n",
    "Use Keras to make a model of arbiraty size and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39cfdc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_size = (2, 2)\n",
    "input_shape = (3, 3, 4)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=pool_size, input_shape=input_shape)\n",
    "])\n",
    "# is this necessary?\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d58472-a59c-45c5-b47c-fc8d571b7677",
   "metadata": {},
   "source": [
    "## Convert keras model into a tflite model\n",
    "The xcore converter cannot optimise a keras model to run on xcore devices, so it must first be converted into a tflite file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aafe3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f5c47",
   "metadata": {},
   "source": [
    "### Representitive Dataset\n",
    "\n",
    "Tensorflow can optimise the converted model if you pass it a a representative dataset. This dataset can be a small subset (around ~100-500 samples) of the training or validation data\n",
    "\n",
    "The below function randomly gemerates this, but see [the tensorflow ducumentation](https://www.tensorflow.org/lite/performance/post_training_quantization) to see how to do this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa57eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example use a random dataset\n",
    "def representative_dataset():\n",
    "    batch_size = 8\n",
    "    for _ in range(100):\n",
    "      data = np.random.uniform(-0.1, 0.001, (batch_size, *input_shape))\n",
    "      yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dca7ac-8a6a-4d9c-b4d3-42d0b3fbb622",
   "metadata": {},
   "source": [
    "* **tf.lite.Optimize.DEFAULT:** Default optimization strategy that quantizes model weights. Enhanced optimizations are gained by providing a representative dataset that quantizes biases and activations as well. Converter will do its best to reduce size and latency, while minimizing the loss in accuracy.\n",
    "\n",
    "* **target_spec.supported_ops:** Import TFLITE ops. [Tensorflow docs](https://www.tensorflow.org/lite/guide/ops_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b093742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/fg/_pf9q2tj3cl9yfjb392zl7rm0000gn/T/tmpoz298kvf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/fg/_pf9q2tj3cl9yfjb392zl7rm0000gn/T/tmpoz298kvf/assets\n",
      "2021-10-29 12:36:01.609244: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-10-29 12:36:01.609484: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-10-29 12:36:01.611708: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2021-10-29 12:36:01.635675: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.\n",
      "2021-10-29 12:36:01.635705: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 9, output_inference_type: 9\n"
     ]
    }
   ],
   "source": [
    "# Set up the converter to convert our float model into int8 quantised model\n",
    "#explain  https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8 \n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_model_path = 'avgpooling2d.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a4e6b-7a17-49c8-b289-0714b315f0bc",
   "metadata": {},
   "source": [
    "# Optimise model for XCore\n",
    "Use `xcore_conv.convert(input_path, output_path)` to make an xcore optimised version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a26aa710",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcore_optimised_path = 'xcore_model.tflite'\n",
    "xcore_conv.convert(tflite_model_path, xcore_optimised_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee3d58-55da-4c5b-9ec5-46c82321b0a8",
   "metadata": {},
   "source": [
    "# Check it worked\n",
    "To check if it worked, we can use the interpreters to run the models and make sure that they produce the same output.\n",
    "\n",
    "For normal tensorflow tflite models, use `tensorflow.lite.Interpreter`. For XCore optimised models, the `XCOREInterpreter` must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f37269b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcore_interpreters import XCOREInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ada7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "tf_interpreter.allocate_tensors()\n",
    "\n",
    "tf_input_details = tf_interpreter.get_input_details()\n",
    "tf_output_details = tf_interpreter.get_output_details()\n",
    "\n",
    "tf_input_shape = tf_input_details[0]['shape']\n",
    "# Fill with 126 so that xcore can be given same input\n",
    "tf_input_data = np.array(np.random.randint(126, 127, tf_input_shape), dtype=np.int8)\n",
    "\n",
    "tf_interpreter.set_tensor(tf_input_details[0]['index'], tf_input_data)\n",
    "\n",
    "tf_interpreter.invoke()\n",
    "tf_output_data = tf_interpreter.get_tensor(tf_output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62d94234",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcore_interpreter = XCOREInterpreter(model_path=xcore_optimised_path)\n",
    "xcore_interpreter.allocate_tensors()\n",
    "\n",
    "xcore_input_details = xcore_interpreter.get_input_details()\n",
    "xcore_output_details = xcore_interpreter.get_output_details()\n",
    "\n",
    "xcore_input_shape = xcore_input_details[0]['shape']\n",
    "# Fill with 126 so that xcore converter has the same inputs\n",
    "xcore_input_data = np.array(np.random.randint(126, 127, xcore_input_shape), dtype=np.int8)\n",
    "\n",
    "xcore_interpreter.set_tensor(xcore_input_details[0]['index'], xcore_input_data)\n",
    "\n",
    "xcore_interpreter.invoke()\n",
    "xcore_output_data = xcore_interpreter.get_tensor(xcore_output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f8e306c-f6f1-42d3-b1b3-384142733fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models' output the same result?\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "print(\"Both models' output the same result?\")\n",
    "print(\"yes\" if np.array_equal(xcore_output_data[0], tf_output_data[0]) else \"no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7356b-80a7-40fe-a0a7-376444114c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4f40b-0f94-45e7-b398-dda5d002294c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0436a0dea52299ed28644175e220c962eae431d92561f4f402c0c00186dcb06f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
