{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(\"Eager execution enabled: \", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and rescale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "scale = tf.constant(255, dtype=tf.dtypes.float32)\n",
    "x_train, x_test = train_images/scale, test_images/scale\n",
    "y_train, y_test = tf.expand_dims(train_labels, 1), tf.expand_dims(test_labels, 1)\n",
    "\n",
    "#mean = tf.math.reduce_mean(x_train)\n",
    "#std = tf.math.reduce_std(x_train)\n",
    "#x_train, x_test = (x_train-mean)/std, (x_test-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define, compile, and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# single dense layer, i.e. multiple logistic regression\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "training_params = {'optimizer': 'adam',\n",
    "                   'loss': 'sparse_categorical_crossentropy',\n",
    "                   'metrics': ['accuracy']}\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "model.compile(**training_params)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TFLite and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = pathlib.Path(\"./mnist_models/\")\n",
    "models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model_float_lite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_float_file = models_dir/\"model_float.tflite\"\n",
    "size_float = model_float_file.write_bytes(model_float_lite)\n",
    "print('Float model size: {:.0f} KB'.format(size_float/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # this doesn't seem to do anything\n",
    "\n",
    "# representative dataset to estimate activation distributions\n",
    "x_train_ds = tf.data.Dataset.from_tensor_slices((x_train)).batch(1)\n",
    "def representative_data_gen():\n",
    "    for input_value in x_train_ds.take(100):\n",
    "        yield [input_value]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "model_quant_lite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant_file = models_dir/\"model_quant.tflite\"\n",
    "size_quant = model_quant_file.write_bytes(model_quant_lite)\n",
    "print('Quantized model size: {:.0f} KB'.format(size_quant/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build interpreters and run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_float = tf.lite.Interpreter(model_content=model_float_lite)\n",
    "interpreter_float.allocate_tensors()\n",
    "interpreter_quant = tf.lite.Interpreter(model_content=model_quant_lite)\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_float = np.NaN*np.zeros((y_test.shape[0], 10))\n",
    "probabilities_quant = np.NaN*np.zeros((y_test.shape[0], 10))\n",
    "probabilities = model(x_test).numpy()\n",
    "\n",
    "for j, img in enumerate(x_test):\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    interpreter_float.set_tensor(interpreter_float.get_input_details()[0][\"index\"], img)\n",
    "    interpreter_float.invoke()\n",
    "    probabilities_float[j] = interpreter_float.get_tensor(interpreter_float.get_output_details()[0][\"index\"])\n",
    "    \n",
    "    interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], img)\n",
    "    interpreter_quant.invoke()\n",
    "    probabilities_quant[j] = interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_abs_err_float = norm(probabilities_float-probabilities, axis=1)\n",
    "prob_abs_err_quant = norm(probabilities_quant-probabilities, axis=1)\n",
    "denom = norm(probabilities, axis=1)\n",
    "prob_rel_err_float = prob_abs_err_float / denom\n",
    "prob_rel_err_quant = prob_abs_err_quant / denom\n",
    "print('Mean relative error of output activations compared to original model output:')\n",
    "print('# Float TFLite model:     {:.5e}'.format(np.mean(prob_rel_err_float)))\n",
    "print('# Quantized TFLite model: {:.5e}'.format(np.mean(prob_rel_err_quant)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_float = np.argmax(probabilities_float, axis=1)\n",
    "predictions_quant = np.argmax(probabilities_quant, axis=1)\n",
    "predictions = np.argmax(probabilities, axis=1)\n",
    "\n",
    "acc = tf.metrics.Accuracy()\n",
    "print('Accuracy of models:')\n",
    "print('# Original keras model:   {:.2%}'.format(acc(test_labels, predictions).numpy()))\n",
    "print('# Float TFLite model:     {:.2%}'.format(acc(test_labels, predictions_float).numpy()))\n",
    "print('# Quantized TFLite model: {:.2%}'.format(acc(test_labels, predictions_quant).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreter surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run interpreters on a single sample\n",
    "img = tf.expand_dims(x_test[10], 0)\n",
    "interpreter_float.set_tensor(interpreter_float.get_input_details()[0][\"index\"], img)\n",
    "interpreter_float.invoke()\n",
    "interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], img)\n",
    "interpreter_quant.invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float interpreter components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_float.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized interpreter components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_quant.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve input image and its quantization, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_float = interpreter_float.get_tensor(1)[0].copy()\n",
    "img_quant_float = interpreter_quant.get_tensor(5)[0].copy()\n",
    "img_quant_int8 = interpreter_quant.get_tensor(1)[0].copy()\n",
    "img_quantization = interpreter_quant.get_tensor_details()[1]['quantization']\n",
    "\n",
    "img_quant_int8_float = (np.float32(img_quant_int8) - img_quantization[1])*img_quantization[0]\n",
    "img_quant_float_int8 = np.int8(img_quant_float/img_quantization[0] + img_quantization[1])\n",
    "img_quant_diff = np.abs((np.float32(img_quant_int8) - img_quantization[1]) * img_quantization[0] - img_quant_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dict = {\"float input\": img_float,\n",
    "           \"quant float input\": img_quant_float,\n",
    "           \"quant int8 input\": img_quant_int8,\n",
    "           \"quant inputs' diff\": img_quant_diff,\n",
    "           \"float from quant int8\": img_quant_int8_float,\n",
    "           \"int8 from quant float\": img_quant_float_int8}\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for j, (title, im) in enumerate(im_dict.items()):\n",
    "    plt.subplot(1, len(im_dict), j+1)\n",
    "    kwargs = {'vmin':0, 'vmax':1} if title == \"quant inputs' diff\" else dict()\n",
    "    plt.imshow(im, cmap='gray', **kwargs)\n",
    "    plt.title(title)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate that the bug corrupts the internal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: file a bug report\n",
    "\n",
    "interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], tf.expand_dims(img_quant_int8_float, 0))\n",
    "interpreter_quant.invoke()\n",
    "print('Output with corrupted image:')\n",
    "print(interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"]).flatten())\n",
    "\n",
    "interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], tf.expand_dims(img_quant_float, 0))\n",
    "interpreter_quant.invoke()\n",
    "print('Output with uncorrupted image:')\n",
    "print(interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"]).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve weights and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_quant = interpreter_quant.get_tensor(3)\n",
    "weights_float = interpreter_float.get_tensor(3)\n",
    "weights_quantization = interpreter_quant.get_tensor_details()[3]['quantization']\n",
    "\n",
    "weights_quant_diff = np.abs(np.float32(weights_quant) - weights_float / weights_quantization[0])\n",
    "weights_rel_err = norm(weights_quant_diff) / norm(np.float32(weights_quant))\n",
    "print('Mean relative error between quantized and float weights: {:.4%}'.format(weights_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights_quant.reshape(-1, 28, 28)\n",
    "plt.figure(figsize=(16, 7))\n",
    "for j in range(10):\n",
    "    plt.subplot(2, 5, j+1)\n",
    "    plt.imshow(w[j,:,:], vmin=-128, vmax=127)\n",
    "    plt.title('Digit {}'.format(j))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of weight quantization errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights_quant_diff.reshape(-1, 28, 28)\n",
    "plt.figure(figsize=(16, 1))\n",
    "for j in range(10):\n",
    "    plt.subplot(1, 10, j+1)\n",
    "    plt.hist(w[j,:,:].reshape(-1))\n",
    "    plt.title('Digit {}'.format(j))\n",
    "plt.subplots_adjust(wspace=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve biases and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_quant = interpreter_quant.get_tensor(4)\n",
    "bias_float = interpreter_float.get_tensor(4)\n",
    "bias_quantization = interpreter_quant.get_tensor_details()[4]['quantization']\n",
    "\n",
    "bias_quant_diff = np.abs(np.float32(bias_quant) - bias_quantization[1] \\\n",
    "                                - bias_float / bias_quantization[0])\n",
    "bias_rel_err = norm(bias_quant_diff) / norm(np.float32(bias_quant))\n",
    "print('Mean relative error between quantized and float matmul bieses: {:.4%}'.format(bias_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve preactivations and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the tensor dense/BiasAdd is actually a preactivation, not a bias\n",
    "preact_quant = interpreter_quant.get_tensor(2)\n",
    "preact_float = interpreter_float.get_tensor(2)\n",
    "preact_quantization = interpreter_quant.get_tensor_details()[2]['quantization']\n",
    "\n",
    "preact_quant_diff = np.abs(np.float32(preact_quant) - preact_quantization[1] - preact_float / preact_quantization[0])\n",
    "preact_rel_err = norm(preact_quant_diff) / norm(np.float32(preact_quant))\n",
    "print('Mean relative error between quantized and float preactivations: {:.4%}'.format(preact_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve outputs and quantizations, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_float = interpreter_float.get_tensor(interpreter_float.get_output_details()[0][\"index\"])\n",
    "output_quant_float = interpreter_quant.get_tensor(interpreter_quant.get_output_details()[0][\"index\"])\n",
    "output_quant_int8 = interpreter_quant.get_tensor(0)\n",
    "output_quantization = interpreter_quant.get_tensor_details()[0]['quantization']\n",
    "\n",
    "output_quant_diff = np.abs(np.float32(output_quant_int8) - output_quantization[1] \\\n",
    "                     - output_float / output_quantization[0])\n",
    "output_rel_err = norm(output_quant_diff) / norm(np.float32(output_quant_int8))\n",
    "print('Mean relative error between quantized and float outputs: {:.4%}'.format(output_rel_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreter reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float interpreter\n",
    "rec_preact_float = np.matmul(weights_float, img_float.flatten()) + bias_float\n",
    "rec_out_float = tf.math.softmax(rec_preact_float).numpy()\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(norm(rec_out_float-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights converted to float and float input (from quant model), compared quant output\n",
    "rec_preact_float2 = np.matmul(\n",
    "    np.float32(weights_quant)*weights_quantization[0],\n",
    "    img_quant_float.flatten()\n",
    ") + bias_quant*bias_quantization[0]\n",
    "\n",
    "rec_out_float2 = tf.math.softmax(rec_preact_float2).numpy()\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float2))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_out_float2-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights converted to float and int input converted to float\n",
    "# NOTE: because of the above bug, float->int8->float converted image is used\n",
    "rec_preact_float3 = np.matmul(\n",
    "    np.float32(weights_quant)*weights_quantization[0],\n",
    "    (np.float32(img_quant_float_int8) - img_quantization[1]).flatten()*img_quantization[0]\n",
    ") + bias_quant*bias_quantization[0]\n",
    "rec_out_float3 = tf.math.softmax(rec_preact_float3).numpy()\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_float3))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_out_float3-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int weights and int input, using 32 bit accumulation and 32 bit bias\n",
    "rec_preact_int = np.matmul(np.int32(weights_quant),\n",
    "                           np.int32(img_quant_float_int8).flatten()) \\\n",
    "    - np.matmul(np.int32(weights_quant),\n",
    "                np.int32(img_quantization[1]*np.ones(img_quant_float_int8.size))) \\\n",
    "    + bias_quant\n",
    "rec_out_int = tf.math.softmax(rec_preact_int*bias_quantization[0]).numpy()\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed output:\\n{}\".format(rec_out_int))\n",
    "    print(\"Original output:\\n{}\".format(output_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_out_int-output_float.flatten())/norm(output_float.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XS3 emulation and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are XS3 hardware parameters\n",
    "bpv, bpe, vac = 256, 8, 8\n",
    "ve = bpv//bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate int16 bias values for XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: on XS3 the accumulator for int8 vector operation is 2 x int8 (vR/vD), hence the int16 bias\n",
    "# NOTE: to avoid saturation while computing the dot prod, the bias should be spread out between the elements\n",
    "#       Thus vR/vD should be initialized with the spread out values in all elements\n",
    "# TODO: there might be a better strategy to apply the bias\n",
    "# TODO: storing the biases in int8 might be okay too, investigate\n",
    "unified_bias = bias_quant - \\\n",
    "    np.matmul(np.int32(weights_quant),\n",
    "              np.int32(img_quantization[1]*np.ones(img_quant_float_int8.size)))\n",
    "unified_bias = unified_bias / np.float32(2**(bpe-2))  # the shift here is b/c of how VLMACCR works\n",
    "unified_bias_int16_ve = np.round(unified_bias / ve)  # spread out the bias between vR/vD elements\n",
    "unified_bias_int16_ve = np.int16(np.clip(unified_bias_int16_ve, -2**(2*bpe-1), 2**(2*bpe-1)-1))\n",
    "\n",
    "print(\"These are int16 bias values that the XS3 implementation should store:\")\n",
    "print(unified_bias_int16_ve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions that emulate vector unit on XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: vacc is equivalent to the vR/vD pair in XS3\n",
    "\n",
    "def VLMACCR(a, b, vacc):\n",
    "    assert len(a) == len(b) == len(vacc)\n",
    "    t = np.round(np.int16(a)*np.int16(b) / np.float32(2**(bpe-2)))  # multiply, round+shift\n",
    "    t = sum(t) + np.float32(vacc[-1])  # sum and apply bias from buffer\n",
    "    t = np.clip(t, -2**(bpe+vac-1)+1, 2**(bpe+vac-1)-1)  # this is how VLMACCR saturates in XS3\n",
    "    vacc = np.hstack([np.int16(t), vacc[:-1]])  # update buffer\n",
    "    return vacc\n",
    "\n",
    "def VLSAT(v, s=0):\n",
    "    t = np.round(np.float32(v) / 2**s)\n",
    "    t = np.clip(t, -2**(bpe-1), 2**(bpe-1)-1)\n",
    "    return np.int8(t)\n",
    "\n",
    "def VLREDSUM(v, s=0):  # this actually doesn't exists in the XS3 ISA (yet?)\n",
    "    vacc = np.zeros(ve, dtype=np.int16)\n",
    "    # use VLMACCR to do the summation, so we need a 2**(bpe-2) shift\n",
    "    vacc = VLMACCR(v, np.int8(np.ones(v.shape) * 2**(bpe-2-s)), vacc)\n",
    "    return vacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def XS3_dot_prod(v, w, bias_vacc=None, scale1=4, scale2=1):\n",
    "    assert len(v) == len(w)\n",
    "    num_vlmaccr = ceil(len(v)/ve)  # this is a trick\n",
    "    \n",
    "    # add bias (distributed accross all elements of vacc)\n",
    "    if bias_vacc is None:\n",
    "        vacc = np.zeros(ve, dtype=np.int16)\n",
    "    else:\n",
    "        vacc = bias_vacc\n",
    "    \n",
    "    pad = num_vlmaccr * ve - len(v)\n",
    "    v, w = np.pad(v, (0, pad)), np.pad(w, (0, pad))\n",
    "    for n in range(num_vlmaccr):\n",
    "        beg, end = n*ve, (n+1)*ve\n",
    "        vacc = VLMACCR(v[beg:end], w[beg:end], vacc)\n",
    "\n",
    "    # saturate vector register contents\n",
    "    # NOTE: the shift here is our choice, maybe optimize for it?\n",
    "    # if this shift is too small, saturation will occure often\n",
    "    # if it's too large, we loose less significant digits, which also leads to loss in accuracy\n",
    "    vR = VLSAT(vacc, scale1)\n",
    "    \n",
    "    # sum contents of vector register\n",
    "    # result is int16 value stored at the beginning of vR/vD\n",
    "    # NOTE: the shift here is our choice, maybe optimize for it?\n",
    "    \n",
    "    # use this for more accuracy\n",
    "    vacc = VLREDSUM(vR)\n",
    "    vR = VLSAT(vacc, scale2)\n",
    "    \n",
    "    # less accurate but one less instruction\n",
    "    #vR = np.int8(VLREDSUM(vR, scale2))\n",
    "\n",
    "    return vR[0]  # result is int8\n",
    "\n",
    "# TODO: there is probably a more efficient way to do matrix-vector multiplication\n",
    "def XS3_fcc_forward(input_int8, weights_int8, bias_int16_ve,\n",
    "                    scale1, scale2):\n",
    "    bias_int16_vacc = np.tile(bias_int16_ve, (ve, 1)).T  # just a copy for easy access\n",
    "    output_int8 = np.zeros(weights_quant.shape[0], dtype=np.int8)\n",
    "    for feature_num in range(10):\n",
    "        w = weights_int8[feature_num]  # these are the feature coeffs\n",
    "        bias_vacc = bias_int16_vacc[feature_num]\n",
    "        output_int8[feature_num] = XS3_dot_prod(w, input_int8, bias_vacc,\n",
    "                                                scale1, scale2)\n",
    "\n",
    "    return output_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate preactivation on xs3, compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the only goal is classification, getting the argmax of the preactivation is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale1, scale2 = 4, 1  # TODO: find these scales by optimization on the training set\n",
    "preact_xs3 = XS3_fcc_forward(input_int8=img_quant_float_int8.flatten(),\n",
    "                             weights_int8=weights_quant, bias_int16_ve=unified_bias_int16_ve,\n",
    "                             scale1=scale1, scale2=scale2)\n",
    "print(\"int8 preactivation values produced by XS3 emulation:\")\n",
    "print(preact_xs3)\n",
    "print(\"int8 preactivation values produced by int32 accumulation:\")\n",
    "print(np.int8(np.round(rec_preact_int / np.float32(2**(bpe-2)) / 2**(scale1+scale2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to float preactivation\n",
    "rec_preact_xs3 = preact_xs3 * np.float32(2**(bpe-2)) * 2**(scale1+scale2) * bias_quantization[0]\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.6e}'.format}):\n",
    "    print(\"Reconstructed preactivation (xs3):\\n{}\".format(rec_preact_xs3))\n",
    "    print(\"Original preactivation:\\n{}\".format(preact_float.flatten()))\n",
    "    print(\"Relative error: {:.6e}\".format(\n",
    "        norm(rec_preact_xs3-preact_float.flatten())/norm(preact_float.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of the XS3 emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while because the XS3 emulation is very inefficient\n",
    "predictions_xs3 = np.zeros(predictions.shape, dtype=np.int64)\n",
    "for j, im in enumerate(test_images):  #.shape, img_quant_float_int8.shape\n",
    "    preact_xs3 = XS3_fcc_forward(input_int8=np.int8(im+img_quantization[1]).flatten(),\n",
    "                                 weights_int8=weights_quant, bias_int16_ve=unified_bias_int16_ve,\n",
    "                                 scale1=scale1, scale2=scale2)\n",
    "    predictions_xs3[j] = np.argmax(preact_xs3)\n",
    "    if (j+1) % 10 == 0:\n",
    "        print('{:6d}/10000'.format(j+1), end='\\r')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = tf.metrics.Accuracy()\n",
    "print('Accuracy of models:')\n",
    "print('# Original keras model:   {:.2%}'.format(acc(test_labels, predictions).numpy()))\n",
    "print('# Float TFLite model:     {:.2%}'.format(acc(test_labels, predictions_float).numpy()))\n",
    "print('# Quantized TFLite model: {:.2%}'.format(acc(test_labels, predictions_quant).numpy()))\n",
    "print('# Emulated XS3 model:     {:.2%}'.format(acc(test_labels, predictions_xs3).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
