{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from XS3VPU import XS3VPU\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Executing eagerly: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "vpu = XS3VPU(bpe=8)\n",
    "int8min, int8max = vpu._sat_bounds(vpu._single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chunk is an `acc_period` $\\times$ `ve` size submatrix that operates on a single vector of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chunk(vpu, W, x, W_start, W_step, x_start):\n",
    "    # ~ 17 instructions\n",
    "    vpu.VLDC(x[x_start:x_start+vpu.ve]); rw = W_start\n",
    "    for _ in range(vpu.acc_period):  # unroll in asm\n",
    "        vpu.VLMACCR(W[rw:rw+vpu.ve]); rw += W_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpu = XS3VPU(bpe=8)\n",
    "\n",
    "# generate coefficients and data for test\n",
    "W = np.random.randint(int8min, int8max, size=(vpu.acc_period, vpu.ve), dtype=np.int8)\n",
    "x = np.random.randint(int8min, int8max, size=(vpu.ve,), dtype=np.int8)\n",
    "\n",
    "# reference output\n",
    "y = np.matmul(np.int32(W), np.int32(x))\n",
    "\n",
    "# mimic XS3 layout in memory\n",
    "W_XS3 = np.copy(W.flatten())\n",
    "x_XS3 = np.copy(x)\n",
    "\n",
    "compute_chunk(vpu, W_XS3, x_XS3, W_start=0, W_step=vpu.ve, x_start=0)\n",
    "print(\"Result matches reference: {}\".format(np.all(np.all(vpu._combine_vD_vR() == y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tile is a sequence of chunks producing a single vector of outputs, while operating on continuously layed out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tile(vpu, W, x, N_chunks, W_start, W_step, W_chunk_step, x_start):\n",
    "    # ~ N_chunks * (17 + 2) + 5\n",
    "    rx = x_start; rw = W_start\n",
    "    for _ in range(N_chunks):\n",
    "        compute_chunk(vpu, W, x, W_start=rw, W_step=W_step, x_start=rx)\n",
    "        rx += vpu.ve; rw += W_chunk_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpu = XS3VPU(bpe=8)\n",
    "N_chunks = 3\n",
    "N = vpu.ve * N_chunks\n",
    "\n",
    "# generate coefficients and data for test\n",
    "W = np.random.randint(int8min, int8max, size=(vpu.acc_period, N), dtype=np.int8)\n",
    "x = np.random.randint(int8min, int8max, size=(N,), dtype=np.int8)\n",
    "\n",
    "# reference output\n",
    "y = np.matmul(np.int32(W), np.int32(x))\n",
    "\n",
    "# mimic XS3 layout in memory\n",
    "W_XS3 = np.copy(W.flatten())\n",
    "x_XS3 = np.copy(x)\n",
    "\n",
    "compute_tile(vpu, W_XS3, x_XS3, N_chunks=N_chunks,\n",
    "             W_start=0, W_step=N, W_chunk_step=vpu.ve,\n",
    "             x_start=0)\n",
    "print(\"Result matches reference: {}\".format(np.all(np.all(vpu._combine_vD_vR() == y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A band is a sequence of tiles producing a single vector of outputs, operating on a sequence of equally spaced continously layed out data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_band(vpu, W, x, N_chunks, N_tiles, W_start, W_step, W_chunk_step, W_tile_step, x_start, x_tile_step):\n",
    "    # N_tiles * (N_chunks * (17 + 2) + 5 + 2) + 5\n",
    "    rx = x_start; rw = W_start\n",
    "    for _ in range(N_tiles):\n",
    "        compute_tile(vpu, W, x, N_chunks, W_start=rw, W_step=W_step, W_chunk_step=W_chunk_step, x_start=rx)\n",
    "        rx += x_tile_step; rw += W_tile_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpu = XS3VPU(bpe=8)\n",
    "N_chunks, N_tiles = 3, 2\n",
    "N = vpu.ve * N_chunks * N_tiles\n",
    "\n",
    "# generate coefficients and data for test\n",
    "W = np.random.randint(int8min, int8max, size=(vpu.acc_period, N), dtype=np.int8)\n",
    "x = np.random.randint(int8min, int8max, size=(N,), dtype=np.int8)\n",
    "\n",
    "# reference output\n",
    "y = np.matmul(np.int32(W), np.int32(x))\n",
    "\n",
    "# mimic XS3 layout in memory\n",
    "W_XS3 = np.copy(W.flatten())\n",
    "x_XS3 = np.copy(x)\n",
    "\n",
    "compute_band(vpu, W_XS3, x_XS3, N_chunks=N_chunks, N_tiles=N_tiles,\n",
    "             W_start=0, W_step=N, W_chunk_step=vpu.ve, W_tile_step=N_chunks*vpu.ve,\n",
    "             x_start=0, x_tile_step=N_chunks*vpu.ve)\n",
    "print(\"Result matches reference: {}\".format(np.all(np.all(vpu._combine_vD_vR() == y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XS3_matmul(vpu, W, x, y, N_bands, N_chunks):\n",
    "    # ~ N_bands * (N_chunks * (17 + 2) + 5 + 8) + 5\n",
    "    rw = 0; ry = 0\n",
    "    for _ in range(N_bands):\n",
    "        vpu.VCLRDR()  # TODO add bias loading\n",
    "        compute_tile(vpu, W, x, N_chunks,\n",
    "                     W_start=rw, W_step=N_chunks*vpu.ve, W_chunk_step=vpu.ve,\n",
    "                     x_start=0)\n",
    "        y[ry:ry+vpu.acc_period] = vpu._combine_vD_vR()  # VLSAT, VPOS, VSTRPV\n",
    "        rw += vpu.acc_period * N_chunks * vpu.ve; ry += vpu.acc_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpu = XS3VPU(bpe=8)\n",
    "N_chunks, N_bands = 3, 2\n",
    "N = N_chunks * vpu.ve\n",
    "M = N_bands * vpu.acc_period\n",
    "\n",
    "# generate coefficients and data\n",
    "W = np.random.randint(int8min, int8max, size=(M, N), dtype=np.int8)\n",
    "x = np.random.randint(int8min, int8max, size=(N,), dtype=np.int8)\n",
    "\n",
    "# reference output\n",
    "y = np.matmul(np.int32(W), np.int32(x))\n",
    "\n",
    "# mimic XS3 layout in memory\n",
    "W_XS3 = np.copy(W.flatten())\n",
    "x_XS3 = np.copy(x)\n",
    "y_XS3 = np.zeros(y.shape, dtype=y.dtype)\n",
    "\n",
    "XS3_matmul(vpu, W_XS3, x_XS3, y_XS3, N_bands, N_chunks)\n",
    "print(\"Output matches reference: {}\".format(np.all(y_XS3 == y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1x1 Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XS3_conv2d_1x1(vpu, K, x, y, height, width, C_out_bands, C_in_chunks):\n",
    "    rx = 0; ry = 0;\n",
    "    for _ in range(height * width):\n",
    "        XS3_matmul(vpu, K, x[rx:], y[ry:], C_out_bands, C_in_chunks)\n",
    "        rx += C_in_chunks * vpu.ve\n",
    "        ry += C_out_bands * vpu.acc_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpu = XS3VPU(bpe=8)\n",
    "C_in_chunks, C_out_bands = 2, 3\n",
    "C_in = C_in_chunks * vpu.ve\n",
    "C_out = C_out_bands * vpu.acc_period\n",
    "width = 4\n",
    "height = 6\n",
    "\n",
    "# generate kernels and data\n",
    "K = np.random.randint(int8min, int8max, size=(C_out, C_in), dtype=np.int8)\n",
    "D = np.random.randint(int8min, int8max, size=(height, width, C_in), dtype=np.int8)\n",
    "\n",
    "# convert to tf.float64, because tf.nn.conv2d cannot handle integer tensors\n",
    "D_tf = tf.convert_to_tensor(D, dtype=tf.float64)\n",
    "D_tf = tf.expand_dims(D_tf, axis=0)\n",
    "K_tf = tf.convert_to_tensor(K.T, dtype=tf.float64)\n",
    "K_tf = tf.expand_dims(tf.expand_dims(K_tf, axis=0), axis=0)\n",
    "\n",
    "# reference output\n",
    "Y_tf = tf.nn.conv2d(D_tf, K_tf, strides=1, padding=\"VALID\", data_format='NHWC')\n",
    "Y_tf = tf.cast(Y_tf, tf.int32)\n",
    "Y = Y_tf.numpy()\n",
    "\n",
    "# mimic XS3 layout in memory\n",
    "K_XS3 = np.copy(K.flatten())\n",
    "D_XS3 = np.copy(D.flatten())\n",
    "Y_XS3 = np.zeros((height, width, C_out), dtype=np.int32).flatten()\n",
    "\n",
    "XS3_conv2d_1x1(vpu, K_XS3, D_XS3, Y_XS3, height, width, C_out_bands, C_in_chunks)\n",
    "print(\"Output matches reference: {}\".format(np.all(Y_XS3 == Y.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NxK Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XS3_conv2d(vpu, K, x, y, height, width, K_h, K_w, C_out_bands, C_in_chunks):\n",
    "    # (height - K_h + 1) * (width - K_w + 1) * (C_out_bands * (K_h * (K_w*C_in_chunks * (17 + 2) + 5 + 2) + 5 + 8) + 5)\n",
    "    rx = 0;# ry = 0;\n",
    "    for hi in range(height - K_h + 1):\n",
    "        for wi in range(width - K_w + 1):\n",
    "            rw = 0;\n",
    "            rx = (hi * width + wi) * C_in_chunks * vpu.ve\n",
    "            ry = (hi * (width - K_w + 1) + wi) * C_out_bands * vpu.acc_period\n",
    "            for _ in range(C_out_bands):\n",
    "                vpu.VCLRDR()  # load bias\n",
    "                compute_band(vpu, K, x, N_chunks=K_w*C_in_chunks, N_tiles=K_h,\n",
    "                             W_start=rw, W_step=K_h*K_w*C_in_chunks*vpu.ve,\n",
    "                             W_chunk_step=vpu.ve, W_tile_step=K_w*C_in_chunks*vpu.ve,\n",
    "                             x_start=rx, x_tile_step=width*C_in_chunks*vpu.ve)\n",
    "                y[ry:ry+vpu.acc_period] = vpu._combine_vD_vR()  # VLSAT, VPOS, VSTRPV\n",
    "                rw += vpu.acc_period * K_h * K_w * C_in_chunks * vpu.ve;\n",
    "                ry += vpu.acc_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpu = XS3VPU(bpe=8)\n",
    "C_in_chunks, C_out_bands = 2, 3\n",
    "C_in = C_in_chunks * vpu.ve\n",
    "C_out = C_out_bands * vpu.acc_period\n",
    "height, width = 7, 5\n",
    "K_h, K_w = 3, 4\n",
    "\n",
    "# generate kernels and data\n",
    "K = np.random.randint(int8min, int8max, size=(C_out, K_h, K_w, C_in), dtype=np.int8)\n",
    "D = np.random.randint(int8min, int8max, size=(height, width, C_in), dtype=np.int8)\n",
    "\n",
    "# convert to tf.float64, because tf.nn.conv2d cannot handle integer tensors\n",
    "D_tf = tf.convert_to_tensor(D, dtype=tf.float64)\n",
    "D_tf = tf.expand_dims(D_tf, axis=0)\n",
    "K_tf = tf.convert_to_tensor(K, dtype=tf.float64)\n",
    "K_tf = tf.transpose(K_tf, perm=[1, 2, 3, 0])\n",
    "\n",
    "# reference output\n",
    "Y_tf = tf.nn.conv2d(D_tf, K_tf, strides=1, padding=\"VALID\", data_format='NHWC')\n",
    "Y_tf = tf.cast(Y_tf, tf.int32)\n",
    "Y = Y_tf.numpy()\n",
    "\n",
    "# mimic XS3 layout in memory\n",
    "K_XS3 = np.copy(K.flatten())\n",
    "D_XS3 = np.copy(D.flatten())\n",
    "Y_XS3 = np.zeros((height - K_h + 1, width - K_w + 1, C_out), dtype=np.int32).flatten()\n",
    "\n",
    "XS3_conv2d(vpu, K_XS3, D_XS3, Y_XS3, height, width, K_h, K_w, C_out_bands, C_in_chunks)\n",
    "print(\"Output matches reference: {}\".format(np.all(Y_XS3 == Y.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick and dirty estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr_buffer_factor = 1/0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an element-wise max on two vectors can be calculated using 8 insturctions\n",
    "# the udge factor is estimated by doing this three times for each 2x2 max pool kernel\n",
    "def max_pool_cnt(height, width, depth, price_per_vector=8):\n",
    "    return (width // 2) * (height // 2) * (depth // 32) * 3 * price_per_vector\n",
    "def conv2d_cnt(height, width, C_in, C_out, K_h=5, K_w=5):\n",
    "    num_out_pixels = (height) * (width)\n",
    "    return num_out_pixels * ((C_out//16) * (K_h * (K_w*(C_in//32) * (17 + 2) + 5 + 2) + 5 + 8) + 5)\n",
    "def fc_cnt(N_in, N_out):\n",
    "    return (N_out // 16) * ((N_in // 32) * (17 + 2) + 5 + 8) + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARM CMSIS-NN\n",
    "\n",
    "Estimate our performance on the exact network described here:\n",
    "\n",
    "https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/new-neural-network-kernels-boost-efficiency-in-microcontrollers-by-5x\n",
    "\n",
    "Note that the below estimates holds for a single thread executing the layers sequentially. We estimate that since the bulk of the computation can be spatially parallelized within each conv2d layer, we could achieve a 5x performance increase on a single tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = []\n",
    "# layer 1, consider 32 input channels instead of 3\n",
    "cnt.append(conv2d_cnt(height=32, width=32, C_in=32, C_out=32))\n",
    "# layer 2\n",
    "cnt.append(max_pool_cnt(height=32, width=32, depth=32))\n",
    "# layer 3\n",
    "cnt.append(conv2d_cnt(height=16, width=16, C_in=32, C_out=32))\n",
    "# layer 4\n",
    "cnt.append(max_pool_cnt(height=16, width=16, depth=32))\n",
    "# layer 5\n",
    "cnt.append(conv2d_cnt(height=8, width=8, C_in=32, C_out=64))\n",
    "# layer 6\n",
    "cnt.append(max_pool_cnt(height=8, width=8, depth=64))\n",
    "# layer 7, consider 16 input channels instead of 10\n",
    "cnt.append(fc_cnt(N_in=4*4*64, N_out=16))\n",
    "\n",
    "t_est = np.sum(cnt) * instr_buffer_factor * 7 / 8e8 * 1e3\n",
    "print(\"Estimate with wasteful input and output layers: {:.2f} ms\".format(t_est))\n",
    "print(\"Layer-wise break down of instruction counts:\")\n",
    "for j, c in enumerate(cnt):\n",
    "    print(\"Layer {}: {:.0f}\".format(j+1, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_efficient = cnt.copy()\n",
    "# layer 1, consider 32 input channels instead of 3\n",
    "cnt_efficient[0] *= 3/32 * 2\n",
    "# layer 7, consider 16 input channels instead of 10\n",
    "cnt_efficient[-1] *= 10/16\n",
    "\n",
    "t_est_eff = np.sum(cnt_efficient) * instr_buffer_factor * 7 / 8e8 * 1e3\n",
    "print(\"Estimate with very efficient input and output layers: {:.2f} ms\".format(t_est_eff))\n",
    "print(\"Layer-wise break down of instruction counts:\")\n",
    "for j, c in enumerate(cnt_efficient):\n",
    "    print(\"Layer {}: {:.0f}\".format(j+1, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\")\n",
    "print(\"Parallelized inference on the network described above would take ~{:.2f} ms\".format(t_est_eff/5))\n",
    "print(\"This is a ~{:d}x speedup compared to ARM's benchmark of {:.1f} ms\".format(int(99.1/t_est_eff*5), 99.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GreenWaves GAP8\n",
    "\n",
    "Estimate our performance on the layers described here:\n",
    "\n",
    "https://greenwaves-technologies.com/gap8-cnn-benchmarks/\n",
    "\n",
    "The GAP8 chip parallelizes convolutions differently: it evaluates each input layer sequentially, which is not the most efficient use of our vector unit. These calculations are less reliable, but were done to intentionally overestimate our instruction counts. Our system would also scale much better as the number of input and output layers grow. Our approach is also tends to be agnostic of the image and kernel size, while GAP8's approach has hard assumptions on possible kernel sizes, outside which it would quickly loose performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def max_pool_cnt_single_layer(height, width, price_per_vector=8):\n",
    "    # do a 2x1 max pool first\n",
    "    # then a 1x2 max pool using a trick by switching to 16 bit mode and shifting down by 8bits\n",
    "    return (height // 2) * ceil(width/32) * price_per_vector + ceil(width/32) * (price_per_vector + 4)\n",
    "\n",
    "def ave_pool_cnt_single_layer(height, width, price_per_vector=12):\n",
    "    # roughly 16 instruction for 2x16 8-bit values\n",
    "    return (height / 2) * (width / 16) * price_per_vector\n",
    "\n",
    "def conv2d_cnt_single_layer(height, width, K_h=5, K_w=5):\n",
    "    # model each convolution by a sequence of VLDC and VLMACCR, followed by a VADDR, VLSAT, VSTRPV and VCLRDR\n",
    "    # VPOS intentionally not counted since the GAP8 benchmark makes no reference to activations\n",
    "    # also, they make no mention of zero padding, so no padding is assumed\n",
    "    price_per_conv = K_h*2 + 4\n",
    "    return (height - K_h + 1) * (width - K_w + 1) * price_per_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cnt = max_pool_cnt_single_layer(height=112, width=112)\n",
    "print(\"Estimate of 2x2/2 max pooling on a 112x112 input: {:.2f} us\".format(\n",
    "      m_cnt * instr_buffer_factor * 7 / 8e8 * 1e6))\n",
    "print(\"Estimate when parallelized: {:.2f} us\".format(\n",
    "      m_cnt * instr_buffer_factor * 7 / 8e8 * 1e6 / 5))\n",
    "print(\"GAP8's benchmark is: {} us\".format(2014/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_cnt = ave_pool_cnt_single_layer(height=112, width=112)\n",
    "print(\"Estimate of 2x2/2 average pooling on a 112x112 input: {:.2f} us\".format(\n",
    "      a_cnt * instr_buffer_factor * 7 / 8e8 * 1e6))\n",
    "print(\"Estimate when parallelized: {:.2f} us\".format(\n",
    "      a_cnt * instr_buffer_factor * 7 / 8e8 * 1e6 / 5))\n",
    "print(\"GAP8's benchmark is: {} us\".format(1861/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cnt = conv2d_cnt_single_layer(height=(112-5+1), width=(112-5+1))\n",
    "print(\"Estimate of 5x5 conv2d on a 112x112 input: {:.2f} us\".format(\n",
    "      c_cnt * instr_buffer_factor * 7 / 8e8 * 1e6))\n",
    "print(\"Estimate when parallelized: {:.2f} us\".format(\n",
    "      c_cnt * instr_buffer_factor * 7 / 8e8 * 1e6 / 5))\n",
    "print(\"GAP8's benchmark is: {} us\".format(12848/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cnt = fc_cnt(N_in=16, N_out=1024)\n",
    "print(\"Estimate of 1024x16 fully connected layer : {:.2f} us\".format(\n",
    "      f_cnt * instr_buffer_factor * 7 / 8e8 * 1e6))\n",
    "print(\"Estimate when parallelized: {:.2f} us\".format(\n",
    "      f_cnt * instr_buffer_factor * 7 / 8e8 * 1e6 / 5))\n",
    "print(\"GAP8's benchmark is: {} us\".format(1252/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion: instead of using the same benchmark as GAP8, we should report the number of 5x5 single layer convolutions our chip can perform per second, and calculate this at the most optimal setup for our chip (Undoubtedly, GAP8 chose this metric because they have significant overhead as the number of layers grow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_cnt = conv2d_cnt(height=(112-5+1), width=(112-5+1), C_in=32, C_out=16)\n",
    "batch_time = batch_cnt * instr_buffer_factor * 7 / 8e8\n",
    "print(\"Estimated number of 5x5 conv2d on a 112x112 input every second (including overhead): {:.2f}/s\".format(\n",
    "      (32*16) / batch_time))\n",
    "print(\"Estimate when parallelized (including overhead): {:.2f}/s\".format(\n",
    "      5 * (32*16) / batch_time))\n",
    "print(\"GAP8's benchmark (without overhead) is: {} us\".format(1/(12848/100/1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\")\n",
    "print(\"(Assuming a very reasonable 20% overhead for GAP8)\")\n",
    "print(\"We have a ~{:.2f}x speedup compared to GAP8's benchmark in terms of convolution layers per second.\".format(\n",
    "    5 * (32*16) / batch_time * (1.2*12848/100/1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
