diff --git larq_compute_engine/mlir/BUILD larq_compute_engine/mlir/BUILD
index 792d189..e3a5dc5 100644
--- larq_compute_engine/mlir/BUILD
+++ larq_compute_engine/mlir/BUILD
@@ -1,4 +1,4 @@
-load("@org_tensorflow//third_party/mlir:tblgen.bzl", "gentbl")
+load("@org_tensorflow//third_party/mlir:tblgen.bzl", "gentbl", "td_library")
 load("@org_tensorflow//tensorflow:tensorflow.bzl", "pybind_extension", "tf_cc_binary")
 
 package(
@@ -6,9 +6,22 @@ package(
     licenses = ["notice"],  # Apache 2.0
 )
 
+td_library(
+    name = "lce_ops_td_file",
+    srcs = [
+        "ir/lce_ops.td",
+    ],
+    deps = [
+        "@llvm-project//mlir:SideEffectTdFiles",
+        "@org_tensorflow//tensorflow/compiler/mlir/lite/quantization:quantization_td_files",
+    ],
+)
+
 gentbl(
     name = "lce_ops_inc_gen",
     tbl_outs = [
+        ("-gen-enum-decls", "ir/lce_enum.h.inc"),
+        ("-gen-enum-defs", "ir/lce_enum.cc.inc"),
         ("-gen-op-decls", "ir/lce_ops.h.inc"),
         ("-gen-op-defs", "ir/lce_ops.cc.inc"),
         ("-gen-dialect-decls -dialect=lq", "ir/lce_dialect.h.inc"),
@@ -18,8 +31,7 @@ gentbl(
     td_file = "ir/lce_ops.td",
     td_includes = ["external/org_tensorflow"],
     td_srcs = [
-        "@org_tensorflow//tensorflow/compiler/mlir/lite/quantization:quantization_td_files",
-        "@llvm-project//mlir:SideEffectTdFiles",
+        ":lce_ops_td_file",
     ],
 )
 
@@ -46,7 +58,7 @@ gentbl(
     td_file = "transforms/prepare_patterns_target_arm.td",
     td_includes = ["external/org_tensorflow"],
     td_srcs = [
-        "ir/lce_ops.td",
+        ":lce_ops_td_file",
         "transforms/op_removal_patterns.td",
         "transforms/prepare_patterns_common.td",
         "@llvm-project//mlir:StdOpsTdFiles",
@@ -64,7 +76,7 @@ gentbl(
     td_file = "transforms/prepare_patterns_common.td",
     td_includes = ["external/org_tensorflow"],
     td_srcs = [
-        "ir/lce_ops.td",
+        ":lce_ops_td_file",
         "transforms/op_removal_patterns.td",
         "@llvm-project//mlir:StdOpsTdFiles",
         "@org_tensorflow//tensorflow/compiler/mlir/tensorflow:tensorflow_ops_td_files",
@@ -81,7 +93,7 @@ gentbl(
     td_file = "transforms/optimize_patterns_target_arm.td",
     td_includes = ["external/org_tensorflow"],
     td_srcs = [
-        "ir/lce_ops.td",
+        ":lce_ops_td_file",
         "transforms/optimize_patterns_common.td",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite_ops_td_files",
         "@llvm-project//mlir:StdOpsTdFiles",
@@ -97,7 +109,7 @@ gentbl(
     td_file = "transforms/optimize_patterns_common.td",
     td_includes = ["external/org_tensorflow"],
     td_srcs = [
-        "ir/lce_ops.td",
+        ":lce_ops_td_file",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite_ops_td_files",
         "@llvm-project//mlir:StdOpsTdFiles",
     ],
@@ -112,7 +124,7 @@ gentbl(
     td_file = "transforms/bitpack_weights_patterns.td",
     td_includes = ["external/org_tensorflow"],
     td_srcs = [
-        "ir/lce_ops.td",
+        ":lce_ops_td_file",
         "transforms/op_removal_patterns.td",
         "@org_tensorflow//tensorflow/compiler/mlir/tensorflow:tensorflow_ops_td_files",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite_ops_td_files",
@@ -129,7 +141,7 @@ gentbl(
     td_file = "transforms/quantize_patterns.td",
     td_includes = ["external/org_tensorflow"],
     td_srcs = [
-        "ir/lce_ops.td",
+        ":lce_ops_td_file",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite_ops_td_files",
     ],
 )
@@ -146,6 +158,7 @@ cc_library(
         "//larq_compute_engine/core:types",
         "//larq_compute_engine/core/bitpacking:bitpack",
         "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:QuantOps",
     ],
 )
 
@@ -153,6 +166,8 @@ cc_library(
     name = "larq_compute_engine",
     srcs = [
         "ir/lce_dialect.h.inc",
+        "ir/lce_enum.cc.inc",
+        "ir/lce_enum.h.inc",
         "ir/lce_ops.cc",
         "ir/lce_ops.cc.inc",
         "ir/lce_ops.h.inc",
@@ -263,6 +278,21 @@ cc_library(
     alwayslink = 1,
 )
 
+cc_library(
+    name = "larq_compute_engine_translate_tflite",
+    srcs = [
+        "transforms/translate_tflite.cc",
+    ],
+    hdrs = [
+        "transforms/passes.h",
+    ],
+    deps = [
+        ":larq_compute_engine",
+        "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite",
+    ],
+    alwayslink = 1,
+)
+
 cc_library(
     name = "larq_compute_engine_quantize",
     srcs = [
@@ -279,6 +309,20 @@ cc_library(
     alwayslink = 1,
 )
 
+cc_library(
+    name = "set_batch_size",
+    srcs = [
+        "transforms/set_batch_size.cc",
+    ],
+    hdrs = [
+        "transforms/passes.h",
+    ],
+    deps = [
+        "@llvm-project//mlir:StandardOps",
+    ],
+    alwayslink = 1,
+)
+
 cc_library(
     name = "lce_tfl_passes",
     srcs = ["tf_tfl_passes.cc"],
@@ -292,10 +336,13 @@ cc_library(
         ":larq_compute_engine_optimize",
         ":larq_compute_engine_prepare",
         ":larq_compute_engine_quantize",
+        ":larq_compute_engine_translate_tflite",
+        ":set_batch_size",
         "@llvm-project//mlir:IR",
         "@llvm-project//mlir:Pass",
         "@llvm-project//mlir:QuantOps",
         "@llvm-project//mlir:Transforms",
+        "@org_tensorflow//tensorflow/compiler/mlir/lite:fake_quant_utils",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite_legalize_tf",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite_optimize",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite_quantize",
@@ -345,6 +392,7 @@ pybind_extension(
     deps = [
         ":lce_tfl_passes",
         ":tf_to_tfl_flatbuffer",
+        "@org_tensorflow//tensorflow/cc/saved_model:loader",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tensorflow_lite",
         "@org_tensorflow//tensorflow/compiler/mlir/lite:tf_to_tfl_flatbuffer",
         "@org_tensorflow//tensorflow/compiler/mlir/lite/python:tf_tfl_flatbuffer_helpers",
diff --git larq_compute_engine/mlir/ir/lce_ops.cc larq_compute_engine/mlir/ir/lce_ops.cc
index ee024d6..f9cc1d1 100644
--- larq_compute_engine/mlir/ir/lce_ops.cc
+++ larq_compute_engine/mlir/ir/lce_ops.cc
@@ -5,22 +5,8 @@
 #include "larq_compute_engine/mlir/transforms/bitpack.h"
 #include "tensorflow/lite/schema/schema_generated.h"
 
-static tflite::Padding ConvertPaddingAttr(llvm::StringRef str) {
-  return llvm::StringSwitch<tflite::Padding>(str)
-      .Case("SAME", tflite::Padding_SAME)
-      .Case("VALID", tflite::Padding_VALID);
-}
-
-static tflite::ActivationFunctionType ConvertActivationAttr(
-    llvm::StringRef str) {
-  return llvm::StringSwitch<tflite::ActivationFunctionType>(str)
-      .Case("NONE", tflite::ActivationFunctionType_NONE)
-      .Case("RELU", tflite::ActivationFunctionType_RELU)
-      .Case("RELU_N1_TO_1", tflite::ActivationFunctionType_RELU_N1_TO_1)
-      .Case("RELU6", tflite::ActivationFunctionType_RELU6);
-}
-
 #define GET_OP_CLASSES
+#include "larq_compute_engine/mlir/ir/lce_enum.cc.inc"
 #include "larq_compute_engine/mlir/ir/lce_ops.cc.inc"
 
 namespace mlir {
@@ -36,9 +22,10 @@ std::vector<uint8_t> Bconv2dOp::buildCustomOptions() {
     fbb.Int("dilation_height_factor", dilation_height_factor());
     fbb.Int("dilation_width_factor", dilation_width_factor());
     fbb.Int("fused_activation_function",
-            (int)ConvertActivationAttr(fused_activation_function()));
+            (int)symbolizeActivationFunctionType(fused_activation_function())
+                .getValue());
     fbb.Int("pad_values", pad_values());
-    fbb.Int("padding", (int)ConvertPaddingAttr(padding()));
+    fbb.Int("padding", (int)symbolizePadding(padding()).getValue());
     fbb.Int("stride_height", stride_height());
     fbb.Int("stride_width", stride_width());
   });
@@ -49,7 +36,7 @@ std::vector<uint8_t> Bconv2dOp::buildCustomOptions() {
 std::vector<uint8_t> BMaxPool2dOp::buildCustomOptions() {
   flexbuffers::Builder fbb;
   fbb.Map([&]() {
-    fbb.Int("padding", (int)ConvertPaddingAttr(padding()));
+    fbb.Int("padding", (int)symbolizePadding(padding()).getValue());
     fbb.Int("stride_width", stride_width());
     fbb.Int("stride_height", stride_height());
     fbb.Int("filter_width", filter_width());
diff --git larq_compute_engine/mlir/ir/lce_ops.h larq_compute_engine/mlir/ir/lce_ops.h
index f19dd81..9becf0a 100644
--- larq_compute_engine/mlir/ir/lce_ops.h
+++ larq_compute_engine/mlir/ir/lce_ops.h
@@ -8,6 +8,8 @@
 #include "larq_compute_engine/mlir/ir/lce_dialect.h.inc"
 // clang-format on
 
+#include "larq_compute_engine/mlir/ir/lce_enum.h.inc"
+
 #define GET_OP_CLASSES
 #include "larq_compute_engine/mlir/ir/lce_ops.h.inc"
 
diff --git larq_compute_engine/mlir/lce_mlir_opt.cc larq_compute_engine/mlir/lce_mlir_opt.cc
index 22a8431..3678051 100644
--- larq_compute_engine/mlir/lce_mlir_opt.cc
+++ larq_compute_engine/mlir/lce_mlir_opt.cc
@@ -12,6 +12,7 @@ int main(int argc, char** argv) {
   registry.insert<mlir::StandardOpsDialect, mlir::quant::QuantizationDialect,
                   mlir::TF::TensorFlowDialect, mlir::TFL::TensorFlowLiteDialect,
                   mlir::lq::LarqDialect>();
-  return failed(mlir::MlirOptMain(
-      argc, argv, "Larq Compute Engine pass driver\n", registry));
+  return failed(mlir::MlirOptMain(argc, argv,
+                                  "Larq Compute Engine pass driver\n", registry,
+                                  /*preloadDialectsInContext=*/false));
 }
diff --git larq_compute_engine/mlir/python/saved_model_tfl_flatbuffer.cc larq_compute_engine/mlir/python/saved_model_tfl_flatbuffer.cc
index 832a6e0..2b69564 100644
--- larq_compute_engine/mlir/python/saved_model_tfl_flatbuffer.cc
+++ larq_compute_engine/mlir/python/saved_model_tfl_flatbuffer.cc
@@ -15,6 +15,7 @@ limitations under the License.
 ==============================================================================*/
 
 #include <exception>
+#include <memory>
 #include <utility>
 
 #include "absl/types/span.h"
@@ -32,6 +33,7 @@ limitations under the License.
 #include "pybind11/pybind11.h"
 #include "pybind11/pytypes.h"
 #include "pybind11/stl.h"
+#include "tensorflow/cc/saved_model/loader.h"
 #include "tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.h"
 #include "tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.h"
 #include "tensorflow/compiler/mlir/lite/transforms/passes.h"
@@ -124,9 +126,11 @@ pybind11::bytes ConvertSavedModelToTFLiteFlatBuffer(
   std::unordered_set<std::string> tags(saved_model_tags.begin(),
                                        saved_model_tags.end());
 
+  auto bundle = std::make_unique<tensorflow::SavedModelBundle>();
   auto module =
       ImportSavedModel(saved_model_dir, saved_model_version, tags,
-                       custom_opdefs, exported_names_span, specs, &context);
+                       custom_opdefs, exported_names_span, specs,
+                       /*enable_variable_lifting=*/true, &context, &bundle);
 
   if (!module.ok()) {
     throw std::runtime_error("Could not import SavedModel.");
diff --git larq_compute_engine/mlir/tests/lce_ops_options_test.cc larq_compute_engine/mlir/tests/lce_ops_options_test.cc
index 381435b..26b4043 100644
--- larq_compute_engine/mlir/tests/lce_ops_options_test.cc
+++ larq_compute_engine/mlir/tests/lce_ops_options_test.cc
@@ -60,9 +60,9 @@ TEST(LCEOpsSerializationTest, BConv2dTest) {
   ASSERT_EQ(m["stride_height"].AsInt32(), 1);
   ASSERT_EQ(m["stride_width"].AsInt32(), 2);
   ASSERT_EQ(m["pad_values"].AsInt32(), 1);
-  ASSERT_EQ((ActivationFunctionType)m["fused_activation_function"].AsInt32(),
-            ActivationFunctionType_RELU);
-  ASSERT_EQ((Padding)m["padding"].AsInt32(), Padding_SAME);
+  ASSERT_EQ((::ActivationFunctionType)m["fused_activation_function"].AsInt32(),
+            ::ActivationFunctionType::RELU);
+  ASSERT_EQ((::Padding)m["padding"].AsInt32(), ::Padding::SAME);
 }
 
 TEST(LCEOpsSerializationTest, BMaxPool2dTest) {
@@ -82,7 +82,7 @@ TEST(LCEOpsSerializationTest, BMaxPool2dTest) {
   std::vector<uint8_t> v = cast<lq::BMaxPool2dOp>(op).buildCustomOptions();
   const flexbuffers::Map& m = flexbuffers::GetRoot(v).AsMap();
 
-  ASSERT_EQ((Padding)m["padding"].AsInt32(), Padding_SAME);
+  ASSERT_EQ((::Padding)m["padding"].AsInt32(), ::Padding::SAME);
   ASSERT_EQ(m["stride_width"].AsInt32(), 2);
   ASSERT_EQ(m["stride_height"].AsInt32(), 1);
   ASSERT_EQ(m["filter_width"].AsInt32(), 3);
diff --git larq_compute_engine/mlir/tests/legalize-lce.mlir larq_compute_engine/mlir/tests/legalize-lce.mlir
index 4739d72..3230cbe 100644
--- larq_compute_engine/mlir/tests/legalize-lce.mlir
+++ larq_compute_engine/mlir/tests/legalize-lce.mlir
@@ -1,4 +1,5 @@
 // RUN: lce-tf-opt %s -tfl-legalize-lce -verify-diagnostics | FileCheck %s
+// RUN: lce-tf-opt %s -tfl-legalize-lce -lce-translate-tfl -verify-diagnostics | FileCheck %s --check-prefix=TRANSLATE
 
 // CHECK-LABEL: @legalize_bconv2d
 func @legalize_bconv2d(%arg0: tensor<256x32x32x1xi32>, %arg1: tensor<16x3x3x3xf32>, %arg2: tensor<16xf32>, %arg3: tensor<16xf32>, %arg4: none) -> tensor<256x30x30x16xf32> {
@@ -7,6 +8,9 @@ func @legalize_bconv2d(%arg0: tensor<256x32x32x1xi32>, %arg1: tensor<16x3x3x3xf3
 
   // CHECK: %0 = "tfl.custom"(%arg0, %arg1, %arg2, %arg3, %arg4) {custom_code = "LceBconv2d", custom_option = opaque<"lq", "0x6368616E6E656C735F696E0064696C6174696F6E5F6865696768745F666163746F720064696C6174696F6E5F77696474685F666163746F720066757365645F61637469766174696F6E5F66756E6374696F6E007061645F76616C7565730070616464696E67007374726964655F686569676874007374726964655F776964746800088277614C3329221508010803010100000101010404040404040404102401"> : tensor<160xi8>} : (tensor<256x32x32x1xi32>, tensor<16x3x3x3xf32>, tensor<16xf32>, tensor<16xf32>, none) -> tensor<256x30x30x16xf32>
   // CHECK-NEXT: return %0
+
+  // TRANSLATE: %0 = "lq.Bconv2d"(%arg0, %arg1, %arg2, %arg3, %arg4) {channels_in = 3 : i32, dilation_height_factor = 1 : i32, dilation_width_factor = 1 : i32, fused_activation_function = "NONE", pad_values = 0 : i32, padding = "VALID", stride_height = 1 : i32, stride_width = 1 : i32} : (tensor<256x32x32x1xi32>, tensor<16x3x3x3xf32>, tensor<16xf32>, tensor<16xf32>, none) -> tensor<256x30x30x16xf32>
+  // TRANSLATE-NEXT: return %0 : tensor<256x30x30x16xf32>
 }
 
 // CHECK-LABEL: @legalize_bmax_pool2d
@@ -16,6 +20,9 @@ func @legalize_bmax_pool2d(%arg0: tensor<256x32x32x3xi32>) -> tensor<256x16x16x3
 
   // CHECK: %0 = "tfl.custom"(%arg0) {custom_code = "LceBMaxPool2d", custom_option = opaque<"lq", "0x70616464696E67007374726964655F7769647468007374726964655F6865696768740066696C7465725F77696474680066696C7465725F68656967687400050F1D412D3B050105020200020204040404040A2401"> : tensor<84xi8>} : (tensor<256x32x32x3xi32>) -> tensor<256x16x16x3xi32>
   // CHECK-NEXT: return %0
+
+  // TRANSLATE: %0 = "lq.BMaxPool2d"(%arg0) {filter_height = 2 : i32, filter_width = 2 : i32, padding = "SAME", stride_height = 2 : i32, stride_width = 2 : i32} : (tensor<256x32x32x3xi32>) -> tensor<256x16x16x3xi32>
+  // TRANSLATE-NEXT: return %0 : tensor<256x16x16x3xi32>
 }
 
 // CHECK-LABEL: @legalize_quantize
@@ -25,6 +32,9 @@ func @legalize_quantize(%arg0: tensor<256x32x32x64xf32>) -> tensor<256x32x32x2xi
 
   // CHECK: %0 = "tfl.custom"(%arg0) {custom_code = "LceQuantize", custom_option = opaque<"lq", "0x"> : tensor<0xi8>} : (tensor<256x32x32x64xf32>) -> tensor<256x32x32x2xi32>
   // CHECK-NEXT: return %0
+
+  // TRANSLATE: %0 = "lq.Quantize"(%arg0) : (tensor<256x32x32x64xf32>) -> tensor<256x32x32x2xi32>
+  // TRANSLATE-NEXT: return %0 : tensor<256x32x32x2xi32>
 }
 
 // CHECK-LABEL: @legalize_dequantize
@@ -34,4 +44,7 @@ func @legalize_dequantize(%arg0: tensor<256x32x32x2xi32>) -> tensor<256x32x32x64
 
   // CHECK: %0 = "tfl.custom"(%arg0) {custom_code = "LceDequantize", custom_option = opaque<"lq", "0x"> : tensor<0xi8>} : (tensor<256x32x32x2xi32>) -> tensor<256x32x32x64xf32>
   // CHECK-NEXT: return %0
+
+  // TRANSLATE: %0 = "lq.Dequantize"(%arg0) : (tensor<256x32x32x2xi32>) -> tensor<256x32x32x64xf32>
+  // TRANSLATE-NEXT: return %0 : tensor<256x32x32x64xf32>
 }
diff --git larq_compute_engine/mlir/tests/set_batch_size.mlir larq_compute_engine/mlir/tests/set_batch_size.mlir
new file mode 100644
index 0000000..203291c
--- /dev/null
+++ larq_compute_engine/mlir/tests/set_batch_size.mlir
@@ -0,0 +1,51 @@
+// RUN: lce-tf-opt %s -mlir-setbatchsize -verify-diagnostics | FileCheck %s
+
+// CHECK-LABEL: @simple
+func @simple(%arg0: tensor<?x6xf32>, %arg1: tensor<2x6xf32>) -> (tensor<?x6xf32>) {
+  %0 = "tf.AddV2"(%arg0, %arg1) : (tensor<?x6xf32>, tensor<2x6xf32>) -> tensor<?x6xf32>
+  return %0 : tensor<?x6xf32>
+  // CHECK: %arg0: tensor<1x6xf32>
+  // Check that the 'batch' size of the second argument is *not* changed to 1
+  // CHECK: %arg1: tensor<2x6xf32>
+}
+
+// This is an IR dump from the following simple 2-input model
+// This is to ensure that the pass does not destroy the extra function attributes that are present
+//    img1 = tf.keras.layers.Input(shape=(4,))
+//    img2 = tf.keras.layers.Input(shape=(6,))
+//    x = tf.keras.layers.Dense(6)(img1) + img2
+//    return tf.keras.Model([img1, img2], x)
+// Both inputs have a dynamic batch size
+
+// CHECK-LABEL: @dual_input_model
+func @dual_input_model(%arg0: tensor<?x6xf32> {tf_saved_model.index_path = ["input_2"]}, %arg1: tensor<?x4xf32> {tf_saved_model.index_path = ["input_1"]}, %arg2: tensor<!tf.resource<tensor<6xf32>>> {tf_saved_model.bound_input = @"dense/bias"}, %arg3: tensor<!tf.resource<tensor<4x6xf32>>> {tf_saved_model.bound_input = @"dense/kernel"}) -> (tensor<?x6xf32> {tf_saved_model.index_path = ["tf.__operators__.add"]}) attributes {tf.entry_function = {control_outputs = "", inputs = "serving_default_input_2:0,serving_default_input_1:0", outputs = "StatefulPartitionedCall:0"}, tf_saved_model.exported_names = ["serving_default"]} {
+  %0 = "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf.resource<tensor<6xf32>>>) -> tensor<6xf32>
+  %1 = "tf.ReadVariableOp"(%arg3) {device = ""} : (tensor<!tf.resource<tensor<4x6xf32>>>) -> tensor<4x6xf32>
+  %2 = "tf.MatMul"(%arg1, %1) {device = "", transpose_a = false, transpose_b = false} : (tensor<?x4xf32>, tensor<4x6xf32>) -> tensor<?x6xf32>
+  %3 = "tf.BiasAdd"(%2, %0) {data_format = "NHWC", device = ""} : (tensor<?x6xf32>, tensor<6xf32>) -> tensor<?x6xf32>
+  %4 = "tf.AddV2"(%3, %arg0) {device = ""} : (tensor<?x6xf32>, tensor<?x6xf32>) -> tensor<?x6xf32>
+  %5 = "tf.Identity"(%4) {device = ""} : (tensor<?x6xf32>) -> tensor<?x6xf32>
+  %6 = "tf.Identity"(%5) {device = ""} : (tensor<?x6xf32>) -> tensor<?x6xf32>
+  return %6 : tensor<?x6xf32>
+  // CHECK: %arg0: tensor<1x6xf32> {tf_saved_model.index_path = ["input_2"]}
+  // CHECK: %arg1: tensor<1x4xf32> {tf_saved_model.index_path = ["input_1"]}
+  // The resource objects and attributes should be unchanged
+  // CHECK: %arg2: tensor<!tf.resource<tensor<6xf32>>> {tf_saved_model.bound_input = @"dense/bias"}, %arg3: tensor<!tf.resource<tensor<4x6xf32>>> {tf_saved_model.bound_input = @"dense/kernel"}) -> (tensor<?x6xf32> {tf_saved_model.index_path = ["tf.__operators__.add"]}) attributes {tf.entry_function = {control_outputs = "", inputs = "serving_default_input_2:0,serving_default_input_1:0", outputs = "StatefulPartitionedCall:0"}, tf_saved_model.exported_names = ["serving_default"]} {
+}
+
+// This is the same model, but one of the two inputs has been given a fixed batch size in Python
+
+// CHECK-LABEL: @dual_input_one_fixed_size
+func @dual_input_one_fixed_size(%arg0: tensor<?x6xf32> {tf_saved_model.index_path = ["input_2"]}, %arg1: tensor<1x4xf32> {tf_saved_model.index_path = ["input_1"]}, %arg2: tensor<!tf.resource<tensor<6xf32>>> {tf_saved_model.bound_input = @"dense/bias"}, %arg3: tensor<!tf.resource<tensor<4x6xf32>>> {tf_saved_model.bound_input = @"dense/kernel"}) -> (tensor<?x6xf32> {tf_saved_model.index_path = ["tf.__operators__.add"]}) attributes {tf.entry_function = {control_outputs = "", inputs = "serving_default_input_2:0,serving_default_input_1:0", outputs = "StatefulPartitionedCall:0"}, tf_saved_model.exported_names = ["serving_default"]} {
+  %0 = "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf.resource<tensor<6xf32>>>) -> tensor<6xf32>
+  %1 = "tf.ReadVariableOp"(%arg3) {device = ""} : (tensor<!tf.resource<tensor<4x6xf32>>>) -> tensor<4x6xf32>
+  %2 = "tf.MatMul"(%arg1, %1) {device = "", transpose_a = false, transpose_b = false} : (tensor<1x4xf32>, tensor<4x6xf32>) -> tensor<1x6xf32>
+  %3 = "tf.BiasAdd"(%2, %0) {data_format = "NHWC", device = ""} : (tensor<1x6xf32>, tensor<6xf32>) -> tensor<1x6xf32>
+  %4 = "tf.AddV2"(%3, %arg0) {device = ""} : (tensor<1x6xf32>, tensor<?x6xf32>) -> tensor<?x6xf32>
+  %5 = "tf.Identity"(%4) {device = ""} : (tensor<?x6xf32>) -> tensor<?x6xf32>
+  %6 = "tf.Identity"(%5) {device = ""} : (tensor<?x6xf32>) -> tensor<?x6xf32>
+  return %6 : tensor<?x6xf32>
+  // CHECK: %arg0: tensor<1x6xf32> {tf_saved_model.index_path = ["input_2"]}
+  // CHECK: %arg1: tensor<1x4xf32> {tf_saved_model.index_path = ["input_1"]}
+  // CHECK: %arg2: tensor<!tf.resource<tensor<6xf32>>> {tf_saved_model.bound_input = @"dense/bias"}, %arg3: tensor<!tf.resource<tensor<4x6xf32>>> {tf_saved_model.bound_input = @"dense/kernel"}) -> (tensor<?x6xf32> {tf_saved_model.index_path = ["tf.__operators__.add"]}) attributes {tf.entry_function = {control_outputs = "", inputs = "serving_default_input_2:0,serving_default_input_1:0", outputs = "StatefulPartitionedCall:0"}, tf_saved_model.exported_names = ["serving_default"]} {
+}
\ No newline at end of file
diff --git larq_compute_engine/mlir/tf_tfl_passes.cc larq_compute_engine/mlir/tf_tfl_passes.cc
index d413a58..66cec67 100644
--- larq_compute_engine/mlir/tf_tfl_passes.cc
+++ larq_compute_engine/mlir/tf_tfl_passes.cc
@@ -6,6 +6,7 @@
 #include "tensorflow/compiler/mlir/lite/quantization/quantization_config.h"
 #include "tensorflow/compiler/mlir/lite/quantization/quantization_passes.h"
 #include "tensorflow/compiler/mlir/lite/transforms/passes.h"
+#include "tensorflow/compiler/mlir/lite/utils/fake_quant_utils.h"
 #include "tensorflow/compiler/mlir/tensorflow/transforms/decode_constant.h"
 #include "tensorflow/compiler/mlir/tensorflow/transforms/passes.h"
 #include "tensorflow/compiler/mlir/tensorflow/transforms/tf_saved_model_passes.h"
@@ -49,6 +50,12 @@ void AddTFToLCETFLConversionPasses(
     const mlir::TFL::QuantizationSpecs& quant_specs,
     mlir::OpPassManager* pass_manager, const LCETarget target,
     const bool experimental_enable_bitpacked_activations) {
+  // This pass wraps all the tf.FakeQuant ops in a custom op so they are not
+  // folded before being converted to tfl.quantize and tfl.dequantize ops.
+  auto wrapped_ops = mlir::TFL::AllTfFakeQuantOps();
+  pass_manager->addNestedPass<mlir::FuncOp>(
+      mlir::TFL::CreateRaiseCustomOpsPass(wrapped_ops));
+
   mlir::TF::StandardPipelineOptions standard_pipeline_options;
   standard_pipeline_options.enable_inliner = false;
   standard_pipeline_options.form_clusters = false;
@@ -92,6 +99,10 @@ void AddTFToLCETFLConversionPasses(
   pass_manager->addPass(
       mlir::tf_saved_model::CreateOptimizeGlobalTensorsPass());
 
+  // Set the batch size of the function input to 1 and let shape inference
+  // propagate this in the next pass.
+  pass_manager->addPass(mlir::CreateSetBatchSizePass());
+
   // Add a shape inference pass to optimize away the unnecessary casts.
   pass_manager->addPass(mlir::TF::CreateTFShapeInferencePass());
 
@@ -171,8 +182,9 @@ void AddTFToLCETFLConversionPasses(
   // so that it can target constants introduced once TensorFlow Identity ops
   // are removed during legalization.
   pass_manager->addPass(mlir::TFL::CreateOptimizeFunctionalOpsPass());
+  std::vector<std::string> empty_wrapped_ops({});
   pass_manager->addNestedPass<mlir::FuncOp>(
-      mlir::TFL::CreateRaiseCustomOpsPass());
+      mlir::TFL::CreateRaiseCustomOpsPass(empty_wrapped_ops));
   pass_manager->addPass(mlir::createSymbolDCEPass());
   pass_manager->addNestedPass<mlir::FuncOp>(mlir::createCanonicalizerPass());
   pass_manager->addNestedPass<mlir::FuncOp>(mlir::createCSEPass());
diff --git larq_compute_engine/mlir/transforms/legalize_tflite.cc larq_compute_engine/mlir/transforms/legalize_tflite.cc
index bb75781..4b1fc5b 100644
--- larq_compute_engine/mlir/transforms/legalize_tflite.cc
+++ larq_compute_engine/mlir/transforms/legalize_tflite.cc
@@ -10,6 +10,9 @@ namespace TFL {
 namespace {
 
 struct LegalizeLCE : public PassWrapper<LegalizeLCE, FunctionPass> {
+  void getDependentDialects(DialectRegistry& registry) const override {
+    registry.insert<mlir::TFL::TensorFlowLiteDialect>();
+  }
   void runOnFunction() override;
 };
 
diff --git larq_compute_engine/mlir/transforms/passes.h larq_compute_engine/mlir/transforms/passes.h
index b9b774b..f251cfc 100644
--- larq_compute_engine/mlir/transforms/passes.h
+++ larq_compute_engine/mlir/transforms/passes.h
@@ -27,7 +27,14 @@ std::unique_ptr<OperationPass<FuncOp>> CreateLCEQuantizePass();
 // Creates an instance of LegalizeLCE pass.
 std::unique_ptr<OperationPass<FuncOp>> CreateLegalizeLCEPass();
 
+// Creates an instance of TranslateToLCE pass.
+std::unique_ptr<OperationPass<FuncOp>> CreateTranslateToLCEPass();
+
 }  // namespace TFL
+
+// Creates an instance of the TensorFlow dialect SetBatchSize pass
+std::unique_ptr<OperationPass<FuncOp>> CreateSetBatchSizePass();
+
 }  // namespace mlir
 
 #endif  // LARQ_COMPUTE_ENGINE_MLIR_PASSES_H_
diff --git larq_compute_engine/mlir/transforms/set_batch_size.cc larq_compute_engine/mlir/transforms/set_batch_size.cc
new file mode 100644
index 0000000..a4d8a57
--- /dev/null
+++ larq_compute_engine/mlir/transforms/set_batch_size.cc
@@ -0,0 +1,68 @@
+#include "mlir/Pass/Pass.h"
+
+// This pass will set the batch dimension of all inputs of the outermost
+// function to 1, leaving it to shape inference to do the rest.
+
+namespace mlir {
+
+namespace {
+
+mlir::Type SetBatchSize(mlir::Type type) {
+  auto tensor_type = type.dyn_cast<mlir::TensorType>();
+  if (tensor_type && tensor_type.hasRank()) {
+    auto shape = tensor_type.getShape();
+    if (shape.size() > 0 && shape[0] == ShapedType::kDynamicSize) {
+      // Create a new shape but set the first dimension to 1
+      llvm::SmallVector<int64_t, 4> shape_new(shape.begin(), shape.end());
+      shape_new[0] = 1;
+
+      return tensor_type.clone(shape_new);
+    }
+  }
+  return nullptr;
+}
+
+struct SetBatchSizePass : public PassWrapper<SetBatchSizePass, FunctionPass> {
+  void runOnFunction() override {
+    FuncOp func = getFunction();
+
+    // We have to edit both the function signature (mlir::Type) *and* the
+    // function arguments (mlir::Value)
+
+    // mlir::FunctionType is a TableGen-autogenerated MLIR type
+    mlir::FunctionType signature = func.getType();
+
+    // Create a mutable copy of the input types, since getInputs returns an
+    // immutable llvm::ArrayRef<mlir::Type>
+    std::vector<mlir::Type> signature_inputs(signature.getInputs());
+
+    for (auto& input_type : signature_inputs) {
+      auto new_type = SetBatchSize(input_type);
+      if (new_type) input_type = new_type;
+    }
+
+    auto signature_new = mlir::FunctionType::get(
+        signature.getContext(), signature_inputs, signature.getResults());
+    // Set the new signature
+    func.typeAttr(mlir::TypeAttr::get(signature_new));
+
+    // Now apply the same change to the mlir::Value objects
+    for (mlir::BlockArgument arg : func.getArguments()) {
+      // mlir::BlockArgument is a sublcass of mlir::Value
+      auto new_type = SetBatchSize(arg.getType());
+      if (new_type) arg.setType(new_type);
+    }
+  }
+};
+
+}  // namespace
+
+// Creates an instance of the ZeroPointCompatibility pass.
+std::unique_ptr<OperationPass<FuncOp>> CreateSetBatchSizePass() {
+  return std::make_unique<SetBatchSizePass>();
+}
+
+static PassRegistration<SetBatchSizePass> pass("mlir-setbatchsize",
+                                               "Set batch size to 1");
+
+}  // namespace mlir
diff --git larq_compute_engine/mlir/transforms/translate_tflite.cc larq_compute_engine/mlir/transforms/translate_tflite.cc
new file mode 100644
index 0000000..1fe799b
--- /dev/null
+++ larq_compute_engine/mlir/transforms/translate_tflite.cc
@@ -0,0 +1,83 @@
+#include "flatbuffers/flexbuffers.h"
+#include "larq_compute_engine/mlir/ir/lce_ops.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "tensorflow/compiler/mlir/lite/ir/tfl_ops.h"
+
+namespace mlir {
+namespace TFL {
+
+namespace {
+
+struct TranslateToLCE : public PassWrapper<TranslateToLCE, FunctionPass> {
+  void getDependentDialects(DialectRegistry& registry) const override {
+    registry.insert<mlir::TFL::TensorFlowLiteDialect, mlir::lq::LarqDialect>();
+  }
+  void runOnFunction() override;
+};
+
+struct TranslateToLCEPattern : public OpRewritePattern<TFL::CustomOp> {
+  using OpRewritePattern<TFL::CustomOp>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(TFL::CustomOp custom_op,
+                                PatternRewriter& rewriter) const override {
+    auto stringData = custom_op.custom_option().getValue();
+
+    // Replace CustomOp with relevant LarqOp
+    if (custom_op.custom_code() == "LceQuantize") {
+      rewriter.replaceOpWithNewOp<lq::QuantizeOp>(
+          custom_op, custom_op->getResultTypes(), custom_op->getOperands());
+    } else if (custom_op.custom_code() == "LceDequantize") {
+      rewriter.replaceOpWithNewOp<lq::DequantizeOp>(
+          custom_op, custom_op->getResultTypes(), custom_op->getOperands());
+    } else if (custom_op.custom_code() == "LceBMaxPool2d") {
+      auto map =
+          flexbuffers::GetRoot((uint8_t*)stringData.data(), stringData.size())
+              .AsMap();
+      rewriter.replaceOpWithNewOp<lq::BMaxPool2dOp>(
+          custom_op, custom_op->getResultTypes(), custom_op->getOperand(0),
+          stringifyPadding(static_cast<Padding>(map["padding"].AsInt32())),
+          map["stride_width"].AsInt32(), map["stride_height"].AsInt32(),
+          map["filter_width"].AsInt32(), map["filter_height"].AsInt32());
+    } else if (custom_op.custom_code() == "LceBconv2d") {
+      auto map =
+          flexbuffers::GetRoot((uint8_t*)stringData.data(), stringData.size())
+              .AsMap();
+      rewriter.replaceOpWithNewOp<lq::Bconv2dOp>(
+          custom_op, custom_op->getResultTypes(), custom_op->getOperand(0),
+          custom_op->getOperand(1), custom_op->getOperand(2),
+          custom_op->getOperand(3), custom_op->getOperand(4),
+          map["channels_in"].AsInt32(), map["dilation_height_factor"].AsInt32(),
+          map["dilation_width_factor"].AsInt32(),
+          stringifyActivationFunctionType(static_cast<ActivationFunctionType>(
+              map["fused_activation_function"].AsInt32())),
+          map["pad_values"].AsInt32(),
+          stringifyPadding(static_cast<Padding>(map["padding"].AsInt32())),
+          map["stride_height"].AsInt32(), map["stride_width"].AsInt32());
+    }
+
+    return success();
+  }
+};
+
+void TranslateToLCE::runOnFunction() {
+  OwningRewritePatternList patterns(&getContext());
+  auto* ctx = &getContext();
+  auto func = getFunction();
+  patterns.insert<TranslateToLCEPattern>(ctx);
+  (void)applyPatternsAndFoldGreedily(func, std::move(patterns));
+}
+
+}  // namespace
+
+// Creates an instance of the TranslateToLCE pass.
+std::unique_ptr<OperationPass<FuncOp>> CreateTranslateToLCEPass() {
+  return std::make_unique<TranslateToLCE>();
+}
+
+static PassRegistration<TranslateToLCE> pass(
+    "lce-translate-tfl", "Translate TFL custom ops to LCE ops");
+
+}  // namespace TFL
+}  // namespace mlir
diff --git third_party/install_android.sh third_party/install_android.sh
index 3a910e4..f6d5075 100755
--- third_party/install_android.sh
+++ third_party/install_android.sh
@@ -6,7 +6,7 @@ export ANDROID_SDK_URL="https://dl.google.com/android/repository/sdk-tools-linux
 export ANDROID_HOME="/tmp/lce_android"
 export ANDROID_VERSION=29
 export ANDROID_BUILD_TOOLS_VERSION=28.0.3
-export ANDROID_NDK_VERSION=18.1.5063045
+export ANDROID_NDK_VERSION=19.2.5345600
 
 # download android SDK
 mkdir -p $ANDROID_HOME; cd $ANDROID_HOME;
diff --git third_party/tensorflow_patches/tf_pr_48546.patch third_party/tensorflow_patches/tf_pr_48546.patch
deleted file mode 100644
index b6b0cab..0000000
--- third_party/tensorflow_patches/tf_pr_48546.patch
+++ /dev/null
@@ -1,36 +0,0 @@
-diff --git a/tensorflow/lite/delegates/gpu/cl/BUILD b/tensorflow/lite/delegates/gpu/cl/BUILD
-index b37629a97aa..5e6518605e4 100644
---- a/tensorflow/lite/delegates/gpu/cl/BUILD
-+++ b/tensorflow/lite/delegates/gpu/cl/BUILD
-@@ -3,6 +3,7 @@ load(
-     "//tensorflow/core/platform:build_config_root.bzl",
-     "tf_gpu_tests_tags",
- )
-+load("//tensorflow:tensorflow.bzl", "workspace_root")
-
- package(
-     default_visibility = ["//visibility:public"],
-@@ -490,7 +491,7 @@ flatbuffer_cc_library(
-     srcs = ["serialization.fbs"],
-     flatc_args = [
-         "--scoped-enums",
--        "-I ./",
-+        "-I " + workspace_root,
-     ],
-     includes = [
-         "//tensorflow/lite/delegates/gpu/common/task:serialization_base_cc_fbs_includes",
-diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl
-index a9da708bb53..2e070d7f137 100644
---- a/tensorflow/tensorflow.bzl
-+++ b/tensorflow/tensorflow.bzl
-@@ -52,6 +52,11 @@ VERSION = "2.5.0"
- VERSION_MAJOR = VERSION.split(".")[0]
- two_gpu_tags = ["requires-gpu-nvidia:2", "notap", "manual", "no_pip"]
-
-+# The workspace root, to be used to set workspace 'include' paths in a way that
-+# will still work correctly when TensorFlow is included as a dependency of an
-+# external project.
-+workspace_root = Label("//:WORKSPACE").workspace_root or "./"
-+
- def clean_dep(target):
-     """Returns string to 'target' in @org_tensorflow repository.
